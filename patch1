diff --git a/bouncer/CMakeLists.txt b/bouncer/CMakeLists.txt
index 18f88f76..e32fd999 100644
--- a/bouncer/CMakeLists.txt
+++ b/bouncer/CMakeLists.txt
@@ -3,7 +3,7 @@
 add_subdirectory(src)
 
 set(VERSION_MAJOR 5)
-set(VERSION_MINOR 18_0)
+set(VERSION_MINOR 18_1)
 
 include_directories(
 	${MONGO_CXX_DRIVER_MONGO_INCLUDE_DIR}
diff --git a/bouncer/src/repo/core/handler/database/repo_query.h b/bouncer/src/repo/core/handler/database/repo_query.h
index 721030af..e56d7ce0 100644
--- a/bouncer/src/repo/core/handler/database/repo_query.h
+++ b/bouncer/src/repo/core/handler/database/repo_query.h
@@ -22,6 +22,7 @@
 #include "repo_query_fwd.h"
 
 #include <vector>
+#include <set>
 #include <string>
 #include <memory>
 #include <variant>
@@ -148,6 +149,12 @@ namespace repo {
 					{
 					public:
 						AddParent(const repo::lib::RepoUUID& uniqueId, std::vector<repo::lib::RepoUUID> parentIds) :
+							uniqueId(uniqueId),
+							parentIds(parentIds.begin(), parentIds.end())
+						{
+						}
+
+						AddParent(const repo::lib::RepoUUID& uniqueId, std::set<repo::lib::RepoUUID> parentIds) :
 							uniqueId(uniqueId),
 							parentIds(parentIds)
 						{
@@ -160,9 +167,27 @@ namespace repo {
 						}
 
 						repo::lib::RepoUUID uniqueId;
-						std::vector<repo::lib::RepoUUID> parentIds;
+						std::set<repo::lib::RepoUUID> parentIds;
 					};
 
+					/*
+					 * Convenience type to build a projection in multiple stages.
+					 */
+					class REPO_API_EXPORT RepoProjectionBuilder
+					{
+					public:
+						
+						void includeField(std::string field) {
+							includedFields.push_back(field);
+						};
+
+						void excludeField(std::string field) {
+							excludedFields.push_back(field);
+						}
+
+						std::vector<std::string> includedFields;
+						std::vector<std::string> excludedFields;
+					};
 				}
 
 				namespace index
diff --git a/bouncer/src/repo/core/handler/database/repo_query_fwd.h b/bouncer/src/repo/core/handler/database/repo_query_fwd.h
index 0c748225..4acc1a2e 100644
--- a/bouncer/src/repo/core/handler/database/repo_query_fwd.h
+++ b/bouncer/src/repo/core/handler/database/repo_query_fwd.h
@@ -28,10 +28,11 @@ namespace repo {
 					class Exists;
 					class Or;
 					class RepoQueryBuilder;
+					class RepoProjectionBuilder;
 
 					class AddParent;
 
-					using RepoQuery = std::variant<Eq, Exists, Or, RepoQueryBuilder>;
+					using RepoQuery = std::variant<Eq, Exists, Or, RepoQueryBuilder, RepoProjectionBuilder>;
 
 					using RepoUpdate = std::variant<AddParent>;
 				}
diff --git a/bouncer/src/repo/core/handler/fileservice/repo_file_manager.cpp b/bouncer/src/repo/core/handler/fileservice/repo_file_manager.cpp
index db30370c..f2a58c50 100644
--- a/bouncer/src/repo/core/handler/fileservice/repo_file_manager.cpp
+++ b/bouncer/src/repo/core/handler/fileservice/repo_file_manager.cpp
@@ -195,11 +195,21 @@ repo::core::model::RepoRefT<repo::lib::RepoUUID> FileManager::getFileRef(
 	);
 }
 
+template<typename IdType>
+std::vector<uint8_t> FileManager::getFile(
+	const std::string& databaseName,
+	const std::string& collectionNamePrefix,
+	const IdType& fileName
+) {
+	return getFile(databaseName, collectionNamePrefix, fileName, Encoding::None);
+}
+
 template<typename IdType>
 std::vector<uint8_t> FileManager::getFile(
 	const std::string                            &databaseName,
 	const std::string                            &collectionNamePrefix,
-	const IdType                                 &fileName
+	const IdType                                 &fileName,
+	const Encoding								 &encoding
 ) {
 	std::vector<uint8_t> file;
 	auto ref = getFileRef(databaseName, collectionNamePrefix, fileName);
@@ -211,6 +221,27 @@ std::vector<uint8_t> FileManager::getFile(
 	{
 		repoTrace << "Getting file (" << keyName << ") from FS";
 		file = fsHandler->getFile(databaseName, collectionNamePrefix, keyName);
+
+		switch (encoding) {
+		case Encoding::Gzip:
+			// Use stringstream as a binary container to hold the uncompressed data
+			std::ostringstream uncompressedstream;
+
+			// Bufferstream operates directly over the user provided array
+			boost::interprocess::bufferstream compressed((char*)file.data(), file.size());
+
+			// In stream form for filtering_istream
+			std::istream compressedStream(compressed.rdbuf());
+
+			boost::iostreams::filtering_istream in;
+			in.push(boost::iostreams::gzip_decompressor());
+			in.push(compressedStream); // For some reason bufferstream is ambigous between stream and streambuf, so wrap it unambiguously
+
+			boost::iostreams::copy(in, uncompressedstream);
+
+			auto uncompresseddata = uncompressedstream.str();
+			file = std::vector<uint8_t>(uncompresseddata.begin(), uncompresseddata.end());
+		}
 	}
 	break;
 	default:
@@ -224,6 +255,8 @@ std::vector<uint8_t> FileManager::getFile(
 
 template std::vector<uint8_t> FileManager::getFile(const std::string&, const std::string&, const std::string&);
 template std::vector<uint8_t> FileManager::getFile(const std::string&, const std::string&, const repo::lib::RepoUUID&);
+template std::vector<uint8_t> FileManager::getFile(const std::string&, const std::string&, const std::string&, const Encoding&);
+template std::vector<uint8_t> FileManager::getFile(const std::string&, const std::string&, const repo::lib::RepoUUID&, const Encoding&);
 
 /**
  * Get the file base on the the ref entry in database
diff --git a/bouncer/src/repo/core/handler/fileservice/repo_file_manager.h b/bouncer/src/repo/core/handler/fileservice/repo_file_manager.h
index 581de0ea..da36ff9a 100644
--- a/bouncer/src/repo/core/handler/fileservice/repo_file_manager.h
+++ b/bouncer/src/repo/core/handler/fileservice/repo_file_manager.h
@@ -69,6 +69,16 @@ namespace repo {
 						const Encoding                               &encoding = Encoding::None
 					);
 
+					/**
+					 * Get the file base on the the ref entry in database
+					 */
+					template<typename IdType>
+					std::vector<uint8_t> getFile(
+						const std::string& databaseName,
+						const std::string& collectionNamePrefix,
+						const IdType& id						
+					);
+
 					/**
 					 * Get the file base on the the ref entry in database
 					 */
@@ -76,7 +86,8 @@ namespace repo {
 					std::vector<uint8_t> getFile(
 						const std::string                            &databaseName,
 						const std::string                            &collectionNamePrefix,
-						const IdType                                 &id
+						const IdType                                 &id,
+						const Encoding								 &encoding
 					);
 
 					/**
diff --git a/bouncer/src/repo/core/handler/repo_database_handler_abstract.h b/bouncer/src/repo/core/handler/repo_database_handler_abstract.h
index d10a6daf..45355398 100644
--- a/bouncer/src/repo/core/handler/repo_database_handler_abstract.h
+++ b/bouncer/src/repo/core/handler/repo_database_handler_abstract.h
@@ -31,6 +31,9 @@ namespace repo {
 			class RepoBSON;
 		}
 		namespace handler {
+			namespace fileservice {				
+				class FileManager;
+			}
 			namespace database {
 				namespace index {
 					class RepoIndex;
@@ -163,6 +166,15 @@ namespace repo {
 				*/
 				virtual void createIndex(const std::string &database, const std::string &collection, const database::index::RepoIndex&) = 0;
 
+				/**
+				* Create an index within the given collection
+				* @param database name of the database
+				* @param name name of the collection
+				* @param index BSONObj specifying the index
+				* @param bool whether this is a sparse index
+				*/
+				virtual void createIndex(const std::string& database, const std::string& collection, const database::index::RepoIndex& index, bool sparse) = 0;
+
 				/**
 				* Insert a single document in database.collection
 				* @param database name
@@ -240,10 +252,54 @@ namespace repo {
 				* @return a vector of RepoBSON objects satisfy the given criteria
 				*/
 				virtual std::vector<repo::core::model::RepoBSON> findAllByCriteria(
+					const std::string& database,
+					const std::string& collection,
+					const database::query::RepoQuery& criteria,
+					const bool loadBinaries = false) = 0;
+
+				/**
+				* Given a search criteria,  find all the documents that passes this query
+				* @param database name of database
+				* @param collection name of collection
+				* @param criteria search criteria in a bson object
+				* @param projection to define the fiels in the returned document
+				* @return a vector of RepoBSON objects satisfy the given criteria
+				*/
+				virtual std::vector<repo::core::model::RepoBSON> findAllByCriteria(
+					const std::string& database,
+					const std::string& collection,
+					const database::query::RepoQuery& filter,
+					const database::query::RepoQuery& projection,
+					const bool loadBinaries = false) = 0;
+
+
+
+				/**
+				* Given a search criteria,  find all the documents that passes this query
+				* @param database name of database
+				* @param collection name of collection
+				* @param criteria search criteria in a bson object
+				* @return a MongoCursor allowing traversal of the documents that satisfy the given criteria
+				*/
+				virtual std::unique_ptr<database::Cursor> findCursorByCriteria(
 					const std::string& database,
 					const std::string& collection,
 					const database::query::RepoQuery& criteria) = 0;
 
+				/**
+				* Given a search criteria,  find all the documents that passes this query
+				* @param database name of database
+				* @param collection name of collection
+				* @param criteria search criteria in a bson object
+				* @param projection to define the fiels in the returned document
+				* @return a MongoCursor allowing traversal of the documents that satisfy the given criteria
+				*/
+				virtual std::unique_ptr<database::Cursor> findCursorByCriteria(
+					const std::string& database,
+					const std::string& collection,
+					const database::query::RepoQuery& filter,
+					const database::query::RepoQuery& projection) = 0;
+
 				/**
 				* Given a search criteria,  find one documents that passes this query
 				* @param database name of database
@@ -306,6 +362,10 @@ namespace repo {
 					const std::string& database,
 					const std::string& collection) = 0;
 
+				virtual void setFileManager(std::shared_ptr<repo::core::handler::fileservice::FileManager> manager) = 0;
+
+				virtual std::shared_ptr<repo::core::handler::fileservice::FileManager> getFileManager() = 0;
+
 			protected:
 				/**
 				* Default constructor
@@ -314,6 +374,11 @@ namespace repo {
 				AbstractDatabaseHandler(uint64_t size) :maxDocumentSize(size) {};
 
 				const uint64_t maxDocumentSize;
+
+				// The fileManager is used in the storage of certain member types, such
+				// as large vectors of binary data. It must be set using setFileManager
+				// before documents containing such members are uploaded.
+				std::shared_ptr<repo::core::handler::fileservice::FileManager> fileManager;
 			};
 		}
 	}
diff --git a/bouncer/src/repo/core/handler/repo_database_handler_mongo.cpp b/bouncer/src/repo/core/handler/repo_database_handler_mongo.cpp
index c3cbc7de..3f0274cf 100644
--- a/bouncer/src/repo/core/handler/repo_database_handler_mongo.cpp
+++ b/bouncer/src/repo/core/handler/repo_database_handler_mongo.cpp
@@ -204,6 +204,19 @@ struct MongoQueryFilterVistior
 			std::visit(*this, q);
 		}
 	}
+
+	void operator() (const query::RepoProjectionBuilder& n) const {
+		// RepoProjectionBuilder allows to construct projections by providing
+		// fields to be included and fields to be excluded
+
+		for (std::string field : n.includedFields) {
+			builder->append(field, 1);
+		}
+
+		for (std::string field : n.excludedFields) {
+			builder->append(field, 0);
+		}
+	}
 };
 
 /*
@@ -226,12 +239,12 @@ struct MongoUpdateVisitor
 
 		if (u.parentIds.size() == 1)
 		{
-			operation.append(REPO_NODE_LABEL_PARENTS, u.parentIds[0]);
+			operation.append(REPO_NODE_LABEL_PARENTS, *u.parentIds.begin());
 		}
 		else
 		{
 			repo::core::model::RepoBSONBuilder array;
-			array.appendArray("$each", u.parentIds);
+			array.appendIteratable("$each", u.parentIds.begin(), u.parentIds.end());
 			operation.append(REPO_NODE_LABEL_PARENTS, array.obj());
 		}
 
@@ -324,6 +337,11 @@ bool MongoDatabaseHandler::caseInsensitiveStringCompare(
 }
 
 void MongoDatabaseHandler::createIndex(const std::string& database, const std::string& collection, const database::index::RepoIndex& index)
+{
+	createIndex(database, collection, index, false);
+}
+
+void MongoDatabaseHandler::createIndex(const std::string& database, const std::string& collection, const database::index::RepoIndex& index, bool sparse)
 {
 	try
 	{
@@ -334,9 +352,14 @@ void MongoDatabaseHandler::createIndex(const std::string& database, const std::s
 			auto col = db.collection(collection);
 			auto obj = (repo::core::model::RepoBSON)index;
 
-			repoInfo << "Creating index for :" << database << "." << collection << " : index: " << obj.toString();
+			repoInfo << "Creating index for :" << database << "." << collection << " : index: " << obj.toString() << " sparse: " << sparse;
+
+			mongocxx::v_noabi::options::index options;			
+			if (sparse) {
+				options.sparse(true);
+			}
 
-			col.create_index(obj.view());
+			col.create_index(obj.view(), options);
 		}
 	}
 	catch (...)
@@ -347,26 +370,10 @@ void MongoDatabaseHandler::createIndex(const std::string& database, const std::s
 
 /*
  * This helper function resolves the binary files for a given document. Any
- * document that might have file mappings should be passed through here
- * before being returned as a RepoBSON.
+ * document that might have file mappings that are not loaded manually later
+ * should be passed through here before being returned as a RepoBSON.
  */
-repo::core::model::RepoBSON MongoDatabaseHandler::createRepoBSON(
-	const std::string& database, 
-	const std::string& collection, 
-	const bsoncxx::document::view& view)
-{
-	fileservice::BlobFilesHandler blobHandler(fileManager, database, collection);
-
-	repo::core::model::RepoBSON orgBson = repo::core::model::RepoBSON(view);
-	if (orgBson.hasFileReference()) {
-		auto ref = orgBson.getBinaryReference();
-		auto buffer = blobHandler.readToBuffer(fileservice::DataRef::deserialise(ref));
-		orgBson.initBinaryBuffer(buffer);
-	}
-	return orgBson;
-}
-
-void MongoDatabaseHandler::loadBinaries(const std::string& database,
+void MongoDatabaseHandler::loadBinaryBuffers(const std::string& database,
 	const std::string& collection, 
 	repo::core::model::RepoBSON& bson)
 {
@@ -435,7 +442,18 @@ void MongoDatabaseHandler::dropDocument(
 std::vector<repo::core::model::RepoBSON> MongoDatabaseHandler::findAllByCriteria(
 	const std::string& database,
 	const std::string& collection,
-	const database::query::RepoQuery& filter)
+	const database::query::RepoQuery& filter,
+	const bool loadBinaries/* = false*/) {
+
+	return findAllByCriteria(database, collection, filter, database::query::RepoProjectionBuilder{}, loadBinaries);
+}
+
+std::vector<repo::core::model::RepoBSON> MongoDatabaseHandler::findAllByCriteria(
+	const std::string& database,
+	const std::string& collection,
+	const database::query::RepoQuery& filter,
+	const database::query::RepoQuery& projection,
+	const bool loadBinaries/* = false*/)
 {
 	try
 	{
@@ -447,10 +465,18 @@ std::vector<repo::core::model::RepoBSON> MongoDatabaseHandler::findAllByCriteria
 			auto db = client->database(database);
 			auto col = db.collection(collection);
 
+			repo::core::model::RepoBSON projectionBson = makeQueryFilterDocument(projection);
+			mongocxx::v_noabi::options::find options;
+			options.projection(projectionBson.view());
+
 			// Find all documents
 			auto cursor = col.find(criteria.view());
 			for (auto& doc : cursor) {
-				data.push_back(createRepoBSON(database, collection, doc));
+				auto bson = repo::core::model::RepoBSON(doc);
+				if (loadBinaries)
+					MongoDatabaseHandler::loadBinaryBuffers(database, collection, bson);
+					
+				data.push_back(bson);
 			}
 		}
 		return data;
@@ -485,7 +511,7 @@ repo::core::model::RepoBSON MongoDatabaseHandler::findOneByCriteria(
 			auto findResult = col.find_one(criteria.view(), options);
 			if (findResult.has_value()) {
 				fileservice::BlobFilesHandler blobHandler(fileManager, database, collection);
-				return createRepoBSON(database, collection, findResult.value());
+				return repo::core::model::RepoBSON(findResult.value());
 			}
 		}
 		return {};
@@ -568,7 +594,7 @@ MongoDatabaseHandler::getAllFromCollectionTailable(
 
 		auto cursor = col.find({}, options);
 		for (auto doc : cursor) {
-			bsons.push_back(createRepoBSON(database, collection, doc));
+			bsons.push_back(repo::core::model::RepoBSON(doc));
 		}
 
 		return bsons;
@@ -815,7 +841,7 @@ public:
 
 		virtual const repo::core::model::RepoBSON operator*()
 		{
-			return cursor->createRepoBSON(mongocxx::v_noabi::cursor::iterator::operator*());
+			return repo::core::model::RepoBSON(mongocxx::v_noabi::cursor::iterator::operator*());
 		}
 
 		virtual void operator++()
@@ -862,31 +888,51 @@ private:
 	std::string database;
 	std::string collection;
 	MongoDatabaseHandler* handler;
-
-	repo::core::model::RepoBSON createRepoBSON(const bsoncxx::document::view& view)
-	{
-		return handler->createRepoBSON(database, collection, view);
-	}
 };
 
-std::unique_ptr<repo::core::handler::database::Cursor> MongoDatabaseHandler::getAllByCriteria(
+std::unique_ptr<Cursor> repo::core::handler::MongoDatabaseHandler::findCursorByCriteria(
 	const std::string& database,
 	const std::string& collection,
-	const database::query::RepoQuery& criteria
-)
+	const database::query::RepoQuery& criteria)
+{	
+	return findCursorByCriteria(database, collection, criteria, database::query::RepoProjectionBuilder{});
+}
+
+std::unique_ptr<Cursor> repo::core::handler::MongoDatabaseHandler::findCursorByCriteria(
+	const std::string& database,
+	const std::string& collection,
+	const database::query::RepoQuery& filter,
+	const database::query::RepoQuery& projection)
 {
 	try
 	{
-		auto client = clientPool->acquire();
-		auto db = client->database(database);
-		auto col = db.collection(collection);
+		repo::core::model::RepoBSON criteria = makeQueryFilterDocument(filter);
+		if (!criteria.isEmpty() && !database.empty() && !collection.empty())
+		{
+			auto client = clientPool->acquire();
+			auto db = client->database(database);
+			auto col = db.collection(collection);
+
+			repo::core::model::RepoBSON projectionBson = makeQueryFilterDocument(projection);
+			mongocxx::v_noabi::options::find options;
+			options.projection(projectionBson.view());
+
+			// Find all documents and return cursor
+			// Some pointer magic, because we have to cast the mongo cursor to its base before returning it.
+			// Ownership will be the caller's after the two raw pointers go out of scope
+			MongoDatabaseHandler::MongoCursor* mongoCursor = new MongoDatabaseHandler::MongoCursor(std::move(col.find(criteria.view())), this);			
+			database::Cursor *baseCursor = mongoCursor;
+			return std::unique_ptr<database::Cursor>(baseCursor);			
+		}
+		else
+		{
+			return nullptr;
+		}
 
-		repo::core::model::RepoBSON q = makeQueryFilterDocument(criteria);
-		return std::make_unique<MongoCursor>(std::move(col.find(q.view())), this);
 	}
 	catch (...)
 	{
-		std::throw_with_nested(MongoDatabaseHandlerException(*this, "getAllByCriteria (cursor)", database, collection));
+		std::throw_with_nested(MongoDatabaseHandlerException(*this, "findCursorByCriteria", database, collection));
 	}
 }
 
diff --git a/bouncer/src/repo/core/handler/repo_database_handler_mongo.h b/bouncer/src/repo/core/handler/repo_database_handler_mongo.h
index b6dacefd..bd777ebd 100644
--- a/bouncer/src/repo/core/handler/repo_database_handler_mongo.h
+++ b/bouncer/src/repo/core/handler/repo_database_handler_mongo.h
@@ -38,6 +38,8 @@
 #include <mongocxx/client-fwd.hpp>
 #include <mongocxx/pool-fwd.hpp>
 #include <bsoncxx/document/view-fwd.hpp>
+#include <mongocxx/pipeline.hpp>
+#include <mongocxx/cursor.hpp>
 
 namespace repo {
 	namespace core {
@@ -171,6 +173,15 @@ namespace repo {
 				*/
 				virtual void createIndex(const std::string &database, const std::string &collection, const database::index::RepoIndex& index);
 
+				/**
+				* Create an index within the given collection
+				* @param database name of the database
+				* @param name name of the collection
+				* @param index BSONObj specifying the index
+				* @param bool whether this is a sparse index
+				*/
+				virtual void createIndex(const std::string& database, const std::string& collection, const database::index::RepoIndex& index, bool sparse);
+
 				/**
 				* Remove a collection from the database
 				* @param database the database the collection resides in
@@ -248,10 +259,52 @@ namespace repo {
 				* @return a vector of RepoBSON objects satisfy the given criteria
 				*/
 				std::vector<repo::core::model::RepoBSON> findAllByCriteria(
+					const std::string& database,
+					const std::string& collection,
+					const database::query::RepoQuery& criteria,
+					const bool loadBinaries = false);
+
+				/**
+				* Given a search criteria,  find all the documents that passes this query
+				* @param database name of database
+				* @param collection name of collection
+				* @param criteria search criteria in a bson object
+				* @param projection to define the fiels in the returned document
+				* @return a vector of RepoBSON objects satisfy the given criteria
+				*/
+				std::vector<repo::core::model::RepoBSON> findAllByCriteria(
+					const std::string& database,
+					const std::string& collection,
+					const database::query::RepoQuery& filter,
+					const database::query::RepoQuery& projection,
+					const bool loadBinaries = false);
+
+				/**
+				* Given a search criteria,  find all the documents that passes this query
+				* @param database name of database
+				* @param collection name of collection
+				* @param criteria search criteria in a bson object
+				* @return a MongoCursor allowing traversal of the documents that satisfy the given criteria
+				*/
+				std::unique_ptr<database::Cursor> findCursorByCriteria(
 					const std::string& database,
 					const std::string& collection,
 					const database::query::RepoQuery& criteria);
 
+				/**
+				* Given a search criteria,  find all the documents that passes this query
+				* @param database name of database
+				* @param collection name of collection
+				* @param criteria search criteria in a bson object
+				* @param projection to define the fiels in the returned document
+				* @return a MongoCursor allowing traversal of the documents that satisfy the given criteria
+				*/
+				std::unique_ptr<database::Cursor> findCursorByCriteria(
+					const std::string& database,
+					const std::string& collection,
+					const database::query::RepoQuery& filter,
+					const database::query::RepoQuery& projection);
+
 				/**
 				* Given a search criteria,  find one documents that passes this query
 				* @param database name of database
@@ -313,12 +366,6 @@ namespace repo {
 					const std::string& collection,
 					const database::query::RepoQuery& criteria);
 
-				std::unique_ptr<repo::core::handler::database::Cursor> getAllByCriteria(
-					const std::string& database,
-					const std::string& collection,
-					const database::query::RepoQuery& criteria
-				);
-
 				std::unique_ptr<database::BulkWriteContext> getBulkWriteContext(
 					const std::string& database,
 					const std::string& collection);
@@ -330,7 +377,7 @@ namespace repo {
 				/*
 				* If a RepoBSON comes from somewhere else, populate the binaries.
 				*/
-				void loadBinaries(const std::string& database,
+				void loadBinaryBuffers(const std::string& database,
 					const std::string& collection, 
 					repo::core::model::RepoBSON& bson);
 
@@ -352,16 +399,6 @@ namespace repo {
 				// specific, container for getting connections. pool pool is threadsafe.
 				std::unique_ptr<mongocxx::pool> clientPool;
 
-				// The fileManager is used in the storage of certain member types, such
-				// as large vectors of binary data. It must be set using setFileManager
-				// before documents containing such members are uploaded.
-				std::shared_ptr<repo::core::handler::fileservice::FileManager> fileManager;
-
-				repo::core::model::RepoBSON createRepoBSON(
-					const std::string& database,
-					const std::string& collection,
-					const bsoncxx::document::view& view);
-
 				/**
 				* Get large file off GridFS
 				* @param database database that it is stored in
diff --git a/bouncer/src/repo/core/model/bson/CMakeLists.txt b/bouncer/src/repo/core/model/bson/CMakeLists.txt
index dfdd116c..f1a76688 100644
--- a/bouncer/src/repo/core/model/bson/CMakeLists.txt
+++ b/bouncer/src/repo/core/model/bson/CMakeLists.txt
@@ -25,6 +25,7 @@ set(SOURCES
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_model_revision.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_reference.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_revision.cpp
+	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_streaming_mesh.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_supermesh.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_texture.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_transformation.cpp
@@ -51,6 +52,7 @@ set(HEADERS
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_model_revision.h
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_reference.h
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_revision.h
+	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_streaming_mesh.h
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_supermesh.h
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_texture.h
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_node_transformation.h
diff --git a/bouncer/src/repo/core/model/bson/repo_bson.cpp b/bouncer/src/repo/core/model/bson/repo_bson.cpp
index 8091ca9c..d341c8fe 100644
--- a/bouncer/src/repo/core/model/bson/repo_bson.cpp
+++ b/bouncer/src/repo/core/model/bson/repo_bson.cpp
@@ -263,28 +263,25 @@ repo::lib::RepoVector3D RepoBSON::getVector3DField(const std::string& label) con
 	return v;
 }
 
-repo::lib::RepoMatrix RepoBSON::getMatrixField(const std::string& label) const
+repo::lib::repo_color3d_t repo::core::model::RepoBSON::getColourField(const std::string& label) const
 {
-	std::vector<float> transformationMatrix;
-	transformationMatrix.resize(16);
-	float* transArr = &transformationMatrix.at(0);
-
-	// matrix is stored as array of arrays, row first
-
-	auto matrixObj = getField(label);
-	const bsoncxx::array::view& rows = matrixObj.get_array();
-	for (uint32_t rowInd = 0; rowInd < 4; rowInd++)
+	auto arr = getFloatArray(label);
+	if (arr.size() < 3)
 	{
-		const bsoncxx::array::view& cols = rows[rowInd].get_array();
-		for (uint32_t colInd = 0; colInd < 4; colInd++)
-		{
-			const auto& e = cols[colInd];
-			uint32_t index = rowInd * 4 + colInd;
-			transArr[index] = e.get_double();
-		}
+		return repo::lib::repo_color3d_t();
 	}
+	else {
+		repo::lib::repo_color3d_t c;
+		c.r = arr[0];
+		c.g = arr[1];
+		c.b = arr[2];
+		return c;
+	}
+}
 
-	return repo::lib::RepoMatrix(transformationMatrix);
+std::vector<repo::lib::RepoMatrix> RepoBSON::getMatrixFieldArray(const std::string& label) const
+{
+	return getArray<repo::lib::RepoMatrix>(label, true);
 }
 
 std::vector<double> RepoBSON::getDoubleVectorField(const std::string& label) const
@@ -406,6 +403,27 @@ int64_t RepoBSON::getLongField(const std::string& label) const
 	}
 }
 
+repo::lib::RepoMatrix getMatrixFromArray(const bsoncxx::array::view& rows) {
+	
+	std::vector<float> transformationMatrix;
+
+	transformationMatrix.resize(16);
+	float* transArr = &transformationMatrix.at(0);
+	
+	for (uint32_t rowInd = 0; rowInd < 4; rowInd++)
+	{
+		const bsoncxx::array::view& cols = rows[rowInd].get_array();
+		for (uint32_t colInd = 0; colInd < 4; colInd++)
+		{
+			const auto& e = cols[colInd];
+			uint32_t index = rowInd * 4 + colInd;
+			transArr[index] = e.get_double();
+		}
+	}
+
+	return repo::lib::RepoMatrix(transformationMatrix);
+}
+
 // These specialisations perform the concrete type conversion for the getArray
 // method.
 
@@ -447,6 +465,20 @@ template<> repo::lib::RepoUUID getValue<repo::lib::RepoUUID>(const bsoncxx::arra
 	throw repo::lib::RepoBSONException("Cannot convert binary field to RepoUUID because it has the wrong subtype");
 }
 
+template<> repo::lib::RepoMatrix getValue<repo::lib::RepoMatrix>(const bsoncxx::array::element& e)
+{
+	if (e.type() == bsoncxx::type::k_array)
+	{
+		return getMatrixFromArray(e.get_array());
+	}
+	throw repo::lib::RepoBSONException("Cannot convert array field to RepoMatrix because it has the wrong subtype");
+}
+
+repo::lib::RepoMatrix RepoBSON::getMatrixField(const std::string& label) const
+{	
+	return getMatrixFromArray(getField(label).get_array());
+}
+
 template<typename T>
 std::vector<T> RepoBSON::getArray(
 	const std::string& label,
diff --git a/bouncer/src/repo/core/model/bson/repo_bson.h b/bouncer/src/repo/core/model/bson/repo_bson.h
index ff0a5129..c4da0757 100644
--- a/bouncer/src/repo/core/model/bson/repo_bson.h
+++ b/bouncer/src/repo/core/model/bson/repo_bson.h
@@ -44,6 +44,7 @@
 #include "repo/lib/datastructure/repo_bounds.h"
 #include "repo/lib/repo_exception.h"
 #include "repo/core/model/bson/repo_bson_element.h"
+#include "repo/lib/datastructure/repo_structs.h"
 
 #include <bsoncxx/document/value.hpp>
 
@@ -117,8 +118,12 @@ namespace repo {
 
 				lib::RepoVector3D getVector3DField(const std::string& label) const;
 
+				lib::repo_color3d_t getColourField(const std::string& label) const;
+
 				repo::lib::RepoMatrix getMatrixField(const std::string& label) const;
 
+				std::vector<repo::lib::RepoMatrix> getMatrixFieldArray(const std::string& label) const;
+
 				std::vector<double> getDoubleVectorField(const std::string& label) const;
 
 				std::vector<std::string> getFileList(const std::string& label) const;
diff --git a/bouncer/src/repo/core/model/bson/repo_bson_builder.h b/bouncer/src/repo/core/model/bson/repo_bson_builder.h
index 7492d34f..2b4fe2e0 100644
--- a/bouncer/src/repo/core/model/bson/repo_bson_builder.h
+++ b/bouncer/src/repo/core/model/bson/repo_bson_builder.h
@@ -90,6 +90,21 @@ namespace repo {
 					append(vec);
 				}
 
+				template <class T>
+				void appendIteratable(
+					const std::string& label,
+					T begin,
+					T end
+				)
+				{
+					core::key_owned(label);
+					core::open_array();
+					for (auto it = begin; it != end; it++) {
+						append(*it);
+					}
+					core::close_array();
+				}
+
 				template<class T>
 				void append(
 					const std::string& label,
diff --git a/bouncer/src/repo/core/model/bson/repo_bson_factory.cpp b/bouncer/src/repo/core/model/bson/repo_bson_factory.cpp
index 49df68b4..59498575 100644
--- a/bouncer/src/repo/core/model/bson/repo_bson_factory.cpp
+++ b/bouncer/src/repo/core/model/bson/repo_bson_factory.cpp
@@ -101,7 +101,7 @@ MeshNode RepoBSONFactory::makeMeshNode(
 	return node;
 }
 
-SupermeshNode RepoBSONFactory::makeSupermeshNode(
+std::unique_ptr<SupermeshNode> RepoBSONFactory::makeSupermeshNode(
 	const std::vector<repo::lib::RepoVector3D>& vertices,
 	const std::vector<repo::lib::repo_face_t>& faces,
 	const std::vector<repo::lib::RepoVector3D>& normals,
@@ -111,21 +111,21 @@ SupermeshNode RepoBSONFactory::makeSupermeshNode(
 	const std::vector<repo::lib::repo_mesh_mapping_t>& mappings
 )
 {
-	SupermeshNode node;
-	node.setVertices(vertices);
-	node.setFaces(faces);
-	node.setNormals(normals);
-	node.setBoundingBox(boundingBox);
+	auto node = std::make_unique<SupermeshNode>();
+	node->setVertices(vertices);
+	node->setFaces(faces);
+	node->setNormals(normals);
+	node->setBoundingBox(boundingBox);
 	for (size_t i = 0; i < uvChannels.size(); i++)
 	{
-		node.setUVChannel(i, uvChannels[i]);
+		node->setUVChannel(i, uvChannels[i]);
 	}
-	node.changeName(name, true);
-	node.setMeshMapping(mappings);
+	node->changeName(name, true);
+	node->setMeshMapping(mappings);
 	return node;
 }
 
-SupermeshNode RepoBSONFactory::makeSupermeshNode(
+std::unique_ptr<SupermeshNode> RepoBSONFactory::makeSupermeshNode(
 	const std::vector<repo::lib::RepoVector3D>& vertices,
 	const std::vector<repo::lib::repo_face_t>& faces,
 	const std::vector<repo::lib::RepoVector3D>& normals,
@@ -137,19 +137,19 @@ SupermeshNode RepoBSONFactory::makeSupermeshNode(
 	const std::vector<float> mappingIds
 )
 {
-	SupermeshNode node;
-	node.setVertices(vertices);
-	node.setFaces(faces);
-	node.setNormals(normals);
-	node.setBoundingBox(boundingBox);
+	auto node = std::make_unique<SupermeshNode>();
+	node->setVertices(vertices);
+	node->setFaces(faces);
+	node->setNormals(normals);
+	node->setBoundingBox(boundingBox);
 	for (size_t i = 0; i < uvChannels.size(); i++)
 	{
-		node.setUVChannel(i, uvChannels[i]);
+		node->setUVChannel(i, uvChannels[i]);
 	}
-	node.setUniqueID(id);
-	node.setSharedID(id);
-	node.setMeshMapping(mappings);
-	node.setSubmeshIds(mappingIds);
+	node->setUniqueID(id);
+	node->setSharedID(id);
+	node->setMeshMapping(mappings);
+	node->setSubmeshIds(mappingIds);
 	return node;
 }
 
diff --git a/bouncer/src/repo/core/model/bson/repo_bson_factory.h b/bouncer/src/repo/core/model/bson/repo_bson_factory.h
index a91b0763..c0964071 100644
--- a/bouncer/src/repo/core/model/bson/repo_bson_factory.h
+++ b/bouncer/src/repo/core/model/bson/repo_bson_factory.h
@@ -171,7 +171,7 @@ namespace repo {
 					const std::string& name = std::string(),
 					const std::vector<repo::lib::RepoUUID>& parents = {});
 
-				static SupermeshNode makeSupermeshNode(
+				static std::unique_ptr<SupermeshNode> makeSupermeshNode(
 					const std::vector<repo::lib::RepoVector3D>& vertices,
 					const std::vector<repo::lib::repo_face_t>& faces,
 					const std::vector<repo::lib::RepoVector3D>& normals,
@@ -181,7 +181,7 @@ namespace repo {
 					const std::vector<repo::lib::repo_mesh_mapping_t>& mappings
 				);
 
-				static SupermeshNode makeSupermeshNode(
+				static std::unique_ptr<SupermeshNode> makeSupermeshNode(
 					const std::vector<repo::lib::RepoVector3D>& vertices,
 					const std::vector<repo::lib::repo_face_t>& faces,
 					const std::vector<repo::lib::RepoVector3D>& normals,
diff --git a/bouncer/src/repo/core/model/bson/repo_node_mesh.cpp b/bouncer/src/repo/core/model/bson/repo_node_mesh.cpp
index 913d6c6b..5dcf4c89 100644
--- a/bouncer/src/repo/core/model/bson/repo_node_mesh.cpp
+++ b/bouncer/src/repo/core/model/bson/repo_node_mesh.cpp
@@ -48,6 +48,10 @@ void MeshNode::deserialise(RepoBSON& bson)
 	if (bson.hasField(REPO_NODE_MESH_LABEL_GROUPING))
 		grouping = bson.getStringField(REPO_NODE_MESH_LABEL_GROUPING);
 
+	if (bson.hasField(REPO_FILTER_TAG_TEXTURE_ID)) {		
+		textureId = bson.getUUIDField(REPO_FILTER_TAG_TEXTURE_ID);
+	}
+
 	primitive = MeshNode::Primitive::TRIANGLES;
 	if (bson.hasField(REPO_NODE_MESH_LABEL_PRIMITIVE))
 		primitive = static_cast<MeshNode::Primitive>(bson.getIntField(REPO_NODE_MESH_LABEL_PRIMITIVE));
@@ -102,7 +106,7 @@ void MeshNode::deserialise(RepoBSON& bson)
 	}
 
 
-	if (bson.hasField(REPO_NODE_MESH_LABEL_UV_CHANNELS_COUNT))
+	if (bson.hasField(REPO_NODE_MESH_LABEL_UV_CHANNELS_COUNT) && bson.hasBinField(REPO_NODE_MESH_LABEL_UV_CHANNELS))
 	{
 		std::vector<repo::lib::RepoVector2D> serialisedChannels;
 		bson.getBinaryFieldAsVector(REPO_NODE_MESH_LABEL_UV_CHANNELS, serialisedChannels);
@@ -212,6 +216,49 @@ void appendUVChannels(RepoBSONBuilder& builder, size_t numChannels, const std::v
 	}
 }
 
+MeshNode::MaterialProperties determineMaterialProperties(
+	repo::lib::repo_material_t material,
+	repo::lib::RepoUUID texId,
+	int numUvChannels
+) {
+	if (!texId.isDefaultValue() && numUvChannels > 0) {
+		return MeshNode::MaterialProperties::TEXTUREDMAT;
+	}
+	else if (material.opacity == 1.f) {
+		return MeshNode::MaterialProperties::OPAQUEMAT;
+	}
+	else {
+		return MeshNode::MaterialProperties::TRANSPARENTMAT;
+	}
+}
+
+void appendFilterTags(
+	RepoBSONBuilder& builder,
+	std::string grouping,
+	MeshNode::MaterialProperties matProps,
+	repo::lib::RepoUUID textureId)
+{
+	if (!grouping.empty())
+		builder.append(REPO_NODE_MESH_LABEL_GROUPING, grouping);
+
+	RepoBSONBuilder matPropsBson;
+	switch (matProps) {
+	case MeshNode::MaterialProperties::OPAQUEMAT:
+		matPropsBson.append(REPO_FILTER_PROP_OPAQUE, true);
+		break;
+	case MeshNode::MaterialProperties::TRANSPARENTMAT:
+		matPropsBson.append(REPO_FILTER_PROP_TRANSPARENT, true);
+		break;
+	case MeshNode::MaterialProperties::TEXTUREDMAT:
+		matPropsBson.append(REPO_FILTER_PROP_TEXTURE_ID, textureId);
+		break;
+	default:
+		repoWarning << "MeshNode is getting serialised without having filter tags set. This node will be ignored during processing";
+	}
+		
+	builder.append(REPO_FILTER_OBJECT_NAME, matPropsBson.obj());
+}
+
 void MeshNode::serialise(repo::core::model::RepoBSONBuilder& builder) const
 {
 	RepoNode::serialise(builder);
@@ -220,6 +267,14 @@ void MeshNode::serialise(repo::core::model::RepoBSONBuilder& builder) const
 	appendFaces(builder, faces);
 	appendNormals(builder, normals);
 	appendUVChannels(builder, channels.size(), getUVChannelsSerialised());
+
+	auto matProps = determineMaterialProperties(material, textureId, getNumUVChannels());
+	appendFilterTags(builder, grouping, matProps, textureId);	
+}
+
+MeshNode::MaterialProperties repo::core::model::MeshNode::getMaterialProperties() const
+{
+	return determineMaterialProperties(material, textureId, getNumUVChannels());
 }
 
 std::vector<repo::lib::RepoVector2D> MeshNode::getUVChannelsSerialised() const
diff --git a/bouncer/src/repo/core/model/bson/repo_node_mesh.h b/bouncer/src/repo/core/model/bson/repo_node_mesh.h
index e7b5b48f..875ef561 100644
--- a/bouncer/src/repo/core/model/bson/repo_node_mesh.h
+++ b/bouncer/src/repo/core/model/bson/repo_node_mesh.h
@@ -75,6 +75,13 @@ namespace repo {
 					QUADS = 4
 				};
 
+				enum class MaterialProperties {
+					UNSET = 0,
+					OPAQUEMAT = 1,
+					TRANSPARENTMAT = 2,
+					TEXTUREDMAT = 3
+				};
+
 				/**
 				* Default constructor
 				*/
@@ -126,7 +133,6 @@ namespace repo {
 				}
 
 			protected:
-				std::string grouping;
 				MeshNode::Primitive primitive;
 				repo::lib::RepoBounds boundingBox;
 				std::vector<repo::lib::repo_face_t> faces;
@@ -134,6 +140,13 @@ namespace repo {
 				std::vector<repo::lib::RepoVector3D> normals;
 				std::vector<std::vector<repo::lib::RepoVector2D>> channels;
 
+				// Filters for supermeshing
+				std::string grouping;
+				repo::lib::RepoUUID textureId;
+
+				// Material struct
+				repo::lib::repo_material_t material;
+
 			public:
 				/**
 				* Get the mesh primitive type (points, lines, triangles, quads) (triangles if not set).
@@ -184,6 +197,22 @@ namespace repo {
 					this->grouping = grouping;
 				}
 
+				void setMaterial(const repo::lib::repo_material_t& m) {
+					this->material = m;
+				}
+
+				const repo::lib::repo_material_t& getMaterial() {
+					return material;
+				}
+
+				void setTextureId(const repo::lib::RepoUUID textureId) {
+					this->textureId = textureId;
+				}
+
+				repo::lib::RepoUUID getTextureId() {					
+					return this->textureId;
+				}
+
 				/**
 				* --------- Convenience functions -----------
 				*/
@@ -218,12 +247,15 @@ namespace repo {
 					}
 				}
 
-				// get sepcific grouping for mesh batching (empty string if not specified)
+				// get specific grouping for mesh batching (empty string if not specified)
 				std::string getGrouping() const
 				{
 					return grouping;
 				}
 
+				// get material property
+				MaterialProperties getMaterialProperties() const;
+
 				/**
 				* Retrieve a vector of vertices from the bson object
 				*/
diff --git a/bouncer/src/repo/core/model/bson/repo_node_streaming_mesh.cpp b/bouncer/src/repo/core/model/bson/repo_node_streaming_mesh.cpp
new file mode 100644
index 00000000..f98a1ba1
--- /dev/null
+++ b/bouncer/src/repo/core/model/bson/repo_node_streaming_mesh.cpp
@@ -0,0 +1,239 @@
+/**
+*  Copyright (C) 2025 3D Repo Ltd
+*
+*  This program is free software: you can redistribute it and/or modify
+*  it under the terms of the GNU Affero General Public License as
+*  published by the Free Software Foundation, either version 3 of the
+*  License, or (at your option) any later version.
+*
+*  This program is distributed in the hope that it will be useful,
+*  but WITHOUT ANY WARRANTY; without even the implied warranty of
+*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+*  GNU Affero General Public License for more details.
+*
+*  You should have received a copy of the GNU Affero General Public License
+*  along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include "repo_node_streaming_mesh.h"
+
+repo::core::model::StreamingMeshNode::SupermeshingData::SupermeshingData(const repo::core::model::RepoBSON& bson, const std::vector<uint8_t>& buffer, const bool ignoreUVs)
+{
+	this->uniqueId = bson.getUUIDField(REPO_NODE_LABEL_ID);
+	deserialise(bson, buffer, ignoreUVs);
+}
+
+void repo::core::model::StreamingMeshNode::SupermeshingData::bakeMeshes(const repo::lib::RepoMatrix& transform)
+{
+	// Vertices
+	for (int i = 0; i < vertices.size(); i++) {
+		vertices[i] = transform * vertices[i];
+	}
+
+	// Normals
+	auto matInverse = transform.invert();
+	auto worldMat = matInverse.transpose();
+
+	auto data = worldMat.getData();
+	data[3] = data[7] = data[11] = 0;
+	data[12] = data[13] = data[14] = 0;
+
+	repo::lib::RepoMatrix multMat(data);
+
+	for (int i = 0; i < normals.size(); i++) {
+		normals[i] = multMat * normals[i];
+		normals[i].normalize();
+	}
+}
+
+void repo::core::model::StreamingMeshNode::SupermeshingData::deserialise(const repo::core::model::RepoBSON& bson, const std::vector<uint8_t>& buffer, const bool ignoreUVs)
+{
+	auto blobRefBson = bson.getObjectField(REPO_LABEL_BINARY_REFERENCE);
+	auto elementsBson = blobRefBson.getObjectField(REPO_LABEL_BINARY_ELEMENTS);
+
+	if (elementsBson.hasField(REPO_NODE_MESH_LABEL_VERTICES)) {
+		auto vertBson = elementsBson.getObjectField(REPO_NODE_MESH_LABEL_VERTICES);
+		deserialiseVector(vertBson, buffer, vertices);
+	}
+
+	if (elementsBson.hasField(REPO_NODE_MESH_LABEL_NORMALS)) {
+		auto normBson = elementsBson.getObjectField(REPO_NODE_MESH_LABEL_NORMALS);
+		deserialiseVector(normBson, buffer, normals);
+	}
+
+	if (elementsBson.hasField(REPO_NODE_MESH_LABEL_FACES)) {
+
+		int32_t faceCount = bson.getIntField(REPO_NODE_MESH_LABEL_FACES_COUNT);
+		faces.reserve(faceCount);
+
+		std::vector<uint32_t> serialisedFaces = std::vector<uint32_t>();
+		auto faceBson = elementsBson.getObjectField(REPO_NODE_MESH_LABEL_FACES);
+		deserialiseVector(faceBson, buffer, serialisedFaces);
+
+		// Retrieve numbers of vertices for each face and subsequent
+		// indices into the vertex array.
+		// In API level 1, mesh is represented as
+		// [n1, v1, v2, ..., n2, v1, v2...]
+
+		int mNumIndicesIndex = 0;
+		while (serialisedFaces.size() > mNumIndicesIndex)
+		{
+			int mNumIndices = serialisedFaces[mNumIndicesIndex];
+			if (serialisedFaces.size() > mNumIndicesIndex + mNumIndices)
+			{
+				repo::lib::repo_face_t face;
+				face.resize(mNumIndices);
+				for (int i = 0; i < mNumIndices; ++i)
+					face[i] = serialisedFaces[mNumIndicesIndex + 1 + i];
+				faces.push_back(face);
+				mNumIndicesIndex += mNumIndices + 1;
+			}
+			else
+			{
+				repoError << "Cannot copy all faces. Buffer size is smaller than expected!";
+			}
+		}
+
+	}
+
+	if (!ignoreUVs && elementsBson.hasField(REPO_NODE_MESH_LABEL_UV_CHANNELS)) {
+		std::vector<repo::lib::RepoVector2D> serialisedChannels;
+		auto uvBson = elementsBson.getObjectField(REPO_NODE_MESH_LABEL_UV_CHANNELS);
+		deserialiseVector(uvBson, buffer, serialisedChannels);
+
+		if (serialisedChannels.size())
+		{
+			//get number of channels and split the serialised.
+			uint32_t nChannels = bson.getIntField(REPO_NODE_MESH_LABEL_UV_CHANNELS_COUNT);
+			uint32_t vecPerChannel = serialisedChannels.size() / nChannels;
+			channels.reserve(nChannels);
+			for (uint32_t i = 0; i < nChannels; i++)
+			{
+				channels.push_back(std::vector<repo::lib::RepoVector2D>());
+				channels[i].reserve(vecPerChannel);
+
+				uint32_t offset = i * vecPerChannel;
+				channels[i].insert(channels[i].begin(), serialisedChannels.begin() + offset,
+					serialisedChannels.begin() + offset + vecPerChannel);
+			}
+		}
+	}
+}
+
+repo::core::model::StreamingMeshNode::StreamingMeshNode(const repo::core::model::RepoBSON& bson)
+{
+	if (bson.hasField(REPO_NODE_LABEL_SHARED_ID)) {
+		sharedId = bson.getUUIDField(REPO_NODE_LABEL_SHARED_ID);
+	}
+	if (bson.hasField(REPO_NODE_MESH_LABEL_VERTICES_COUNT)) {
+		numVertices = bson.getIntField(REPO_NODE_MESH_LABEL_VERTICES_COUNT);
+	}
+	if (bson.hasField(REPO_NODE_LABEL_PARENTS)) {
+		auto parents = bson.getUUIDFieldArray(REPO_NODE_LABEL_PARENTS);
+		parent = parents[0];
+	}
+	if (bson.hasField(REPO_NODE_MESH_LABEL_BOUNDING_BOX)) {
+		bounds = bson.getBoundsField(REPO_NODE_MESH_LABEL_BOUNDING_BOX);
+	}
+}
+
+void repo::core::model::StreamingMeshNode::loadSupermeshingData(const repo::core::model::RepoBSON& bson, const std::vector<uint8_t>& buffer, const bool ignoreUVs)
+{
+	if (supermeshingDataLoaded())
+	{
+		repoWarning << "StreamingMeshNode instructed to load geometry data, but geometry data is already loaded.";
+		unloadSupermeshingData();
+	}
+
+	supermeshingData = std::make_unique<SupermeshingData>(bson, buffer, ignoreUVs);
+}
+
+void repo::core::model::StreamingMeshNode::transformBounds(const repo::lib::RepoMatrix& transform)
+{
+	auto newMinBound = transform * bounds.min();
+	auto newMaxBound = transform * bounds.max();
+	bounds = repo::lib::RepoBounds(newMinBound, newMaxBound);
+}
+
+const repo::lib::RepoUUID repo::core::model::StreamingMeshNode::getUniqueId()
+{
+	if (supermeshingDataLoaded()) {
+		return supermeshingData->getUniqueId();
+	}
+	else {
+		repoError << "Tried to access supermesh geometry of StreamingMeshNode without loading geometry first. Empty returned.";
+		return repo::lib::RepoUUID();
+	}
+}
+
+const std::uint32_t repo::core::model::StreamingMeshNode::getNumLoadedFaces() {
+	if (supermeshingDataLoaded()) {
+		return supermeshingData->getNumFaces();
+	}
+	else {
+		repoError << "Tried to access supermesh geometry of StreamingMeshNode without loading geometry first. Empty returned.";
+		return 0;
+	}
+}
+
+const std::vector<repo::lib::repo_face_t>& repo::core::model::StreamingMeshNode::getLoadedFaces()
+{
+	if (supermeshingDataLoaded()) {
+		return supermeshingData->getFaces();
+	}
+	else {
+		repoError << "Tried to access supermesh geometry of StreamingMeshNode without loading geometry first. Empty returned.";
+		return emptyFace;
+	}
+}
+
+const std::uint32_t repo::core::model::StreamingMeshNode::getNumLoadedVertices() {
+	if (supermeshingDataLoaded()) {
+		return supermeshingData->getNumVertices();
+	}
+	else {
+		repoError << "Tried to access supermesh geometry of StreamingMeshNode without loading geometry first. Empty returned.";
+		return 0;
+	}
+}
+
+const std::vector<repo::lib::RepoVector3D>& repo::core::model::StreamingMeshNode::getLoadedVertices() {
+	if (supermeshingDataLoaded()) {
+		return supermeshingData->getVertices();
+	}
+	else {
+		repoError << "Tried to access supermesh geometry of StreamingMeshNode without loading geometry first. Empty returned.";
+		return empty3D;
+	}
+}
+
+void repo::core::model::StreamingMeshNode::bakeLoadedMeshes(const repo::lib::RepoMatrix& transform) {
+	if (supermeshingDataLoaded()) {
+		supermeshingData->bakeMeshes(transform);
+	}
+	else {
+		repoError << "Tried to access supermesh geometry of StreamingMeshNode without loading geometry first. No action performed.";
+	}
+}
+
+const std::vector<repo::lib::RepoVector3D>& repo::core::model::StreamingMeshNode::getLoadedNormals()
+{
+	if (supermeshingDataLoaded()) {
+		return supermeshingData->getNormals();
+	}
+	else {
+		repoError << "Tried to access supermesh geometry of StreamingMeshNode without loading geometry first. Empty returned.";
+		return empty3D;
+	}
+}
+
+const std::vector<std::vector<repo::lib::RepoVector2D>>& repo::core::model::StreamingMeshNode::getLoadedUVChannelsSeparated()
+{
+	if (supermeshingDataLoaded()) {
+		return supermeshingData->getUVChannelsSeparated();
+	}
+	else {
+		repoError << "Tried to access supermesh geometry of StreamingMeshNode without loading geometry first. Empty returned.";
+		return emptyUV;
+	}
+}
diff --git a/bouncer/src/repo/core/model/bson/repo_node_streaming_mesh.h b/bouncer/src/repo/core/model/bson/repo_node_streaming_mesh.h
new file mode 100644
index 00000000..6884cbfb
--- /dev/null
+++ b/bouncer/src/repo/core/model/bson/repo_node_streaming_mesh.h
@@ -0,0 +1,172 @@
+/**
+*  Copyright (C) 2025 3D Repo Ltd
+*
+*  This program is free software: you can redistribute it and/or modify
+*  it under the terms of the GNU Affero General Public License as
+*  published by the Free Software Foundation, either version 3 of the
+*  License, or (at your option) any later version.
+*
+*  This program is distributed in the hope that it will be useful,
+*  but WITHOUT ANY WARRANTY; without even the implied warranty of
+*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+*  GNU Affero General Public License for more details.
+*
+*  You should have received a copy of the GNU Affero General Public License
+*  along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#pragma once
+#include <repo/lib/datastructure/repo_uuid.h>
+#include <repo/lib/datastructure/repo_vector.h>
+#include <repo/lib/datastructure/repo_structs.h>
+#include <repo/lib/datastructure/repo_matrix.h>
+#include <repo/core/model/bson/repo_bson.h>
+#include <repo/core/model/bson/repo_node_mesh.h>
+
+namespace repo {
+	namespace core {
+		namespace model {
+			class StreamingMeshNode {
+
+				class SupermeshingData {
+
+					repo::lib::RepoUUID uniqueId;
+
+					// Geometry
+					std::vector<repo::lib::RepoVector3D> vertices;
+					std::vector<repo::lib::repo_face_t> faces;
+					std::vector<repo::lib::RepoVector3D> normals;
+					std::vector<std::vector<repo::lib::RepoVector2D>> channels;
+
+				public:
+					SupermeshingData(
+						const repo::core::model::RepoBSON& bson,
+						const std::vector<uint8_t>& buffer,
+						const bool ignoreUVs);
+
+					repo::lib::RepoUUID getUniqueId() const {
+						return uniqueId;
+					}
+
+					std::uint32_t getNumFaces() const {
+						return faces.size();
+					}
+					std::vector<repo::lib::repo_face_t>& getFaces()
+					{
+						return faces;
+					}
+
+					std::uint32_t getNumVertices() const {
+						return vertices.size();
+					}
+					const std::vector<repo::lib::RepoVector3D>& getVertices() const {
+						return vertices;
+					}
+
+					void bakeMeshes(const repo::lib::RepoMatrix& transform);
+
+					const std::vector<repo::lib::RepoVector3D>& getNormals() const
+					{
+						return normals;
+					}
+
+					const std::vector<std::vector<repo::lib::RepoVector2D>>& getUVChannelsSeparated() const
+					{
+						return channels;
+					}
+
+				private:
+					void deserialise(
+						const repo::core::model::RepoBSON& bson,
+						const std::vector<uint8_t>& buffer,
+						const bool ignoreUVs);
+
+					template <class T>
+					void deserialiseVector(
+						const repo::core::model::RepoBSON& bson,
+						const std::vector<uint8_t>& buffer,
+						std::vector<T>& vec)
+					{
+						auto start = bson.getIntField(REPO_LABEL_BINARY_START);
+						auto size = bson.getIntField(REPO_LABEL_BINARY_SIZE);
+
+						vec.resize(size / sizeof(T));
+						memcpy(vec.data(), buffer.data() + (sizeof(uint8_t) * start), size);
+					}
+				};
+
+				repo::lib::RepoUUID sharedId;
+				std::uint32_t numVertices = 0;
+				repo::lib::RepoUUID parent;
+				repo::lib::RepoBounds bounds;
+
+				std::unique_ptr<SupermeshingData> supermeshingData;
+
+				// Empty vectors as standins when the supermeshingData is not loaded
+				std::vector<repo::lib::repo_face_t> emptyFace;
+				std::vector<repo::lib::RepoVector3D> empty3D;
+				std::vector<std::vector<repo::lib::RepoVector2D>> emptyUV;
+
+			public:
+				StreamingMeshNode()
+				{
+					// Default constructor so instances can be initialised for vectors
+				}
+
+				StreamingMeshNode(const repo::core::model::RepoBSON& bson);
+
+				bool supermeshingDataLoaded() {
+					return supermeshingData != nullptr;
+				}
+
+				void loadSupermeshingData(
+					const repo::core::model::RepoBSON& bson,
+					const std::vector<uint8_t>& buffer,
+					const bool ignoreUVs);
+
+				void unloadSupermeshingData() {
+					supermeshingData.reset();
+				}
+
+				const repo::lib::RepoUUID getSharedId() const {
+					return sharedId;
+				}
+
+				const std::uint32_t getNumVertices() const
+				{
+					return numVertices;
+				}
+
+				const repo::lib::RepoBounds getBoundingBox() const
+				{
+					return bounds;
+				}
+
+				const repo::lib::RepoUUID getParent() const
+				{
+					return parent;
+				}
+
+				void transformBounds(const repo::lib::RepoMatrix& transform);
+
+				// Requiring the supermeshing data to be loaded
+
+				const repo::lib::RepoUUID getUniqueId();
+
+				const std::uint32_t getNumLoadedFaces();
+
+				const std::vector<repo::lib::repo_face_t>& getLoadedFaces();
+
+				const std::uint32_t getNumLoadedVertices();
+
+				const std::vector<repo::lib::RepoVector3D>& getLoadedVertices();
+
+				void bakeLoadedMeshes(const repo::lib::RepoMatrix& transform);
+
+				const std::vector<repo::lib::RepoVector3D>& getLoadedNormals();
+
+				const std::vector<std::vector<repo::lib::RepoVector2D>>& getLoadedUVChannelsSeparated();
+			};
+		}
+	}
+}
\ No newline at end of file
diff --git a/bouncer/src/repo/core/model/collection/repo_scene.cpp b/bouncer/src/repo/core/model/collection/repo_scene.cpp
index 1b64ca0c..80876c12 100644
--- a/bouncer/src/repo/core/model/collection/repo_scene.cpp
+++ b/bouncer/src/repo/core/model/collection/repo_scene.cpp
@@ -1136,7 +1136,7 @@ bool RepoScene::loadScene(
 	}
 
 	std::vector<RepoBSON> nodes = handler->findAllByCriteria(
-		databaseName, projectName + "." + REPO_COLLECTION_SCENE, core::handler::database::query::Eq(REPO_NODE_STASH_REF, revNode->getUniqueID())
+		databaseName, projectName + "." + REPO_COLLECTION_SCENE, core::handler::database::query::Eq(REPO_NODE_STASH_REF, revNode->getUniqueID()), true
 	);
 
 	repoInfo << "# of nodes in this unoptimised scene = " << nodes.size();
diff --git a/bouncer/src/repo/core/model/repo_model_global.h b/bouncer/src/repo/core/model/repo_model_global.h
index bb837b54..5aa2a57a 100644
--- a/bouncer/src/repo/core/model/repo_model_global.h
+++ b/bouncer/src/repo/core/model/repo_model_global.h
@@ -189,6 +189,7 @@
 #define REPO_NODE_STASH_REF             "rev_id"
 #define REPO_NODE_LABEL_EXTENSION		"extension"
 #define REPO_NODE_LABEL_FORMAT			"format"
+#define REPO_NODE_LABEL_MATRIX			"matrix"
 
 //-----------------------------------------------------------------------------
 
@@ -221,4 +222,17 @@
 #define REPO_ASSETS_LABEL_PRIMITIVE		"primitive"
 #define REPO_ASSETS_LABEL_MIN			"min"
 #define REPO_ASSETS_LABEL_MAX			"max"
-#define REPO_ASSETS_LABEL_METADATA      "metadata"
\ No newline at end of file
+#define REPO_ASSETS_LABEL_METADATA      "metadata"
+
+//-----------------------------------------------------------------------------
+//
+// Filter tags
+//
+//-----------------------------------------------------------------------------
+#define REPO_FILTER_OBJECT_NAME			"materialProperties"
+#define REPO_FILTER_PROP_OPAQUE			"isOpaque"
+#define REPO_FILTER_PROP_TRANSPARENT	"isTransparent"
+#define REPO_FILTER_PROP_TEXTURE_ID		"textureId"
+#define REPO_FILTER_TAG_OPAQUE			"materialProperties.isOpaque"
+#define REPO_FILTER_TAG_TRANSPARENT		"materialProperties.isTransparent"
+#define REPO_FILTER_TAG_TEXTURE_ID		"materialProperties.textureId"
diff --git a/bouncer/src/repo/lib/datastructure/repo_structs.h b/bouncer/src/repo/lib/datastructure/repo_structs.h
index 54c4286d..2d20957b 100644
--- a/bouncer/src/repo/lib/datastructure/repo_structs.h
+++ b/bouncer/src/repo/lib/datastructure/repo_structs.h
@@ -38,19 +38,6 @@ namespace repo {
 			repo::core::model::RepoAssets repoAssets; //RepoBundles assets list
 		};
 
-		//This is used to map info for multipart optimization
-		typedef struct {
-			repo::lib::RepoVector3D min;
-			repo::lib::RepoVector3D max;
-			repo::lib::RepoUUID  mesh_id;
-			repo::lib::RepoUUID  shared_id;
-			repo::lib::RepoUUID  material_id; // MaterialNode Unique Id
-			int32_t       vertFrom;
-			int32_t       vertTo;
-			int32_t       triFrom;
-			int32_t       triTo;
-		}repo_mesh_mapping_t;
-
 		struct repo_mesh_entry_t
 		{
 			std::vector<float> min;
@@ -303,6 +290,21 @@ namespace repo {
 			}
 		};
 
+		//This is used to map info for multipart optimization		
+		typedef struct {
+			repo::lib::RepoVector3D min;
+			repo::lib::RepoVector3D max;
+			repo::lib::RepoUUID  mesh_id;
+			repo::lib::RepoUUID  shared_id;
+			repo::lib::RepoUUID  material_id; // MaterialNode Unique Id
+			repo::lib::repo_material_t material;
+			repo::lib::RepoUUID  texture_id;
+			int32_t       vertFrom;
+			int32_t       vertTo;
+			int32_t       triFrom;
+			int32_t       triTo;
+		}repo_mesh_mapping_t;
+
 		/*
 		* A single object intended to be passed by value type that expresses a face with
 		* up to three indices. In the future we could consider templating this type for
diff --git a/bouncer/src/repo/manipulator/modelconvertor/export/CMakeLists.txt b/bouncer/src/repo/manipulator/modelconvertor/export/CMakeLists.txt
index dd75fe49..99f91c92 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/export/CMakeLists.txt
+++ b/bouncer/src/repo/manipulator/modelconvertor/export/CMakeLists.txt
@@ -8,12 +8,10 @@ add_subdirectory(auxiliary)
 set(SOURCES
 	${SOURCES}
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_model_export_abstract.cpp
-	${CMAKE_CURRENT_SOURCE_DIR}/repo_model_export_web.cpp
 	CACHE STRING "SOURCES" FORCE)
 
 set(HEADERS
 	${HEADERS}
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_model_export_abstract.h
-	${CMAKE_CURRENT_SOURCE_DIR}/repo_model_export_web.h
 	CACHE STRING "HEADERS" FORCE)
 
diff --git a/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_abstract.cpp b/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_abstract.cpp
index a353cab3..9961812c 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_abstract.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_abstract.cpp
@@ -24,8 +24,16 @@
 using namespace repo::manipulator::modelconvertor;
 
 AbstractModelExport::AbstractModelExport(
-	const repo::core::model::RepoScene *scene)
-	: scene(scene)
+	repo::core::handler::AbstractDatabaseHandler* dbHandler,
+	const std::string databaseName,
+	const std::string projectName,
+	const repo::lib::RepoUUID revId,
+	const std::vector<double> worldOffset)
+	: dbHandler(dbHandler),
+	databaseName(databaseName),
+	projectName(projectName),
+	revId(revId),
+	worldOffset(worldOffset)
 {
 }
 
diff --git a/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_abstract.h b/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_abstract.h
index b1d99b66..1b805588 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_abstract.h
+++ b/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_abstract.h
@@ -24,34 +24,55 @@
 #include <string>
 
 #include "../../../core/model/collection/repo_scene.h"
+#include <repo/core/model/bson/repo_bson_factory.h>
 
 namespace repo{
 	namespace manipulator{
 		namespace modelconvertor{
+
+			enum class ExportType { REPO };
+
 			class AbstractModelExport
 			{
 			public:
+
 				/**
 				* Default Constructor, export model with default settings
-				*/
-				AbstractModelExport(const repo::core::model::RepoScene *scene);
+				*/				
+				AbstractModelExport(
+					repo::core::handler::AbstractDatabaseHandler* dbHandler,
+					const std::string databaseName,
+					const std::string projectName,
+					const repo::lib::RepoUUID revId,
+					const std::vector<double> worldOffset);
 
 				/**
 				* Default Deconstructor
 				*/
 				virtual ~AbstractModelExport();
+								
+				/**
+				* Exports supermesh to file and adds its information to the ongoing export process.
+				* @param a pointer to the supermesh
+				*/
+				virtual void addSupermesh(repo::core::model::SupermeshNode* supermesh) = 0;
 
 				/**
-				* Export a repo scene graph to file
-				* @param scene repo scene representation
-				* @param filePath path to destination file
-				* @return returns true upon success
+				* Finalises the export by writing out the metadata and mapping information collected
+				* during the ongoing export process.
 				*/
-				virtual bool exportToFile(
-					const std::string &filePath) = 0; //FIXME: this shoudl be const, but it requires quite a major refactoring on assimp export
+				virtual void finalise() = 0;
 
 			protected:
-				const repo::core::model::RepoScene *scene;
+
+				// Database handler
+				repo::core::handler::AbstractDatabaseHandler* dbHandler;
+
+				// Model info
+				std::string databaseName;
+				std::string projectName;
+				repo::lib::RepoUUID revId;
+				std::vector<double> worldOffset;
 			};
 		} //namespace modelconvertor
 	} //namespace manipulator
diff --git a/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_web.cpp b/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_web.cpp
deleted file mode 100644
index 9b215a38..00000000
--- a/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_web.cpp
+++ /dev/null
@@ -1,135 +0,0 @@
-/**
-*  Copyright (C) 2016 3D Repo Ltd
-*
-*  This program is free software: you can redistribute it and/or modify
-*  it under the terms of the GNU Affero General Public License as
-*  published by the Free Software Foundation, either version 3 of the
-*  License, or (at your option) any later version.
-*
-*  This program is distributed in the hope that it will be useful,
-*  but WITHOUT ANY WARRANTY; without even the implied warranty of
-*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-*  GNU Affero General Public License for more details.
-*
-*  You should have received a copy of the GNU Affero General Public License
-*  along with this program.  If not, see <http://www.gnu.org/licenses/>.
-*/
-
-/**
-* Allows Export functionality from 3D Repo World to SRC
-*/
-
-#include <repo_log.h>
-#include "repo_model_export_web.h"
-#include "repo/core/model/bson/repo_bson_factory.h"
-
-#include <boost/filesystem.hpp>
-
-using namespace repo::lib;
-using namespace repo::manipulator::modelconvertor;
-
-WebModelExport::WebModelExport(
-	const repo::core::model::RepoScene *scene
-	) : AbstractModelExport(scene)
-{
-	//We don't cache reference scenes
-	if (convertSuccess = scene && !scene->getAllReferences(repo::core::model::RepoScene::GraphType::DEFAULT).size())
-	{
-		if (scene->hasRoot(repo::core::model::RepoScene::GraphType::OPTIMIZED))
-		{
-			gType = repo::core::model::RepoScene::GraphType::OPTIMIZED;
-		}
-		else if (scene->hasRoot(repo::core::model::RepoScene::GraphType::DEFAULT))
-		{
-			gType = repo::core::model::RepoScene::GraphType::DEFAULT;
-		}
-		else
-		{
-			repoError << "Failed to export to scene : Failed to find root node within the scene!";
-			convertSuccess = false;
-		}
-	}
-}
-
-WebModelExport::~WebModelExport()
-{
-}
-
-bool WebModelExport::exportToFile(
-	const std::string &filePath)
-{
-	if (!convertSuccess) {
-		return convertSuccess;
-	}
-
-	const repo_web_buffers_t buffers = getAllFilesExportedAsBuffer();
-
-	boost::filesystem::path boostPath(filePath);
-
-	for (const auto &buff : buffers.geoFiles)
-	{
-		std::string fname = sanitizeFileName(buff.first);
-		boostPath /= fname;
-		FILE* fp = fopen(boostPath.string().c_str(), "wb");
-		if (fp)
-		{
-			fwrite(buff.second.data(), sizeof(*buff.second.data()), buff.second.size(), fp);
-			fclose(fp);
-		}
-		else
-		{
-			repoError << "Failed to open file for writing: " << fname;
-		}
-	}
-
-	for (const auto &buff : buffers.jsonFiles)
-	{
-		std::string fname = sanitizeFileName(buff.first);
-		boostPath /= fname;
-		FILE* fp = fopen(boostPath.string().c_str(), "wb");
-		if (fp)
-		{
-			fwrite(buff.second.data(), sizeof(*buff.second.data()), buff.second.size(), fp);
-			fclose(fp);
-		}
-		else
-		{
-			repoError << "Failed to open file for writing: " << fname;
-		}
-	}
-
-	return convertSuccess;
-}
-
-std::unordered_map<std::string, std::vector<uint8_t>> WebModelExport::getJSONFilesAsBuffer() const
-{
-	std::unordered_map < std::string, std::vector<uint8_t> > fileBuffers;
-
-	for (const auto &treePair : jsonTrees)
-	{
-		std::stringstream ss;
-		treePair.second.write_json(ss);
-		std::string jsonStr = ss.str();
-
-		fileBuffers[treePair.first] = std::vector<uint8_t>();
-		fileBuffers[treePair.first].resize(jsonStr.size());
-		memcpy(fileBuffers[treePair.first].data(), jsonStr.c_str(), jsonStr.size());
-	}
-
-	return fileBuffers;
-}
-
-std::string WebModelExport::getSupportedFormats()
-{
-	return ".src, .repobundle";
-}
-
-std::string WebModelExport::sanitizeFileName(
-	const std::string &name) const
-{
-	std::string res = name;
-	std::replace(res.begin(), res.end(), '\\', '_');
-	std::replace(res.begin(), res.end(), '/', '_');
-
-	return res;
-}
\ No newline at end of file
diff --git a/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_web.h b/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_web.h
deleted file mode 100644
index 757ccdee..00000000
--- a/bouncer/src/repo/manipulator/modelconvertor/export/repo_model_export_web.h
+++ /dev/null
@@ -1,99 +0,0 @@
-/**
-*  Copyright (C) 2015 3D Repo Ltd
-*
-*  This program is free software: you can redistribute it and/or modify
-*  it under the terms of the GNU Affero General Public License as
-*  published by the Free Software Foundation, either version 3 of the
-*  License, or (at your option) any later version.
-*
-*  This program is distributed in the hope that it will be useful,
-*  but WITHOUT ANY WARRANTY; without even the implied warranty of
-*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-*  GNU Affero General Public License for more details.
-*
-*  You should have received a copy of the GNU Affero General Public License
-*  along with this program.  If not, see <http://www.gnu.org/licenses/>.
-*/
-
-/**
-* Allows Export functionality from 3D Repo World to containers suitable
-* for dynamic loading over the web.
-*/
-
-#pragma once
-
-#include <string>
-
-#include "repo_model_export_abstract.h"
-#include "../../../lib/repo_property_tree.h"
-#include "../../../lib/datastructure/repo_structs.h"
-#include "../../../core/model/collection/repo_scene.h"
-
-namespace repo{
-	namespace manipulator{
-		namespace modelconvertor{
-			enum class WebExportType { REPO };
-
-			class WebModelExport : public AbstractModelExport
-			{
-			public:
-				/**
-				* Default Constructor, export model with default settings
-				* @param scene repo scene to convert
-				*/
-				WebModelExport(const repo::core::model::RepoScene *scene);
-
-				/**
-				* Default Destructor
-				*/
-				virtual ~WebModelExport();
-
-				/**
-				* Export the repo scene graph to file
-				* @param filePath path to destination file
-				* @return returns true upon success
-				*/
-				virtual bool exportToFile(
-					const std::string &filePath);
-
-				/**
-				* Export all necessary files as buffers
-				* @return returns a repo_src_export_t containing all files needed for this
-				*          model to be rendered
-				*/
-				virtual repo::lib::repo_web_buffers_t getAllFilesExportedAsBuffer() const = 0;
-
-				/**
-				* Return the JSON file as raw bytes buffer
-				* returns an empty map if the export has failed
-				*/
-				virtual std::unordered_map<std::string, std::vector<uint8_t>> getJSONFilesAsBuffer() const;
-
-				/**
-				* Get supported file formats for this exporter
-				*/
-				static std::string getSupportedFormats();
-
-				/**
-				* Returns the status of the converter,
-				* whether it has successfully converted the model
-				* @return returns true if success
-				*/
-				bool isOk() const
-				{
-					return convertSuccess;
-				}
-
-			protected:
-				bool convertSuccess;
-				repo::core::model::RepoScene::GraphType gType;
-				std::unordered_map<std::string, repo::lib::PropertyTree> trees;
-				std::unordered_map<std::string, repo::lib::PropertyTree> jsonTrees;
-
-			private:
-				std::string sanitizeFileName(
-					const std::string &name) const;
-			};
-		} //namespace modelconvertor
-	} //namespace manipulator
-} //namespace repo
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/odaHelper/data_processor_nwd.cpp b/bouncer/src/repo/manipulator/modelconvertor/import/odaHelper/data_processor_nwd.cpp
index b73835ef..83206f11 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/odaHelper/data_processor_nwd.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/odaHelper/data_processor_nwd.cpp
@@ -798,8 +798,8 @@ OdResult processGeometry(OdNwModelItemPtr pNode, RepoNwTraversalContext context)
 	meshBuilder.extractMeshes(nodes);
 	for (auto& mesh : nodes)
 	{
+		mesh.setMaterial(meshBuilder.getMaterial());
 		context.sceneBuilder->addNode(mesh);
-		context.sceneBuilder->addMaterialReference(meshBuilder.getMaterial(), mesh.getSharedID());
 	}
 
 	return eOk;
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/odaHelper/geometry_collector.cpp b/bouncer/src/repo/manipulator/modelconvertor/import/odaHelper/geometry_collector.cpp
index c5f03b71..b6184117 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/odaHelper/geometry_collector.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/odaHelper/geometry_collector.cpp
@@ -136,8 +136,8 @@ void GeometryCollector::addMeshes(std::string id, std::vector<std::pair<repo::co
 	auto parent = getSharedId(id);
 	for (auto& p : meshes) {
 		p.first.setParents({ parent });
+		p.first.setMaterial(p.second);
 		addNode(p.first);
-		addMaterialReference(p.second, p.first.getSharedID());
 	}
 }
 
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_3drepo.cpp b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_3drepo.cpp
index 592afc3d..880d80e6 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_3drepo.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_3drepo.cpp
@@ -172,7 +172,7 @@ public:
 		minBufferSize(0),
 		numMaterials(0)
 	{
-		createIndexes();
+		createIndexes(false);
 	}
 
 	struct Ids
@@ -191,6 +191,7 @@ public:
 	std::unordered_map<int, repo::lib::RepoMatrix64> modelToWorld;
 	std::unordered_map<int, Ids> materialIds;
 	std::unordered_map<int, Ids> textureIds;
+	std::unordered_map<repo::lib::RepoUUID, std::vector<std::shared_ptr<repo::core::model::MeshNode>>, repo::lib::RepoUUIDHasher> matToMeshNodes;	
 	size_t numMaterials;
 	std::vector<View*> dataMap;
 	size_t minBufferSize;
@@ -261,7 +262,7 @@ public:
 			node->setUniqueID(repo::lib::RepoUUID::createUUID());
 			node->setSharedID(repo::lib::RepoUUID::createUUID());
 			node->setParents(parentIds);
-
+			
 			if (!isEntity) {
 				parentIds = { node->getSharedID() };
 			}
@@ -281,7 +282,10 @@ public:
 			}
 
 			auto& material = materialIds[r.geometry.material];
-			references.push_back({ material.uniqueId, node->getSharedID() });
+			references.push_back({ material.uniqueId, node->getSharedID() });		
+
+			auto& nodes = matToMeshNodes[material.uniqueId];
+			nodes.push_back(node);			
 		}
 
 		if (r.metadata.size())
@@ -300,10 +304,23 @@ public:
 		n.setUniqueID(ids.uniqueId);
 		n.setSharedID(ids.sharedId);
 		addNode(n);
+				
+		auto matStruct = n.getMaterialStruct();
+		auto& nodes = matToMeshNodes[ids.uniqueId];
+		for (auto& node : nodes) {
+			node->setMaterial(matStruct);
+		}
 
 		if (texture != -1) {
-			references.push_back({ textureIds[texture].uniqueId, ids.sharedId });
+			auto texId = textureIds[texture].uniqueId;
+			references.push_back({ texId, ids.sharedId });
+			for (auto& node : nodes) {
+				node->setTextureId(texId);
+			}
 		}
+		
+		// Remove entry from the map to release nodes
+		matToMeshNodes.erase(ids.uniqueId);		
 	}
 
 	void createTexture(const TextureRecord& t) override
@@ -324,6 +341,11 @@ public:
 		for (auto view : dataMap) {
 			minBufferSize = std::max(minBufferSize, view->size());
 		}
+
+		// Release all node references in the map for the material properties.
+		// They should already have been released individually, but this guarantees
+		// that none are missed.
+		matToMeshNodes.clear();
 	}
 
 	void readView(View* v, char* buffer)
@@ -435,6 +457,7 @@ public:
 		for (auto& r : references) {
 			addParent(r.first, r.second);
 		}
+
 		RepoSceneBuilder::finalise();
 	}
 
@@ -514,7 +537,7 @@ repo::core::model::RepoScene* RepoModelImport::importModel(std::string filePath,
 		TreeParser::ParseJson(jsonBuf, builder);
 
 		builder->prepareDataMap();
-
+		
 		repoInfo << "Pre-processing scene bounds (BIM004 and below)...";
 
 		// Get offset
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_assimp.cpp b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_assimp.cpp
index 51675bf4..e595ee08 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_assimp.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_assimp.cpp
@@ -408,6 +408,7 @@ repo::core::model::MeshNode AssimpModelImport::createMeshRepoNode(
 	const std::vector<repo::core::model::RepoNode *> &materials,
 	std::unordered_map < repo::core::model::RepoNode*, std::vector<repo::lib::RepoUUID>> &matMap,
 	const bool hasTexture,
+	const repo::lib::RepoUUID& texId,
 	const std::vector<double> &offset)
 {
 	repo::core::model::MeshNode meshNode;
@@ -519,6 +520,15 @@ repo::core::model::MeshNode AssimpModelImport::createMeshRepoNode(
 	{
 		repo::core::model::RepoNode *materialNode = materials[assimpMesh->mMaterialIndex];
 		matMap[materialNode].push_back(meshNode.getSharedID());
+
+		// Shim to enable the new multipart optimiser to read Assimp models until this importer
+		// is refactored to use the scene builder.
+		auto matNode = ((repo::core::model::MaterialNode*)materialNode);
+		auto material = matNode->getMaterialStruct();
+		meshNode.setMaterial(material);
+				
+		if(hasTexture)
+			meshNode.setTextureId(texId);		
 	}
 
 	///*
@@ -869,11 +879,26 @@ repo::core::model::RepoScene* AssimpModelImport::convertAiSceneToRepoScene()
 					repoInfo << "Constructing " << i << " of " << assimpScene->mNumMeshes;
 				}
 
-				int numTextures = assimpScene->mMaterials[assimpScene->mMeshes[i]->mMaterialIndex]->GetTextureCount(aiTextureType_DIFFUSE);
+				auto aiMaterial = assimpScene->mMaterials[assimpScene->mMeshes[i]->mMaterialIndex];
+				int numTextures = aiMaterial->GetTextureCount(aiTextureType_DIFFUSE);
+
+				repo::lib::RepoUUID texId;
+				if (numTextures > 0) {
+					aiString texPath;
+					aiMaterial->GetTexture(aiTextureType_DIFFUSE, 0, &texPath);
+					auto texture = nameToTexture.find(texPath.data);
+
+					if (nameToTexture.end() != texture)
+					{
+						texId = texture->second->getUniqueID();
+					}
+				}
+				
 				auto mesh = createMeshRepoNode(
 					assimpScene->mMeshes[i],
 					originalOrderMaterial,
 					matParents, numTextures > 0,
+					texId,
 					sceneBbox.size() ? sceneBbox[0] : std::vector<double>());
 
 				if (!mesh.getNumVertices())
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_assimp.h b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_assimp.h
index a9b3f732..30a67e83 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_assimp.h
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_assimp.h
@@ -115,6 +115,7 @@ namespace repo {
 					const std::vector<repo::core::model::RepoNode *> &materials,
 					std::unordered_map < repo::core::model::RepoNode*, std::vector<repo::lib::RepoUUID>> &matMap,
 					const bool hasTexture,
+					const repo::lib::RepoUUID& texId,
 					const std::vector<double> &offset);
 
 				/**
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_ifc.cpp b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_ifc.cpp
index e382e5d7..533f88a8 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_ifc.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_ifc.cpp
@@ -52,7 +52,7 @@ repo::core::model::RepoScene* IFCModelImport::importModel(std::string filePath,
 		settings.getProjectName(),
 		settings.getRevisionId()
 		);
-	sceneBuilder->createIndexes();
+	sceneBuilder->createIndexes(false);
 
 	auto serialiser = ifcUtils::IfcUtils::CreateSerialiser(filePath);
 
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_manager.cpp b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_manager.cpp
index 5b93f377..de61faea 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_manager.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_manager.cpp
@@ -25,6 +25,10 @@
 #include "repo_model_import_synchro.h"
 #include "repo_model_units.h"
 #include <boost/filesystem.hpp>
+#include <repo/core/handler/repo_database_handler_abstract.h>
+#include <repo/core/handler/database/repo_query.h>
+#include <repo/core/model/bson/repo_bson.h>
+#include <repo/core/model/bson/repo_node.h>
 
 using namespace repo::manipulator::modelconvertor;
 
@@ -76,7 +80,18 @@ repo::core::model::RepoScene* ModelImportManager::ImportFromFile(
 
 			error = REPOERR_OK;
 		}
-		
+
+		// Apply the in-leaf node metadata shim, on the scene directly or the db,
+		// depending on whether this was a streamed import or not.
+		if (scene) {
+			repoInfo << "Connecting metadata within leaf nodes...";
+			if (scene->getAllMeshes(repo::core::model::RepoScene::GraphType::DEFAULT).size()) {
+				connectMetadataNodes(scene);
+			}
+			else {
+				connectMetadataNodes(scene, handler);
+			}
+		}
 	}
 
 	return scene;
@@ -104,4 +119,127 @@ std::shared_ptr<AbstractModelImport> ModelImportManager::chooseModelConvertor(
 		modelConvertor = std::shared_ptr<AbstractModelImport>(new repo::manipulator::modelconvertor::SynchroModelImport(config));
 
 	return modelConvertor;
-}
\ No newline at end of file
+}
+
+void ModelImportManager::connectMetadataNodes(repo::core::model::RepoScene* scene) const
+{
+	for (auto& metadata : scene->getAllMetadata(repo::core::model::RepoScene::GraphType::DEFAULT)) {
+		for (auto& parent : scene->getParentNodesFiltered(repo::core::model::RepoScene::GraphType::DEFAULT, metadata, repo::core::model::NodeType::TRANSFORMATION)) {
+
+			if (scene->getChildrenNodesFiltered(repo::core::model::RepoScene::GraphType::DEFAULT,
+				parent->getSharedID(),
+				repo::core::model::NodeType::TRANSFORMATION).size())
+			{
+				continue;
+			}
+
+			bool isLeafNode = true;
+
+			repo::core::model::RepoNodeSet meshNodes;
+
+			for (auto& mesh : scene->getChildrenNodesFiltered(repo::core::model::RepoScene::GraphType::DEFAULT,
+				parent->getSharedID(),
+				repo::core::model::NodeType::MESH))
+			{
+				if (mesh->getName().size()) {
+					isLeafNode = false;
+					break;
+				}
+
+				meshNodes.insert(mesh);
+			}
+
+			if (!isLeafNode) {
+				continue;
+			}
+
+			scene->addInheritance(repo::core::model::RepoScene::GraphType::DEFAULT,
+				meshNodes,
+				metadata);
+		}
+	}
+}
+
+void ModelImportManager::connectMetadataNodes(repo::core::model::RepoScene* scene,
+	std::shared_ptr<repo::core::handler::AbstractDatabaseHandler> handler) const
+{
+	// This is a shim that parents metadata nodes to any hidden meshes of leaf
+	// nodes for an already committed scene.
+
+	using namespace repo::core::handler::database;
+
+	query::RepoQueryBuilder filter;
+	filter.append(query::Eq(REPO_NODE_REVISION_ID, scene->getRevisionID()));
+	filter.append(query::Eq(REPO_NODE_LABEL_TYPE, {
+		std::string(REPO_NODE_TYPE_TRANSFORMATION),
+		std::string(REPO_NODE_TYPE_MESH),
+		std::string(REPO_NODE_TYPE_METADATA)
+	}));
+
+	repo::core::handler::database::query::RepoProjectionBuilder projection;
+	projection.includeField(REPO_NODE_LABEL_ID);
+	projection.includeField(REPO_NODE_LABEL_SHARED_ID);
+	projection.includeField(REPO_NODE_LABEL_PARENTS);
+	projection.includeField(REPO_NODE_LABEL_TYPE);
+	projection.includeField(REPO_NODE_LABEL_NAME);
+
+	struct TransformationNode {
+		std::set<repo::lib::RepoUUID> meshSharedIds;
+		std::set<repo::lib::RepoUUID> metadataUniqueIds;
+
+		bool isLeafNode;
+
+		TransformationNode()
+			:isLeafNode(true)
+		{
+		}
+	};
+
+	// Indexed by shared id, as is used to determine relationships within the db
+	std::unordered_map<repo::lib::RepoUUID, TransformationNode, repo::lib::RepoUUIDHasher> nodes;
+
+	auto cursor = handler->findCursorByCriteria(scene->getDatabaseName(), scene->getProjectName() + ".scene", filter, projection);
+	for (auto bson : *cursor)
+	{
+		auto node = repo::core::model::RepoNode(bson);
+
+		auto type = bson.getStringField(REPO_NODE_LABEL_TYPE);
+		if (type == REPO_NODE_TYPE_TRANSFORMATION) {
+			for (auto& p : node.getParentIDs()) {
+				auto& n = nodes[p];
+				n.isLeafNode = false; // Nodes that have transformations as children cannot be leaf nodes
+			}
+		}
+		else if (type == REPO_NODE_TYPE_MESH) {
+			for (auto& p : node.getParentIDs()) {
+				auto& n = nodes[p];
+				if (node.getName().size()) {
+					n.isLeafNode = false; // Nodes that have named meshes as children cannot be leaf nodes
+				}
+				else
+				{
+					n.meshSharedIds.insert(node.getSharedID());
+				}
+			}
+		}
+		else if (type == REPO_NODE_TYPE_METADATA) {
+			for (auto& p : node.getParentIDs()) {
+				auto& n = nodes[p];
+				n.metadataUniqueIds.insert(node.getUniqueID());
+			}
+		}
+	}
+
+	// The nodes map now has everything required to work out which metadata
+	// nodes need additional parents.
+
+	auto context = handler->getBulkWriteContext(scene->getDatabaseName(), scene->getProjectName() + ".scene");
+	for (auto& p : nodes) {
+		auto& t = p.second;
+		if (t.isLeafNode) {
+			for (auto& m : t.metadataUniqueIds) {
+				context->updateDocument(query::AddParent(m, t.meshSharedIds));
+			}
+		}
+	}
+}
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_manager.h b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_manager.h
index d7492113..47182186 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_manager.h
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_manager.h
@@ -44,6 +44,11 @@ namespace repo {
 					const std::string &file,
 					const repo::manipulator::modelconvertor::ModelImportConfig &config
 				) const;
+
+				void connectMetadataNodes(repo::core::model::RepoScene* scene,
+					std::shared_ptr<repo::core::handler::AbstractDatabaseHandler> handler) const;
+
+				void connectMetadataNodes(repo::core::model::RepoScene* scene) const;
 			};
 		} //namespace modelconvertor
 	} //namespace manipulator
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_oda.cpp b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_oda.cpp
index cc647ba3..9a7ddd27 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_oda.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_oda.cpp
@@ -60,7 +60,7 @@ repo::core::model::RepoScene* OdaModelImport::importModel(std::string filePath,
 		settings.getProjectName(),
 		settings.getRevisionId()
 	);
-	sceneBuilder->createIndexes();
+	sceneBuilder->createIndexes(false);
 
 	odaProcessor = odaHelper::FileProcessor::getFileProcessor(filePath, sceneBuilder.get(), settings);
 	auto result = odaProcessor->readFile();
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_synchro.cpp b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_synchro.cpp
index 5cc8ee15..b1aea58e 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_synchro.cpp
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_synchro.cpp
@@ -92,7 +92,8 @@ repo::core::model::RepoScene* SynchroModelImport::importModel(std::string filePa
 
 std::pair<repo::core::model::RepoNodeSet, repo::core::model::RepoNodeSet> SynchroModelImport::generateMatNodes(
 	std::unordered_map<std::string, repo::lib::RepoUUID> &synchroIDtoRepoID,
-	std::unordered_map<repo::lib::RepoUUID, repo::core::model::RepoNode*, repo::lib::RepoUUIDHasher> &repoIDToNode) {
+	std::unordered_map<repo::lib::RepoUUID, repo::core::model::RepoNode*, repo::lib::RepoUUIDHasher> &repoIDToNode,
+	std::unordered_map<repo::lib::RepoUUID, repo::core::model::TextureNode*, repo::lib::RepoUUIDHasher>& matIDToTex) {
 	repo::core::model::RepoNodeSet matNodes, textNodes;
 
 	for (const auto matEntry : reader->getMaterials()) {
@@ -111,6 +112,7 @@ std::pair<repo::core::model::RepoNodeSet, repo::core::model::RepoNodeSet> Synchr
 				textBuff.size(), matEntry.second.texture.width, matEntry.second.texture.height, { matNode->getSharedID() }));
 			textNodes.insert(textNode);
 			repoIDToNode[textNode->getUniqueID()] = textNode;
+			matIDToTex[matNode->getUniqueID()] = textNode;
 		}
 
 		repoIDToNode[matNode->getUniqueID()] = matNode;
@@ -276,8 +278,9 @@ repo::core::model::RepoScene* SynchroModelImport::constructScene(
 	repo::core::model::RepoNodeSet transNodes, matNodes, textNodes, meshNodes, metaNodes;
 	std::unordered_map<std::string, repo::lib::RepoUUID> synchroIDToRepoID;
 	std::unordered_map<repo::lib::RepoUUID, repo::core::model::RepoNode*, repo::lib::RepoUUIDHasher> repoIDToNode;
+	std::unordered_map<repo::lib::RepoUUID, repo::core::model::TextureNode*, repo::lib::RepoUUIDHasher> matIDToTex;
 	repoInfo << "Generating materials.... ";
-	auto matPairs = generateMatNodes(synchroIDToRepoID, repoIDToNode);
+	auto matPairs = generateMatNodes(synchroIDToRepoID, repoIDToNode, matIDToTex);
 	matNodes = matPairs.first;
 	textNodes = matPairs.second;
 
@@ -358,6 +361,14 @@ repo::core::model::RepoScene* SynchroModelImport::constructScene(
 			auto matNode = repoIDToNode[synchroIDToRepoID[matID]];
 			auto matNodeID = matNode->getUniqueID();
 
+			// Shim to enable the new multipart optimiser to read Synchro models until this importer
+			// is refactored to use the scene builder.
+			mesh->setMaterial(((repo::core::model::MaterialNode*)matNode)->getMaterialStruct());
+			auto tex = matIDToTex.find(matNode->getUniqueID());
+			if (tex != matIDToTex.end()) {
+				mesh->setTextureId(tex->second->getUniqueID());
+			}
+
 			if (nodeToParents.find(matNodeID) == nodeToParents.end())
 				nodeToParents[matNodeID] = { mesh->getSharedID() };
 			else
diff --git a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_synchro.h b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_synchro.h
index 1e31ce18..caa8b09f 100644
--- a/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_synchro.h
+++ b/bouncer/src/repo/manipulator/modelconvertor/import/repo_model_import_synchro.h
@@ -30,6 +30,7 @@
 #include "../../../core/model/bson/repo_node_mesh.h"
 #include "../../../core/model/bson/repo_node_metadata.h"
 #include "../../../core/model/bson/repo_node_transformation.h"
+#include "../../../core/model/bson/repo_node_texture.h"
 #include "../../../lib/repo_property_tree.h"
 #include "../../../error_codes.h"
 
@@ -101,7 +102,8 @@ namespace repo {
 
 				std::pair<repo::core::model::RepoNodeSet, repo::core::model::RepoNodeSet> generateMatNodes(
 					std::unordered_map<std::string, repo::lib::RepoUUID> &synchroIDtoRepoID,
-					std::unordered_map<repo::lib::RepoUUID, repo::core::model::RepoNode*, repo::lib::RepoUUIDHasher> &repoIDToNode);
+					std::unordered_map<repo::lib::RepoUUID, repo::core::model::RepoNode*, repo::lib::RepoUUIDHasher> &repoIDToNode,
+					std::unordered_map<repo::lib::RepoUUID, repo::core::model::TextureNode*, repo::lib::RepoUUIDHasher> &matIDToTex);
 
 				repo::core::model::MetadataNode* createMetaNode(
 					const std::unordered_map<std::string, std::string> &metadata,
diff --git a/bouncer/src/repo/manipulator/modeloptimizer/repo_optimizer_multipart.cpp b/bouncer/src/repo/manipulator/modeloptimizer/repo_optimizer_multipart.cpp
index 3285ba83..b5228b51 100644
--- a/bouncer/src/repo/manipulator/modeloptimizer/repo_optimizer_multipart.cpp
+++ b/bouncer/src/repo/manipulator/modeloptimizer/repo_optimizer_multipart.cpp
@@ -35,10 +35,6 @@ using namespace repo::manipulator::modeloptimizer;
 
 auto defaultGraph = repo::core::model::RepoScene::GraphType::DEFAULT;
 
-using Scalar = float;
-using Bvh = bvh::Bvh<Scalar>;
-using BvhVector3 = bvh::Vector3<Scalar>;
-
 // The vertex count is used as a rough approximation of the total geometry size.
 // This figure is empirically set to end up with an average bundle size of 24 Mb.
 static const size_t REPO_MP_MAX_VERTEX_COUNT = 1200000;
@@ -52,89 +48,563 @@ static const size_t REPO_MODEL_LOW_CLUSTERING_RATIO = 0.2f;
 
 #define CHRONO_DURATION(start) std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::high_resolution_clock::now() - start).count()
 
-bool MultipartOptimizer::apply(repo::core::model::RepoScene *scene)
+bool MultipartOptimizer::processScene(
+	std::string database,
+	std::string collection,
+	repo::lib::RepoUUID revId,
+	repo::core::handler::AbstractDatabaseHandler *handler,
+	repo::manipulator::modelconvertor::AbstractModelExport* exporter)
 {
-	bool success = false;
-	if (!scene)
-	{
-		repoError << "Failed to create Optimised scene: nullptr to scene.";
-		return false;
+	// Getting Transforms
+	repoInfo << "Getting Transforms";
+	auto transformMap = getAllTransforms(handler, database, collection, revId);
+
+	// Get lookup map for material properties
+	repoInfo << "Getting Materials";
+	auto matPropMap = getAllMaterials(handler, database, collection, revId);
+
+	// Get all groupings
+	repoInfo << "Getting Groupings";
+	auto groupings = getAllGroupings(handler, database, collection, revId);
+	repoInfo << "Found " << groupings.size() << " groupings";
+
+	// Create jobs
+	repoInfo << "Creating Processing Jobs";
+	std::vector<ProcessingJob> jobs;
+	for (auto grouping : groupings) {
+
+		// Job for opaque, prim 2
+		{
+			std::string description = "Grouping: " + grouping + ", Opaque, Primitive 2";			
+			jobs.push_back(createUntexturedJob(description, revId, 2, grouping, true));
+		}
+
+		// Job for opaque, prim 3
+		{
+			std::string description = "Grouping: " + grouping + ", Opaque, Primitive 3";
+			jobs.push_back(createUntexturedJob(description, revId, 3, grouping, true));
+		}
+
+		// Job for transparent, prim 2
+		{
+			std::string description = "Grouping: " + grouping + ", Transparent, Primitive 2";
+			jobs.push_back(createUntexturedJob(description, revId, 2, grouping, false));
+		}
+
+		// Job for transparent, prim 3
+		{
+			std::string description = "Grouping: " + grouping + ", Transparent, Primitive 3";
+			jobs.push_back(createUntexturedJob(description, revId, 3, grouping, false));
+		}
+
+		// Get Texture IDs
+		repoInfo << "Getting all texture Ids for grouping " << grouping;
+		auto texIds = getAllTextureIds(handler, database, collection, revId, grouping);
+
+		// Create jobs for each texture group
+		for (auto texId : texIds) {
+
+			// Job for textured, prim 2
+			// One cannot map a texture to a line, however, customers can assign materials with textures to lines
+			// so we need to be able to process them.
+			{
+				std::string description = "Grouping: " + grouping + ", Textured " + texId.toString() + " , Primitive 2";
+				jobs.push_back(createTexturedJob(description, revId, 2, grouping, texId));
+			}
+
+			// Job for textured, prim 3
+			{
+				std::string description = "Grouping: " + grouping + ", Textured " + texId.toString() + " , Primitive 3";
+				jobs.push_back(createTexturedJob(description, revId, 3, grouping, texId));
+			}
+		}
 	}
 
-	if (!scene->hasRoot(repo::core::model::RepoScene::GraphType::DEFAULT))
-	{
-		repoError << "Failed to create Optimised scene: scene is empty!";
-		return false;
+	// Process jobs
+	repoInfo << "Processing Jobs";
+
+	for (auto job : jobs) {
+		clusterAndSupermesh(
+			database,
+			collection,
+			handler,
+			exporter,
+			transformMap,
+			matPropMap,
+			job);
 	}
 
-	if (scene->hasRoot(repo::core::model::RepoScene::GraphType::OPTIMIZED))
-	{
-		repoInfo << "The scene already has a stash graph, removing...";
-		scene->clearStash();
+	// Finalise export
+	exporter->finalise();
+
+	return true;
+}
+
+std::unordered_map<repo::lib::RepoUUID, repo::lib::RepoMatrix, repo::lib::RepoUUIDHasher> MultipartOptimizer::getAllTransforms(
+	repo::core::handler::AbstractDatabaseHandler *handler,
+	const std::string &database,
+	const std::string &collection,
+	const repo::lib::RepoUUID &revId
+)
+{
+
+	repo::core::handler::database::query::RepoQueryBuilder filter;
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_REVISION_ID, revId));
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_LABEL_TYPE, std::string(REPO_NODE_TYPE_TRANSFORMATION)));
+
+	repo::core::handler::database::query::RepoProjectionBuilder projection;
+	projection.excludeField(REPO_NODE_LABEL_ID);
+	projection.includeField(REPO_NODE_LABEL_SHARED_ID);
+	projection.includeField(REPO_NODE_LABEL_MATRIX);
+	projection.includeField(REPO_NODE_LABEL_PARENTS);
+
+	auto sceneCollection = collection + "." + REPO_COLLECTION_SCENE;
+	auto cursor = handler->findCursorByCriteria(database, sceneCollection, filter, projection);
+
+	std::unordered_map<repo::lib::RepoUUID, repo::lib::RepoMatrix, repo::lib::RepoUUIDHasher> transformMap;
+
+	if (cursor) {
+		repo::core::model::RepoBSON rootNode;
+		std::unordered_map<repo::lib::RepoUUID, std::vector<repo::core::model::RepoBSON>, repo::lib::RepoUUIDHasher> childNodeMap;
+		for (auto bson : (*cursor)) {
+			if (bson.hasField(REPO_NODE_LABEL_PARENTS)) {
+				auto parentId = bson.getUUIDFieldArray(REPO_NODE_LABEL_PARENTS)[0];
+
+				if (childNodeMap.contains(parentId)) {
+					childNodeMap.at(parentId).push_back(bson);
+				}
+				else {
+					auto children = std::vector<repo::core::model::RepoBSON>{ bson };
+					childNodeMap.insert({ parentId, children });
+				}
+			}
+			else {
+				rootNode = bson;
+			}
+		}
+		if (rootNode.isEmpty()) {
+			repoWarning << "getAllTransforms; no transformations returned by database query.";
+		}
+		else {
+			traverseTransformTree(rootNode, childNodeMap, transformMap);
+		}
+	}
+	else {
+		repoWarning << "getAllTransforms; getting cursor was not successful; no transforms in output map";
 	}
 
-	return generateMultipartScene(scene);
+	return transformMap;
 }
 
-bool MultipartOptimizer::getBakedMeshNodes(
-	const repo::core::model::RepoScene* scene,
-	const repo::core::model::RepoNode* node,
-	repo::lib::RepoMatrix mat,
-	MeshMap& nodes)
+void MultipartOptimizer::traverseTransformTree(
+	const repo::core::model::RepoBSON &root,
+	const std::unordered_map<repo::lib::RepoUUID, std::vector<repo::core::model::RepoBSON>,	repo::lib::RepoUUIDHasher> &childNodeMap,
+	std::unordered_map<repo::lib::RepoUUID,	repo::lib::RepoMatrix, repo::lib::RepoUUIDHasher> &transforms)
 {
-	bool success = false;
-	if (success = scene && node)
-	{
-		switch (node->getTypeAsEnum())
-		{
-		case repo::core::model::NodeType::TRANSFORMATION:
-		{
-			auto trans = (repo::core::model::TransformationNode*)node;
-			mat = mat * trans->getTransMatrix();
-			auto children = scene->getChildrenAsNodes(defaultGraph, trans->getSharedID());
-			for (const auto& child : children)
-			{
-				success &= getBakedMeshNodes(scene, child, mat, nodes);
+
+	// Create stacks for the nodes and the matrices
+	std::stack<std::pair<repo::core::model::RepoBSON, repo::lib::RepoMatrix>> stack;
+
+	// Starting matrix
+	repo::lib::RepoMatrix identity;
+
+	// Push starting node and starting matrix on the stack
+	stack.push({ root, identity });
+
+	// DFS traversal of the transformation tree, summing up the matrices along the way
+	while (!stack.empty()) {
+
+		// Remove top node from the stack
+		auto top = stack.top();
+		stack.pop();
+
+		auto topBson = top.first;
+		auto topMat = top.second;
+
+		// Get node information
+		auto nodeId = topBson.getUUIDField(REPO_NODE_LABEL_SHARED_ID);
+		auto matrix = topBson.getMatrixField(REPO_NODE_LABEL_MATRIX);
+
+		// Apply the node's trnsaformation
+		auto newMat = topMat * matrix;
+
+		// Insert the transform for children of this node into the map
+		transforms.insert({ nodeId, newMat });
+
+		// if this node has other transforms as children, push them on the stack
+		if (childNodeMap.contains(nodeId)) {			
+			auto children = childNodeMap.at(nodeId);
+			for (auto child : children) {
+				stack.push({ child, newMat });
 			}
-			break;
 		}
 
-		case repo::core::model::NodeType::MESH:
-		{
-			auto mesh = (repo::core::model::MeshNode*)node;
-			repo::core::model::MeshNode transformedMesh = mesh->cloneAndApplyTransformation(mat);
-			nodes.insert({ node->getUniqueID(), transformedMesh });
+	}
+}
+
+MultipartOptimizer::MaterialPropMap MultipartOptimizer::getAllMaterials(
+	repo::core::handler::AbstractDatabaseHandler *handler,
+	const std::string &database,
+	const std::string &collection,
+	const repo::lib::RepoUUID &revId)
+{
+	repo::core::handler::database::query::RepoQueryBuilder filter;
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_REVISION_ID, revId));
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_LABEL_TYPE, std::string(REPO_NODE_TYPE_MATERIAL)));
+
+	auto sceneCollection = collection + "." + REPO_COLLECTION_SCENE;
+	auto materialBsons = handler->findAllByCriteria(database, sceneCollection, filter);
+
+	MaterialPropMap matMap;
+	for (auto &materialBson : materialBsons) {
+
+		// Create material node
+		auto matNode = std::make_shared<repo::core::model::MaterialNode>(materialBson);			
+				
+		// Go over the parents and add the pointer for each so that the map can be used to lookup
+		// the material for a given meshNode
+		auto parents = materialBson.getUUIDFieldArray(REPO_NODE_LABEL_PARENTS);
+		for (auto parent : parents) {
+			matMap.insert({ parent, matNode });
 		}
+	}
+
+	return matMap;
+}
+
+std::set<std::string> MultipartOptimizer::getAllGroupings(
+	repo::core::handler::AbstractDatabaseHandler* handler,
+	const std::string& database,
+	const std::string& collection,
+	const repo::lib::RepoUUID& revId
+) {
+	// Create filter
+	repo::core::handler::database::query::RepoQueryBuilder filter;
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_REVISION_ID, revId));
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_LABEL_TYPE, std::string(REPO_NODE_TYPE_MESH)));
+	filter.append(repo::core::handler::database::query::Exists(REPO_NODE_MESH_LABEL_GROUPING, true));
+
+	repo::core::handler::database::query::RepoProjectionBuilder projection;
+	projection.excludeField(REPO_NODE_LABEL_ID);
+	projection.includeField(REPO_NODE_MESH_LABEL_GROUPING);
+
+	std::set<std::string> groupings;
+
+	// Add default grouping
+	groupings.insert("");
+
+	auto sceneCollection = collection + "." + REPO_COLLECTION_SCENE;
+	auto cursor = handler->findCursorByCriteria(database, sceneCollection, filter, projection);
+
+	if (cursor) {
+		for (auto document : (*cursor)) {
+			auto bson = repo::core::model::RepoBSON(document);
+			groupings.insert(bson.getStringField(REPO_NODE_MESH_LABEL_GROUPING));
 		}
 	}
+	else {
+		repoWarning << "getAllGroupings; getting cursor was not successful; no groupings from db in output vector";
+	}
+
+	return groupings;
+}
+
+std::vector<repo::lib::RepoUUID> MultipartOptimizer::getAllTextureIds(
+	repo::core::handler::AbstractDatabaseHandler *handler,
+	const std::string &database,
+	const std::string &collection,
+	const repo::lib::RepoUUID &revId,
+	const std::string& grouping) {
+
+	// Create filter
+	repo::core::handler::database::query::RepoQueryBuilder filter;
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_REVISION_ID, revId));
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_LABEL_TYPE, std::string(REPO_NODE_TYPE_TEXTURE)));
+	if (!grouping.empty())
+		filter.append(repo::core::handler::database::query::Eq(REPO_NODE_MESH_LABEL_GROUPING, grouping));
 	else
-	{
-		repoError << "Unable to get baked MeshNode: scene or node is null.";
+		filter.append(repo::core::handler::database::query::Exists(REPO_NODE_MESH_LABEL_GROUPING, false));
+
+	repo::core::handler::database::query::RepoProjectionBuilder projection;
+	projection.includeField(REPO_NODE_LABEL_ID);
+
+	std::vector<repo::lib::RepoUUID> texIds;
+
+	auto sceneCollection = collection + "." + REPO_COLLECTION_SCENE;
+	auto cursor = handler->findCursorByCriteria(database, sceneCollection, filter, projection);
+
+	if (cursor) {
+		for (auto document : (*cursor)) {
+			auto bson = repo::core::model::RepoBSON(document);
+			texIds.push_back(bson.getUUIDField(REPO_NODE_LABEL_ID));
+		}
+	}
+	else {
+		repoWarning << "getAllTextureIds; getting cursor was not successful; no texture Ids in output vector";
 	}
 
-	return success;
+	return texIds;
+}
+
+MultipartOptimizer::ProcessingJob repo::manipulator::modeloptimizer::MultipartOptimizer::createUntexturedJob(
+	const std::string &description,
+	const repo::lib::RepoUUID &revId,
+	const int primitive,
+	const std::string &grouping,
+	const bool isOpaque)
+{
+	// Create filter
+	repo::core::handler::database::query::RepoQueryBuilder filter;
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_REVISION_ID, revId));
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_MESH_LABEL_PRIMITIVE, primitive));
+	if (!grouping.empty())
+		filter.append(repo::core::handler::database::query::Eq(REPO_NODE_MESH_LABEL_GROUPING, grouping));
+	else
+		filter.append(repo::core::handler::database::query::Exists(REPO_NODE_MESH_LABEL_GROUPING, false));
+	if (isOpaque)
+		filter.append(repo::core::handler::database::query::Eq(REPO_FILTER_TAG_OPAQUE, true));
+	else
+		filter.append(repo::core::handler::database::query::Eq(REPO_FILTER_TAG_TRANSPARENT, true));
+
+	// Create job
+	return ProcessingJob({ description, filter, {} });
+}
+
+MultipartOptimizer::ProcessingJob repo::manipulator::modeloptimizer::MultipartOptimizer::createTexturedJob(
+	const std::string &description,
+	const repo::lib::RepoUUID &revId,
+	const int primitive,
+	const std::string &grouping,
+	const repo::lib::RepoUUID &texId)
+{
+	// Create filter
+	repo::core::handler::database::query::RepoQueryBuilder filter;
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_REVISION_ID, revId));
+	filter.append(repo::core::handler::database::query::Eq(REPO_FILTER_TAG_TEXTURE_ID, texId));
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_MESH_LABEL_PRIMITIVE, primitive));
+	if (!grouping.empty())
+		filter.append(repo::core::handler::database::query::Eq(REPO_NODE_MESH_LABEL_GROUPING, grouping));
+	else
+		filter.append(repo::core::handler::database::query::Exists(REPO_NODE_MESH_LABEL_GROUPING, false));
+
+	// Create job
+	return ProcessingJob({ description, filter, texId });
+}
+
+void MultipartOptimizer::clusterAndSupermesh(
+	const std::string &database,
+	const std::string &collection,
+	repo::core::handler::AbstractDatabaseHandler *handler,
+	repo::manipulator::modelconvertor::AbstractModelExport *exporter,
+	const TransformMap& transformMap,
+	const MaterialPropMap& matPropMap,
+	const MultipartOptimizer::ProcessingJob &job
+) {
+	repoInfo << "Processing Job: " << job.description;
+
+	// Create projection
+	repo::core::handler::database::query::RepoProjectionBuilder projection;
+	projection.excludeField(REPO_NODE_LABEL_ID);
+	projection.includeField(REPO_NODE_LABEL_SHARED_ID);
+	projection.includeField(REPO_NODE_MESH_LABEL_BOUNDING_BOX);
+	projection.includeField(REPO_NODE_MESH_LABEL_VERTICES_COUNT);
+	projection.includeField(REPO_NODE_LABEL_PARENTS);
+
+	// Get cursor
+	auto sceneCollection = collection + "." + REPO_COLLECTION_SCENE;
+	auto cursor = handler->findCursorByCriteria(database, sceneCollection, job.filter, projection);
+
+	// iterate cursor and pack outcomes in lightweight mesh node structure
+	std::vector<repo::core::model::StreamingMeshNode> nodes;
+	if (cursor) {
+		for (auto bson : (*cursor)) {
+			nodes.push_back(repo::core::model::StreamingMeshNode(bson));
+		}
+	}
+	else {
+		repoWarning << "clusterAndSupermesh; getting cursor was not successful; no nodes filtered for processing";
+	}
+
+	// Check whether there are any nodes in this group
+	if (nodes.size() == 0) {
+		repoInfo << "No nodes to process in this group. Returning.";
+		return;
+	}
+
+	// Transform bounds before clustering
+	for (auto& node : nodes) {
+		auto bounds = node.getBoundingBox();
+
+		// Transform bounds
+		auto parentId = node.getParent();
+
+		if (transformMap.contains(parentId)) {
+			auto transMat = transformMap.at(node.getParent());
+			node.transformBounds(transMat);
+		}
+		else {
+			repoWarning << "clusterAndSupermesh; current node " << node.getSharedId().toString() << " has no transform parents; this should not happen; malformed file?";
+		}
+	}
+
+	// Cluster the mesh nodes
+	repoInfo << "Clustering Nodes";
+	auto clusters = clusterMeshNodes(nodes);
+
+	// Create Supermeshes from the clusters
+	repoInfo << "Creating Supermeshes from clustered Nodes";
+	auto texId = job.isTexturedJob() ? job.texId : repo::lib::RepoUUID();
+	createSuperMeshes(database, collection, handler, exporter, transformMap, matPropMap, nodes, clusters, texId);
+}
+
+void repo::manipulator::modeloptimizer::MultipartOptimizer::createSuperMeshes(
+	const std::string &database,
+	const std::string &collection,
+	repo::core::handler::AbstractDatabaseHandler *handler,
+	repo::manipulator::modelconvertor::AbstractModelExport *exporter,
+	const TransformMap& transformMap,
+	const MaterialPropMap& matPropMap,
+	std::vector<repo::core::model::StreamingMeshNode>& meshNodes,
+	const std::vector<std::vector<int>>& clusters,
+	const repo::lib::RepoUUID &texId)
+{
+	// Get blobHandler
+	auto sceneCollection = collection + "." + REPO_COLLECTION_SCENE;
+	repo::core::handler::fileservice::BlobFilesHandler blobHandler(handler->getFileManager(), database, sceneCollection);
+
+	for (auto cluster : clusters) {
+
+		std::unordered_map<repo::lib::RepoUUID, int, repo::lib::RepoUUIDHasher> clusterMap;
+		std::vector<repo::lib::RepoUUID> sharedIdsInCluster;
+		for (auto& index : cluster) {
+			auto& node = meshNodes[index];
+			auto sharedId = node.getSharedId();
+			clusterMap.insert({ sharedId, index });
+
+			sharedIdsInCluster.push_back(sharedId);
+		}
+
+		// Create filter
+		auto filter = repo::core::handler::database::query::Eq(REPO_NODE_LABEL_SHARED_ID, sharedIdsInCluster);
+
+		// Create projection
+		repo::core::handler::database::query::RepoProjectionBuilder projection;
+		projection.excludeField(REPO_NODE_LABEL_ID);
+		projection.includeField(REPO_NODE_LABEL_SHARED_ID);
+		projection.includeField(REPO_NODE_MESH_LABEL_VERTICES_COUNT);
+		projection.includeField(REPO_NODE_MESH_LABEL_FACES_COUNT);
+		projection.includeField(REPO_NODE_MESH_LABEL_UV_CHANNELS_COUNT);
+		projection.includeField(REPO_NODE_MESH_LABEL_PRIMITIVE);
+		projection.includeField(REPO_LABEL_BINARY_REFERENCE);
+
+		auto binNodes = handler->findAllByCriteria(database, sceneCollection, filter, projection);
+
+		// Iterate over the meshes and decide what to do with each. The options are
+		// to append to the existing supermesh, start a new supermesh, or split into
+		// multiple supermeshes.
+
+		mapped_mesh_t currentSupermesh;
+
+		for (auto& nodeBson : binNodes) {
+
+			// Find streamed node
+			auto sharedId = nodeBson.getUUIDField(REPO_NODE_LABEL_SHARED_ID);
+			auto nodeIndex = clusterMap.at(sharedId);
+			auto& sNode = meshNodes[nodeIndex];
+
+			// Load geometry for this node.
+			// Placed In its own scope so that buffer can be discarded as soon as it is processed
+			{
+				auto binRef = nodeBson.getBinaryReference();
+				auto dataRef = repo::core::handler::fileservice::DataRef::deserialise(binRef);
+				auto buffer = blobHandler.readToBuffer(dataRef);
+
+				// If there is no texture present, we ignore UV values.
+				// This allows us to group more meshes together.
+				bool ignoreUVs = texId.isDefaultValue();
+
+				sNode.loadSupermeshingData(nodeBson, buffer, ignoreUVs);
+			}
+
+			// Bake the streaming mesh node by applying the transformation to the vertices
+			// Note that the bounds have already been transformed by calling transformBounds earlier
+			auto parentId = sNode.getParent();
+			if (transformMap.contains(parentId)) {
+				auto transform = transformMap.at(parentId);
+				sNode.bakeLoadedMeshes(transform);
+			}
+			else {
+				repoWarning << "createSuperMeshes; no transform found for this mesh node. Mesh will not be baked";
+			}
+
+			if (currentSupermesh.vertices.size() + sNode.getNumLoadedVertices() <= REPO_MP_MAX_VERTEX_COUNT)
+			{
+				// The current node can be added to the supermesh OK				
+				appendMesh(sNode, matPropMap, currentSupermesh, texId);
+			}
+			else if (sNode.getNumLoadedVertices() > REPO_MP_MAX_VERTEX_COUNT)
+			{
+				// The node is too big to fit into any supermesh, so it must be split
+				splitMesh(sNode, exporter, matPropMap, texId);
+			}
+			else
+			{
+				// The node is small enough to fit within one supermesh, just not this one
+				createSuperMesh(exporter, currentSupermesh);
+				currentSupermesh = mapped_mesh_t();
+				appendMesh(sNode, matPropMap, currentSupermesh, texId);
+			}
+
+			// Unload the streaming node
+			sNode.unloadSupermeshingData();
+		}
+
+		// Add the last supermesh to be built
+		if (currentSupermesh.vertices.size()) {
+			createSuperMesh(exporter, currentSupermesh);
+		}
+	}
+}
+
+void MultipartOptimizer::createSuperMesh(
+	repo::manipulator::modelconvertor::AbstractModelExport *exporter,
+	const mapped_mesh_t& mappedMesh)
+{
+	// Create supermesh node
+	auto supermeshNode = createSupermeshNode(mappedMesh);
+
+	exporter->addSupermesh(supermeshNode.get());
 }
 
 void MultipartOptimizer::appendMesh(
-	const repo::core::model::RepoScene* scene,
-	repo::core::model::MeshNode node,
-	mapped_mesh_t& mapped
+	repo::core::model::StreamingMeshNode &node,
+	const MaterialPropMap &matPropMap,
+	mapped_mesh_t &mapped,
+	const repo::lib::RepoUUID &texId
 )
 {
 	repo_mesh_mapping_t meshMap;
 
-	meshMap.material_id = getMaterialID(scene, &node);
-	meshMap.mesh_id = node.getUniqueID();
-	meshMap.shared_id = node.getSharedID();
+	// Get material information
+	auto matNode = matPropMap.at(node.getSharedId());
+	meshMap.material_id = matNode->getUniqueID();
+	meshMap.material = matNode->getMaterialStruct();
+
+	// set texture id if passed in
+	if (!texId.isDefaultValue())
+		meshMap.texture_id = texId;
+
+	meshMap.mesh_id = node.getUniqueId();
+	meshMap.shared_id = node.getSharedId();
 
 	auto bbox = node.getBoundingBox();
 	meshMap.min = (repo::lib::RepoVector3D)bbox.min();
 	meshMap.max = (repo::lib::RepoVector3D)bbox.max();
 
-	std::vector<repo::lib::RepoVector3D> submVertices = node.getVertices();
-	std::vector<repo::lib::RepoVector3D> submNormals = node.getNormals();
-	std::vector<repo_face_t> submFaces = node.getFaces();
-	std::vector<std::vector<repo::lib::RepoVector2D>> submUVs = node.getUVChannelsSeparated();
+	std::vector<repo::lib::RepoVector3D> submVertices = node.getLoadedVertices();
+	std::vector<repo::lib::RepoVector3D> submNormals = node.getLoadedNormals();
+	std::vector<repo_face_t> submFaces = node.getLoadedFaces();
+	std::vector<std::vector<repo::lib::RepoVector2D>> submUVs = node.getLoadedUVChannelsSeparated();
 
 	if (submVertices.size() && submFaces.size())
 	{
@@ -175,8 +645,8 @@ void MultipartOptimizer::appendMesh(
 		}
 		else
 		{
-			//This shouldn't happen, if it does, then it means the mFormat isn't set correctly
-			repoError << "Unexpected transformedMesh format mismatch occured!";
+			//This shouldn't happen, if it does, then it means that mesh nodes with and without uvs have been grouped together
+			repoError << "Unexpected mismatch occured! Meshes with and without UVs grouped together!";
 		}
 	}
 	else
@@ -187,16 +657,16 @@ void MultipartOptimizer::appendMesh(
 
 // Constructs a bounding volumne hierarchy of all the Faces in the MeshNode
 
-Bvh buildFacesBvh(
-	const repo::core::model::MeshNode node
+MultipartOptimizer::Bvh MultipartOptimizer::buildFacesBvh(
+	repo::core::model::StreamingMeshNode &node
 )
 {
 	// Create a set of bounding boxes & centers for each Face in the oversized
 	// mesh.
 	// The BVH builder expects a set of bounding boxes and centers to work with.
 
-	auto faces = node.getFaces();
-	auto vertices = node.getVertices();
+	auto faces = node.getLoadedFaces();
+	auto vertices = node.getLoadedVertices();
 	auto boundingBoxes = std::vector<bvh::BoundingBox<Scalar>>();
 	auto centers = std::vector<BvhVector3>();
 
@@ -230,10 +700,10 @@ Bvh buildFacesBvh(
 // Create a breadth first list of all the leaf, and branch, nodes in a Bvh.
 // Nodes will only appear in one of the two lists.
 
-void flattenBvh(
+void MultipartOptimizer::flattenBvh(
 	const Bvh& bvh,
-	std::vector<size_t>& leaves,
-	std::vector<size_t>& branches
+	std::vector<size_t> &leaves,
+	std::vector<size_t> &branches
 )
 {
 	std::queue<size_t> nodeQueue; // Using a queue instead of a stack means the child nodes are handled later, resulting in a breadth first traversal
@@ -260,7 +730,7 @@ void flattenBvh(
 
 // Get a list of all the primitives under a branch of a Bvh.
 
-std::vector<size_t> getBranchPrimitives(
+std::vector<size_t> MultipartOptimizer::getBranchPrimitives(
 	const Bvh& bvh,
 	size_t head
 )
@@ -295,7 +765,7 @@ std::vector<size_t> getBranchPrimitives(
 // For each node in the Bvh, return a list of unique vertex Ids that are
 // referenced by the faces (primitives) in that node.
 
-std::vector<std::set<uint32_t>> getUniqueVertices(
+std::vector<std::set<uint32_t>> MultipartOptimizer::getUniqueVertices(
 	const Bvh& bvh,
 	const std::vector<repo_face_t>& primitives // The primitives in this tree are faces
 )
@@ -347,9 +817,9 @@ std::vector<std::set<uint32_t>> getUniqueVertices(
 // Gets the branch nodes that contain fewer than REPO_MP_MAX_VERTEX_COUNT beneath
 // them in total.
 
-std::vector<size_t> getSupermeshBranchNodes(
-	const Bvh& bvh,
-	std::vector<size_t> vertexCounts)
+std::vector<size_t> MultipartOptimizer::getSupermeshBranchNodes(
+	const Bvh &bvh,
+	const std::vector<size_t> &vertexCounts)
 {
 	std::vector<size_t> branchNodes;
 	std::stack<size_t> nodeStack;
@@ -380,9 +850,10 @@ std::vector<size_t> getSupermeshBranchNodes(
 }
 
 void MultipartOptimizer::splitMesh(
-	const repo::core::model::RepoScene* scene,
-	repo::core::model::MeshNode node,
-	std::vector<mapped_mesh_t>& mappedMeshes
+	repo::core::model::StreamingMeshNode &node,
+	repo::manipulator::modelconvertor::AbstractModelExport *exporter,
+	const MaterialPropMap &matPropMap,
+	const repo::lib::RepoUUID &texId
 )
 {
 	// The purpose of this method is to split large MeshNodes into smaller ones.
@@ -401,7 +872,7 @@ void MultipartOptimizer::splitMesh(
 	// We get the vertex counts by first computing all the vertices referenced
 	// by the node(s), which will be used in the re-indexing.
 
-	auto faces = node.getFaces();
+	auto faces = node.getLoadedFaces();
 	auto uniqueVerticesByNode = getUniqueVertices(bvh, faces);
 
 	auto vertexCounts = std::vector<size_t>();
@@ -440,9 +911,9 @@ void MultipartOptimizer::splitMesh(
 
 	// Get the vertex attributes for building the sub mapped meshes
 
-	auto vertices = node.getVertices();
-	auto normals = node.getNormals();
-	auto uvChannels = node.getUVChannelsSeparated();
+	auto vertices = node.getLoadedVertices();
+	auto normals = node.getLoadedNormals();
+	auto uvChannels = node.getLoadedUVChannelsSeparated();
 
 	for (const auto head : branchNodes)
 	{
@@ -515,73 +986,30 @@ void MultipartOptimizer::splitMesh(
 		}
 		mapping.min = (repo::lib::RepoVector3D)bounds.min();
 		mapping.max = (repo::lib::RepoVector3D)bounds.max();
-		mapping.mesh_id = node.getUniqueID();
-		mapping.shared_id = node.getSharedID();
-		mapping.material_id = getMaterialID(scene, &node);
+		mapping.mesh_id = node.getUniqueId();
+		mapping.shared_id = node.getSharedId();
 
-		mapped.meshMapping.push_back(mapping);
+		// Get material information
+		auto matNode = matPropMap.at(node.getSharedId());
+		mapping.material_id = matNode->getUniqueID();
+		mapping.material = matNode->getMaterialStruct();
 
-		mappedMeshes.push_back(mapped);
-	}
+		// set texture id if passed in
+		if (!texId.isDefaultValue())
+			mapping.texture_id = texId;
 
-	repoInfo << "Split mesh with " << node.getNumVertices() << " vertices into " << mappedMeshes.size() << " submeshes in " << CHRONO_DURATION(start) << " ms";
-}
-
-void MultipartOptimizer::createSuperMeshes(
-	const repo::core::model::RepoScene* scene,
-	const std::vector<repo::core::model::MeshNode> &nodes,
-	const bool isGrouped,
-	std::vector<repo::core::model::SupermeshNode*> &supermeshNodes)
-{
-	// This will hold the final set of supermeshes
-
-	std::vector<mapped_mesh_t> mappedMeshes;
-
-	// Iterate over the meshes and decide what to do with each. The options are
-	// to append to the existing supermesh, start a new supermesh, or split into
-	// multiple supermeshes.
-
-	mapped_mesh_t currentSupermesh;
+		mapped.meshMapping.push_back(mapping);
 
-	for (const auto& node : nodes)
-	{
-		if (currentSupermesh.vertices.size() + node.getNumVertices() <= REPO_MP_MAX_VERTEX_COUNT)
-		{
-			// The current node can be added to the supermesh OK
-			appendMesh(scene, node, currentSupermesh);
-		}
-		else if (node.getNumVertices() > REPO_MP_MAX_VERTEX_COUNT)
-		{
-			// The node is too big to fit into any supermesh, so it must be split
-			splitMesh(scene, node, mappedMeshes);
-		}
-		else
-		{
-			// The node is small enough to fit within one supermesh, just not this one
-			mappedMeshes.push_back(currentSupermesh);
-			currentSupermesh = mapped_mesh_t();
-			appendMesh(scene, node, currentSupermesh);
-		}
+		createSuperMesh(exporter, mapped);
 	}
 
-	// Add the last supermesh to be built
-
-	if (currentSupermesh.vertices.size())
-	{
-		mappedMeshes.push_back(currentSupermesh);
-	}
+	repoInfo << "Split mesh with " << node.getNumLoadedVertices() << " vertices into " << branchNodes.size() << " submeshes in " << CHRONO_DURATION(start) << " ms";
+}
 
-	// Finally, construct the SupermeshNodes for each Supermesh
 
-	for (auto& mapped : mappedMeshes)
-	{
-		supermeshNodes.push_back(createSupermeshNode(mapped, isGrouped));
-	}
-}
 
-repo::core::model::SupermeshNode* MultipartOptimizer::createSupermeshNode(
-	const mapped_mesh_t& mapped,
-	bool isGrouped
+std::unique_ptr<repo::core::model::SupermeshNode> MultipartOptimizer::createSupermeshNode(
+	const mapped_mesh_t &mapped
 )
 {
 	if (!mapped.meshMapping.size())
@@ -600,146 +1028,24 @@ repo::core::model::SupermeshNode* MultipartOptimizer::createSupermeshNode(
 		bbox.encapsulate(meshMapping[i].max);
 	}
 
-	auto supermesh = repo::core::model::RepoBSONFactory::makeSupermeshNode(
+	return repo::core::model::RepoBSONFactory::makeSupermeshNode(
 		mapped.vertices,
 		mapped.faces,
 		mapped.normals,
 		bbox,
 		mapped.uvChannels,
-		isGrouped ? "grouped" : "",
+		"",
 		meshMapping);
-
-	return new repo::core::model::SupermeshNode(supermesh);
-}
-
-bool MultipartOptimizer::generateMultipartScene(repo::core::model::RepoScene *scene)
-{
-	bool success = false;
-	auto meshes = scene->getAllMeshes(defaultGraph);
-	if (success = meshes.size())
-	{
-		// Bake all the meshes into model space, creating a lookup for processMeshGroup
-
-		repoInfo << "Baking " << meshes.size() << " meshes...";
-
-		MeshMap bakedMeshNodes;
-		getBakedMeshNodes(scene, scene->getRoot(defaultGraph), repo::lib::RepoMatrix(), bakedMeshNodes);
-
-		//Sort the meshes into 3 different groupings
-
-		std::unordered_map<std::string, std::unordered_map<uint32_t, std::vector<std::set<repo::lib::RepoUUID>>>> transparentMeshes, normalMeshes;
-		std::unordered_map < std::string, std::unordered_map < uint32_t, std::unordered_map < repo::lib::RepoUUID,
-			std::vector<std::set<repo::lib::RepoUUID>>, repo::lib::RepoUUIDHasher >>>texturedMeshes;
-
-		repoInfo << "Sorting " << meshes.size() << " meshes...";
-
-		sortMeshes(scene, meshes, normalMeshes, transparentMeshes, texturedMeshes);
-
-		repo::core::model::RepoNodeSet mergedMeshes, trans, textures, dummy;
-
-		auto rootNode = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-		trans.insert(rootNode);
-		repo::lib::RepoUUID rootID = rootNode->getSharedID();
-
-		for (const auto &meshGroup : normalMeshes)
-		{
-			for (const auto &formats : meshGroup.second)
-			{
-				for (const auto formatSet : formats.second)
-				{
-					success &= processMeshGroup(scene, bakedMeshNodes, formatSet, rootID, mergedMeshes, !meshGroup.first.empty());
-				}
-			}
-		}
-
-		for (const auto &meshGroup : transparentMeshes)
-		{
-			for (const auto &groupings : meshGroup.second)
-			{
-				for (const auto grouping : groupings.second)
-				{
-					success &= processMeshGroup(scene, bakedMeshNodes, grouping, rootID, mergedMeshes, !meshGroup.first.empty());
-				}
-			}
-		}
-
-		//textured meshes
-
-		for (const auto &meshGroup : texturedMeshes)
-		{
-			for (const auto &textureMeshMap : meshGroup.second)
-			{
-				for (const auto &groupings : textureMeshMap.second)
-				{
-					for (const auto grouping : groupings.second)
-					{
-						success &= processMeshGroup(scene, bakedMeshNodes, grouping, rootID, mergedMeshes, !meshGroup.first.empty());
-					}
-				}
-			}
-		}
-
-		if (success)
-		{
-			// The new in-memory only stash graph does not hold its own
-			// materials or textures. In the future, we may want to copy
-			// these nodes temporarily from default, so we can free it.
-
-			scene->addStashGraph(mergedMeshes, {}, {}, trans);
-		}
-		else
-		{
-			repoError << "Failed to process Mesh Groups";
-		}
-	}
-	else
-	{
-		repoError << "Cannot generate a multipart scene for a scene with no meshes";
-	}
-
-	return success;
 }
 
-repo::lib::RepoUUID MultipartOptimizer::getMaterialID(
-	const repo::core::model::RepoScene *scene,
-	const repo::core::model::MeshNode  *mesh
-)
-{
-	repo::lib::RepoUUID matID = repo::lib::RepoUUID(REPO_HISTORY_MASTER_BRANCH);
-	const auto mat = scene->getChildrenNodesFiltered(defaultGraph, mesh->getSharedID(), repo::core::model::NodeType::MATERIAL);
-	if (mat.size())
-	{
-		matID = mat[0]->getUniqueID();
-	}
 
-	return matID;
-}
-
-bool MultipartOptimizer::hasTexture(
-	const repo::core::model::RepoScene *scene,
-	const repo::core::model::MeshNode  *mesh,
-	repo::lib::RepoUUID	&texID
-)
-{
-	bool hasText = false;
-	const auto mat = scene->getChildrenNodesFiltered(defaultGraph, mesh->getSharedID(), repo::core::model::NodeType::MATERIAL);
-	if (mat.size())
-	{
-		const auto texture = scene->getChildrenNodesFiltered(defaultGraph, mat[0]->getSharedID(), repo::core::model::NodeType::TEXTURE);
-		if (hasText = texture.size())
-		{
-			texID = texture[0]->getSharedID();
-		}
-	}
-
-	return hasText;
-}
 
 // Builds a Bvh of the bounds of the MeshNodes. Each MeshNode in the array is
 // the primitive.
 
-Bvh buildBoundsBvh(
-	const std::vector<repo::core::model::MeshNode>& meshes
+MultipartOptimizer::Bvh MultipartOptimizer::buildBoundsBvh(
+	const std::vector<int>& binIndexes,
+	const std::vector<repo::core::model::StreamingMeshNode>& meshes
 )
 {
 	// The BVH builder requires a set of bounding boxes and centers to work with.
@@ -748,8 +1054,9 @@ Bvh buildBoundsBvh(
 	// is the same (which it is).
 
 	auto boundingBoxes = std::vector<bvh::BoundingBox<Scalar>>();
-	for (const auto& node : meshes)
+	for (const int index : binIndexes)
 	{
+		auto& node = meshes[index];
 		auto bounds = node.getBoundingBox();
 		auto min = BvhVector3(bounds.min().x, bounds.min().y, bounds.min().z);
 		auto max = BvhVector3(bounds.max().x, bounds.max().y, bounds.max().z);
@@ -776,9 +1083,10 @@ Bvh buildBoundsBvh(
 // Gets the vertex counts of each node in the Bvh, when the Bvh primitives are
 // MeshNodes.
 
-std::vector<size_t> getVertexCounts(
+std::vector<size_t> MultipartOptimizer::getVertexCounts(
 	const Bvh& bvh,
-	const std::vector<repo::core::model::MeshNode>& primitives
+	const std::vector<int>& binIndexes,
+	const std::vector<repo::core::model::StreamingMeshNode>& meshes
 )
 {	// To do this, get the nodes in list form, 'bottom up', in order to set the
 	// vertex counts of each leaf node.
@@ -800,12 +1108,13 @@ std::vector<size_t> getVertexCounts(
 		{
 			auto primitiveIndex = bvh.primitive_indices[node.first_child_or_primitive + i];
 
-			if (primitiveIndex < 0 || primitiveIndex >= primitives.size())
+			if (primitiveIndex < 0 || primitiveIndex >= binIndexes.size())
 			{
 				repoError << "Bvh primitive index out of range. This means something has gone wrong with the BVH construction.";
 			}
 
-			vertexCount += primitives[primitiveIndex].getNumVertices();
+			auto& node = meshes[binIndexes[primitiveIndex]];
+			vertexCount += node.getNumVertices();
 		}
 
 		vertexCounts[nodeIndex] = vertexCount;
@@ -827,17 +1136,18 @@ std::vector<size_t> getVertexCounts(
 // Groups MeshNodes into clusters based on their location (given by the bounds)
 // and vertex count.
 
-void clusterMeshNodesBvh(
-	const std::vector<repo::core::model::MeshNode>& meshes,
-	std::vector<std::vector<repo::core::model::MeshNode>>& clusters)
+void MultipartOptimizer::clusterMeshNodesBvh(
+	const std::vector<repo::core::model::StreamingMeshNode> &meshes,
+	const std::vector<int> &binIndexes,
+	std::vector<std::vector<int>> &clusters)
 {
-	auto bvh = buildBoundsBvh(meshes);
+	auto bvh = buildBoundsBvh(binIndexes, meshes);
 
 	// The tree contains all the submesh bounds grouped in space.
 	// Create a list of vertex counts for each node so we can decide where to
 	// prune in order to build the clusters.
 
-	auto vertexCounts = getVertexCounts(bvh, meshes);
+	auto vertexCounts = getVertexCounts(bvh, binIndexes, meshes);
 
 	// Next, traverse the tree again, but this time depth first, cutting the tree
 	// at places the vertex count drops below a target threshold.
@@ -849,36 +1159,21 @@ void clusterMeshNodesBvh(
 
 	for (const auto head : branchNodes)
 	{
-		std::vector<repo::core::model::MeshNode> cluster;
+		std::vector<int> cluster;
 		for (const auto primitive : getBranchPrimitives(bvh, head))
 		{
-			cluster.push_back(meshes[primitive]);
+			cluster.push_back(binIndexes[primitive]);
 		}
 		clusters.push_back(cluster);
 	}
 }
 
-bool MultipartOptimizer::isTransparent(
-	const repo::core::model::RepoScene *scene,
-	const repo::core::model::MeshNode  *mesh)
+void MultipartOptimizer::splitBigClusters(
+	std::vector<std::vector<int>>& clusters)
 {
-	bool isTransparent = false;
-	const auto mat = scene->getChildrenNodesFiltered(defaultGraph, mesh->getSharedID(), repo::core::model::NodeType::MATERIAL);
-	if (mat.size())
-	{
-		const repo::core::model::MaterialNode* matNode = (repo::core::model::MaterialNode*)mat[0];
-		const auto matStruct = matNode->getMaterialStruct();
-		isTransparent = matStruct.opacity != 1;
-	}
-
-	return isTransparent;
-}
-
-void splitBigClusters(std::vector<std::vector<repo::core::model::MeshNode>>& clusters)
-{
-	auto clustersToSplit = std::vector<std::vector<repo::core::model::MeshNode>>();
+	auto clustersToSplit = std::vector<std::vector<int>>();
 	clusters.erase(std::remove_if(clusters.begin(), clusters.end(),
-		[&](std::vector<repo::core::model::MeshNode> cluster)
+		[&](std::vector<int> cluster)
 		{
 			if (cluster.size() > REPO_MP_MAX_MESHES_IN_SUPERMESH)
 			{
@@ -893,7 +1188,7 @@ void splitBigClusters(std::vector<std::vector<repo::core::model::MeshNode>>& clu
 		clusters.end()
 	);
 
-	std::vector<repo::core::model::MeshNode> cluster;
+	std::vector<int> cluster;
 	for (auto const clusterToSplit : clustersToSplit)
 	{
 		int i = 0;
@@ -902,20 +1197,20 @@ void splitBigClusters(std::vector<std::vector<repo::core::model::MeshNode>>& clu
 			cluster.push_back(clusterToSplit[i++]);
 			if (cluster.size() >= REPO_MP_MAX_MESHES_IN_SUPERMESH)
 			{
-				clusters.push_back(std::vector<repo::core::model::MeshNode>(cluster));
+				clusters.push_back(std::vector<int>(cluster));
 				cluster.clear();
 			}
 		}
 
 		if (cluster.size())
 		{
-			clusters.push_back(std::vector<repo::core::model::MeshNode>(cluster));
+			clusters.push_back(std::vector<int>(cluster));
 		}
 	}
 }
 
-std::vector<std::vector<repo::core::model::MeshNode>> MultipartOptimizer::clusterMeshNodes(
-	const std::vector<repo::core::model::MeshNode>& meshes
+std::vector<std::vector<int>> MultipartOptimizer::clusterMeshNodes(
+	const std::vector<repo::core::model::StreamingMeshNode>& meshes
 )
 {
 	// Takes one set of MeshNodes and groups them into N sets of MeshNodes,
@@ -928,7 +1223,7 @@ std::vector<std::vector<repo::core::model::MeshNode>> MultipartOptimizer::cluste
 	// nodes by their efficiency
 
 	struct meshMetric {
-		repo::core::model::MeshNode node;
+		int nodeIndex;
 		float efficiency;
 	};
 
@@ -936,9 +1231,10 @@ std::vector<std::vector<repo::core::model::MeshNode>> MultipartOptimizer::cluste
 	for (size_t i = 0; i < meshes.size(); i++)
 	{
 		auto& mesh = metrics[i];
-		mesh.node = meshes[i];
-		auto bounds = mesh.node.getBoundingBox();
-		auto numVertices = mesh.node.getNumVertices();
+		mesh.nodeIndex = i;
+		auto& node = meshes[i];
+		auto bounds = node.getBoundingBox();
+		auto numVertices = node.getNumVertices();
 		auto width = bounds.size().x;
 		auto height = bounds.size().y;
 		auto length = bounds.size().z;
@@ -948,7 +1244,7 @@ std::vector<std::vector<repo::core::model::MeshNode>> MultipartOptimizer::cluste
 
 	std::sort(metrics.begin(), metrics.end(), [](meshMetric a, meshMetric b) {
 		return a.efficiency < b.efficiency;
-	});
+		});
 
 	// Bin the nodes based on vertex count. The bottom 20% will form pure-LOD
 	// groups, while the top 80% will be spatialised.
@@ -961,25 +1257,26 @@ std::vector<std::vector<repo::core::model::MeshNode>> MultipartOptimizer::cluste
 
 	auto modelLowerThreshold = std::max<int>(REPO_MP_MAX_VERTEX_COUNT, (int)(totalVertices * REPO_MODEL_LOW_CLUSTERING_RATIO));
 
-	std::vector<repo::core::model::MeshNode> bin;
+	std::vector<int> bin;
 	auto binVertexCount = 0;
 	auto binsVertexCount = 0;
 
-	std::vector<std::vector<repo::core::model::MeshNode>> clusters;
+	std::vector<std::vector<int>> clusters;
 	auto metricsIterator = metrics.begin();
 
 	while (metricsIterator != metrics.end() && binsVertexCount <= modelLowerThreshold)
 	{
 		auto& item = *metricsIterator++;
+		auto& node = meshes[item.nodeIndex];
 
-		binVertexCount += item.node.getNumVertices();
-		binsVertexCount += item.node.getNumVertices();
-		bin.push_back(item.node);
+		binVertexCount += node.getNumVertices();
+		binsVertexCount += node.getNumVertices();
+		bin.push_back(item.nodeIndex);
 
 		if (binVertexCount >= REPO_MP_MAX_VERTEX_COUNT) // If we've filled up one supermesh
 		{
 			// Copy
-			auto cluster = std::vector<repo::core::model::MeshNode>(bin);
+			auto cluster = std::vector<int>(bin);
 			clusters.push_back(cluster);
 
 			// And reset
@@ -990,7 +1287,7 @@ std::vector<std::vector<repo::core::model::MeshNode>> MultipartOptimizer::cluste
 
 	if (bin.size()) // Either we have some % of the model remaining, or the entire model fits below 65K
 	{
-		auto cluster = std::vector<repo::core::model::MeshNode>(bin);
+		auto cluster = std::vector<int>(bin);
 		clusters.push_back(cluster);
 
 		bin.clear();
@@ -1001,12 +1298,12 @@ std::vector<std::vector<repo::core::model::MeshNode>> MultipartOptimizer::cluste
 	{
 		auto& item = *metricsIterator++;
 
-		bin.push_back(item.node);
+		bin.push_back(item.nodeIndex);
 	}
 
 	if (bin.size())
 	{
-		clusterMeshNodesBvh(bin, clusters);
+		clusterMeshNodesBvh(meshes, bin, clusters);
 	}
 
 	// If clusters contain too many meshes, we can exceed the maximum BSON size.
@@ -1017,128 +1314,4 @@ std::vector<std::vector<repo::core::model::MeshNode>> MultipartOptimizer::cluste
 	repoInfo << "Created " << clusters.size() << " clusters in " << CHRONO_DURATION(start) << " milliseconds.";
 
 	return clusters;
-}
-
-bool MultipartOptimizer::processMeshGroup(
-	const repo::core::model::RepoScene* scene,
-	const MeshMap& bakedMeshNodes,
-	const std::set<repo::lib::RepoUUID>& groupMeshIds,
-	const repo::lib::RepoUUID& rootID,
-	repo::core::model::RepoNodeSet& mergedMeshes,
-	const bool isGrouped
-)
-{
-	// This method turns the submesh UUIDs into a set of supermesh MeshNodes
-	// and duplicate material nodes.
-	// They are added to the mergedMeshes and mergedMeshesMaterials arrays, to
-	// be added to the stash graph when this method has been called for all
-	// supermesh groups.
-
-	if (!groupMeshIds.size())
-	{
-		return true;
-	}
-
-	// First get all the meshes to build into the supermesh set. This snippet
-	// returns the meshes baked into world space (where they should be when
-	// combined).
-
-	std::vector<repo::core::model::MeshNode> nodes;
-	for (const auto id : groupMeshIds)
-	{
-		auto range = bakedMeshNodes.equal_range(id);
-		for (auto pair = range.first; pair != range.second; pair++)
-		{
-			nodes.push_back(pair->second);
-		}
-	}
-
-	// Next partition them into sets that should form the supermeshes
-
-	auto clusters = clusterMeshNodes(nodes);
-
-	// Build the actual supermesh nodes that hold the combined or split geometry from the sets
-
-	std::vector<repo::core::model::SupermeshNode*> supermeshes;
-	for (const auto cluster : clusters)
-	{
-		createSuperMeshes(scene, cluster, isGrouped, supermeshes);
-	}
-
-	for (const auto supermesh : supermeshes)
-	{
-		supermesh->addParent(rootID);
-		mergedMeshes.insert(supermesh);
-	}
-
-	return true;
-}
-
-void MultipartOptimizer::sortMeshes(
-	const repo::core::model::RepoScene                                      *scene,
-	const repo::core::model::RepoNodeSet                                    &meshes,
-	std::unordered_map<std::string, std::unordered_map<uint32_t, std::vector<std::set<repo::lib::RepoUUID>>>>	&normalMeshes,
-	std::unordered_map < std::string, std::unordered_map<uint32_t, std::vector<std::set<repo::lib::RepoUUID>>>>	&transparentMeshes,
-	std::unordered_map < std::string, std::unordered_map < uint32_t, std::unordered_map < repo::lib::RepoUUID,
-	std::vector<std::set<repo::lib::RepoUUID>>, repo::lib::RepoUUIDHasher >>> &texturedMeshes
-)
-{
-	for (const auto &node : meshes)
-	{
-		auto mesh = (repo::core::model::MeshNode*) node;
-		if (!mesh->getVertices().size() || !mesh->getFaces().size())
-		{
-			repoWarning << "mesh " << mesh->getUniqueID() << " has no vertices/faces, skipping...";
-			continue;
-		}
-		auto meshGroup = mesh->getGrouping();
-
-		/**
-		* 1 - figure out it's mFormat (what buffers, flags and primitives does it have)
-		* 2 - check if it has texture
-		* 3 - if not, check if it is transparent
-		*/
-		uint32_t mFormat = mesh->getMFormat();
-
-		repo::lib::RepoUUID texID;
-		if (hasTexture(scene, mesh, texID))
-		{
-			if (texturedMeshes.find(meshGroup) == texturedMeshes.end()) {
-				texturedMeshes[meshGroup] = std::unordered_map < uint32_t, std::unordered_map < repo::lib::RepoUUID,
-					std::vector<std::set<repo::lib::RepoUUID>>, repo::lib::RepoUUIDHasher >>();
-			}
-
-			auto it = texturedMeshes[meshGroup].find(mFormat);
-			if (it == texturedMeshes[meshGroup].end())
-			{
-				texturedMeshes[meshGroup][mFormat] = std::unordered_map<repo::lib::RepoUUID, std::vector<std::set<repo::lib::RepoUUID>>, repo::lib::RepoUUIDHasher>();
-			}
-			auto it2 = texturedMeshes[meshGroup][mFormat].find(texID);
-
-			if (it2 == texturedMeshes[meshGroup][mFormat].end())
-			{
-				texturedMeshes[meshGroup][mFormat][texID] = std::vector<std::set<repo::lib::RepoUUID>>();
-				texturedMeshes[meshGroup][mFormat][texID].push_back(std::set<repo::lib::RepoUUID>());
-			}
-			texturedMeshes[meshGroup][mFormat][texID].back().insert(mesh->getUniqueID());
-		}
-		else
-		{
-			//no texture, check if it is transparent
-			const bool istransParentMesh = isTransparent(scene, mesh);
-			auto &meshMap = istransParentMesh ? transparentMeshes : normalMeshes;
-
-			if (meshMap.find(meshGroup) == meshMap.end()) {
-				meshMap[meshGroup] = std::unordered_map<uint32_t, std::vector<std::set<repo::lib::RepoUUID>>>();
-			}
-
-			auto it = meshMap[meshGroup].find(mFormat);
-			if (it == meshMap[meshGroup].end())
-			{
-				meshMap[meshGroup][mFormat] = std::vector<std::set<repo::lib::RepoUUID>>();
-				meshMap[meshGroup][mFormat].push_back(std::set<repo::lib::RepoUUID>());
-			}
-			meshMap[meshGroup][mFormat].back().insert(mesh->getUniqueID());
-		}
-	}
 }
\ No newline at end of file
diff --git a/bouncer/src/repo/manipulator/modeloptimizer/repo_optimizer_multipart.h b/bouncer/src/repo/manipulator/modeloptimizer/repo_optimizer_multipart.h
index 4b4ffbdd..0617b08b 100644
--- a/bouncer/src/repo/manipulator/modeloptimizer/repo_optimizer_multipart.h
+++ b/bouncer/src/repo/manipulator/modeloptimizer/repo_optimizer_multipart.h
@@ -25,47 +25,37 @@
 #include "../../core/model/bson/repo_node_mesh.h"
 #include "../../core/model/bson/repo_node_supermesh.h"
 #include "../../core/model/bson/repo_node_material.h"
+#include "../../core/handler/database/repo_query.h"
+#include "../../core/handler/fileservice/repo_blob_files_handler.h"
+#include "../../lib/datastructure/repo_structs.h"
+#include "bvh/bvh.hpp"
+#include "bvh/sweep_sah_builder.hpp"
+#include <repo/manipulator/modelconvertor/export/repo_model_export_abstract.h>
+#include <repo/core/model/bson/repo_bson.h>
+#include <repo/core/model/bson/repo_node_streaming_mesh.h>
 
 namespace repo {
 	namespace manipulator {
 		namespace modeloptimizer {
 			class MultipartOptimizer
 			{
-			public:
-				/**
-				* Apply optimisation on the given repoScene
-				* @param scene takes in a repoScene to optimise
-				* @return returns true upon success
-				*/
-				bool apply(repo::core::model::RepoScene *scene);
+				
+				typedef float Scalar;
+				typedef bvh::Bvh<Scalar> Bvh;
+				typedef bvh::Vector3<Scalar> BvhVector3;
 
-			private:
-				/*
-				* Maps between two Repo UUIDs
-				*/
-				using UUIDMap = std::unordered_map<repo::lib::RepoUUID, repo::lib::RepoUUID, repo::lib::RepoUUIDHasher>;
-
-				/*
-				* A dictionary of MaterialNodes indexed by UUIDs
-				*/
-				using MaterialMap = std::unordered_map<repo::lib::RepoUUID, repo::core::model::MaterialNode*, repo::lib::RepoUUIDHasher>;
+			public:
+				
+				bool processScene(
+					std::string database,
+					std::string collection,
+					repo::lib::RepoUUID revId,
+					repo::core::handler::AbstractDatabaseHandler *handler,
+					repo::manipulator::modelconvertor::AbstractModelExport *exporter
+				);
 
-				/*
-				* A dictionary of MeshNodes indexed by UUIDs. Each UUID may map to multiple nodes.
-				*/
-				using MeshMap = std::unordered_multimap<repo::lib::RepoUUID, repo::core::model::MeshNode, repo::lib::RepoUUIDHasher>;
 
-				/**
-				* Recursively enumerates scene to find all the instances of
-				* MeshNodes that match the Id, and returns copies of them
-				* with geometry baked to its world space location in the
-				* scene graph.
-				*/
-				bool getBakedMeshNodes(
-					const repo::core::model::RepoScene* scene,
-					const repo::core::model::RepoNode* node,
-					repo::lib::RepoMatrix mat,
-					MeshMap &nodes);
+			private:
 
 				/**
 				* Represents a batched set of geometry.
@@ -78,10 +68,122 @@ namespace repo {
 					std::vector<repo::lib::repo_mesh_mapping_t> meshMapping;
 				};
 
-				void appendMesh(
-					const repo::core::model::RepoScene* scene,
-					repo::core::model::MeshNode node,
-					mapped_mesh_t& mappedMesh
+				struct ProcessingJob {
+					std::string description;
+					repo::core::handler::database::query::RepoQuery filter;
+					repo::lib::RepoUUID texId;
+
+					bool isTexturedJob() const {
+						return !texId.isDefaultValue();
+					}
+				};							
+				
+
+				typedef std::unordered_map <repo::lib::RepoUUID, std::shared_ptr<repo::core::model::MaterialNode>, repo::lib::RepoUUIDHasher> MaterialPropMap;
+				typedef std::unordered_map<repo::lib::RepoUUID, repo::lib::RepoMatrix, repo::lib::RepoUUIDHasher> TransformMap;
+
+				std::unordered_map<repo::lib::RepoUUID, repo::lib::RepoMatrix, repo::lib::RepoUUIDHasher> getAllTransforms(
+					repo::core::handler::AbstractDatabaseHandler *handler,
+					const std::string &database,
+					const std::string &collection,
+					const repo::lib::RepoUUID &revId
+				);
+
+				void traverseTransformTree(
+					const repo::core::model::RepoBSON &root,
+					const std::unordered_map<repo::lib::RepoUUID, std::vector<repo::core::model::RepoBSON>, repo::lib::RepoUUIDHasher> &childNodeMap,
+					std::unordered_map<repo::lib::RepoUUID,	repo::lib::RepoMatrix, repo::lib::RepoUUIDHasher> &transforms);
+
+				MaterialPropMap getAllMaterials(
+					repo::core::handler::AbstractDatabaseHandler *handler,
+					const std::string &database,
+					const std::string &collection,
+					const repo::lib::RepoUUID &revId
+				);
+
+				std::set<std::string> getAllGroupings(
+					repo::core::handler::AbstractDatabaseHandler* handler,
+					const std::string& database,
+					const std::string& collection,
+					const repo::lib::RepoUUID& revId
+				);
+
+				std::vector < repo::lib::RepoUUID> getAllTextureIds(
+					repo::core::handler::AbstractDatabaseHandler *handler,
+					const std::string &database,
+					const std::string &collection,
+					const repo::lib::RepoUUID &revId,
+					const std::string& grouping
+				);
+
+				ProcessingJob createUntexturedJob(
+					const std::string &description,
+					const repo::lib::RepoUUID &revId,
+					const int primitive,
+					const std::string &grouping,
+					const bool isOpaque
+				);
+
+				ProcessingJob createTexturedJob(
+					const std::string &description,
+					const repo::lib::RepoUUID &revId,
+					const int primitive,
+					const std::string &grouping,
+					const repo::lib::RepoUUID &texId
+				);
+
+				void clusterAndSupermesh(
+					const std::string &database,
+					const std::string &collection,
+					repo::core::handler::AbstractDatabaseHandler *handler,
+					repo::manipulator::modelconvertor::AbstractModelExport *exporter,
+					const TransformMap& transformMap,
+					const MaterialPropMap& matPropMap,
+					const ProcessingJob &job
+				);
+
+				void createSuperMeshes(
+					const std::string &database,
+					const std::string &collection,
+					repo::core::handler::AbstractDatabaseHandler *handler,
+					repo::manipulator::modelconvertor::AbstractModelExport *exporter,
+					const TransformMap& transformMap,
+					const MaterialPropMap& matPropMap,
+					std::vector<repo::core::model::StreamingMeshNode>& meshNodes,
+					const std::vector<std::vector<int>>& clusters,
+					const repo::lib::RepoUUID &texId
+				);
+
+				void createSuperMesh(
+					repo::manipulator::modelconvertor::AbstractModelExport *exporter,
+					const mapped_mesh_t& mappedMesh
+				);
+
+				void appendMesh(					
+					repo::core::model::StreamingMeshNode &node,
+					const MaterialPropMap &matPropMap,
+					mapped_mesh_t &mappedMesh,
+					const repo::lib::RepoUUID &texId
+				);
+
+				Bvh buildFacesBvh(
+					repo::core::model::StreamingMeshNode &node
+				);
+
+				void flattenBvh(
+					const Bvh &bvh,
+					std::vector<size_t> &leaves,
+					std::vector<size_t> &branches
+				);
+
+				std::vector<size_t> getBranchPrimitives(
+					const Bvh& bvh,
+					size_t head
+				);
+
+				std::vector<std::set<uint32_t>> getUniqueVertices(
+					const Bvh& bvh,
+					const std::vector<repo::lib::repo_face_t>& primitives // The primitives in this tree are faces
 				);
 
 				/*
@@ -89,119 +191,52 @@ namespace repo {
 				* each mapped_mesh_t has a vertex count below a certain size.
 				*/
 				void splitMesh(
-					const repo::core::model::RepoScene* scene,
-					repo::core::model::MeshNode node,
-					std::vector<mapped_mesh_t> &mappedMeshes
+					repo::core::model::StreamingMeshNode &node,
+					repo::manipulator::modelconvertor::AbstractModelExport *exporter,
+					const MaterialPropMap &matPropMap,
+					const repo::lib::RepoUUID &texId
 				);
 
 				/*
 				* Turns a mapped_mesh_t into a MeshNode that can be added to the database
 				*/
-				repo::core::model::SupermeshNode* createSupermeshNode(
-					const mapped_mesh_t& mapped,
-					bool isGrouped
+				std::unique_ptr < repo::core::model::SupermeshNode> createSupermeshNode(
+					const mapped_mesh_t &mapped
 				);
 
-				/**
-				* Builds a set of Supermesh MeshNodes, based on the UUIDs in meshGroup.
-				* The method may output an arbitrary number (including 1) supermeshes,
-				* from an arbitrary number (including 1) UUIDs. If no UUIDs are provided,
-				* no Supermeshes are created.
-				* Each Supermesh mapping re-maps its Material UUID to a new UUID, which
-				* is used to copy the materials. If an existing remapping exists, it is
-				* used, otherwise one is created on demand.
-				* Each Supermesh is guaranteed to be below a certain size.
-				*/
-				void createSuperMeshes(
-					const repo::core::model::RepoScene *scene,
-					const std::vector<repo::core::model::MeshNode> &nodes,
-					const bool isGrouped,
-					std::vector<repo::core::model::SupermeshNode*> &supermeshNodes);
-
-				/**
-				* Generate the multipart scene
-				* @param scene scene to base on, this will also be modified to store the stash graph
-				* @return returns true upon success
-				*/
-				bool generateMultipartScene(repo::core::model::RepoScene *scene);
 
 				/**
-				* Get child's material id from a mesh
-				* @param scene scene it belongs to
-				* @param mesh mesh in question
-				* @return returns unqiue ID of the material
+				* Groups the MeshNodes into sets based on their location and
+				* geometry size.
 				*/
-				repo::lib::RepoUUID getMaterialID(
-					const repo::core::model::RepoScene *scene,
-					const repo::core::model::MeshNode  *mesh
+				std::vector<std::vector<int>> clusterMeshNodes(
+					const std::vector<repo::core::model::StreamingMeshNode>& nodes
 				);
 
-				/**
-				* Check if the mesh has texture
-				* @param scene scene as reference
-				* @param mesh mesh to check
-				* @param texID texID to return (if any)
-				* @return returns true if there is texture
-				*/
-				bool hasTexture(
-					const repo::core::model::RepoScene *scene,
-					const repo::core::model::MeshNode *mesh,
-					repo::lib::RepoUUID &texID);
-
-				/**
-				* Check if the mesh is (semi) Transparent
-				* @param scene scene as reference
-				* @param mesh mesh to check
-				* @return returns true if the mesh is not fully opaque
-				*/
-				bool isTransparent(
-					const repo::core::model::RepoScene *scene,
-					const repo::core::model::MeshNode  *mesh);
+				void clusterMeshNodesBvh(
+					const std::vector<repo::core::model::StreamingMeshNode>& meshes,
+					const std::vector<int>& binIndexes,
+					std::vector<std::vector<int>>& clusters);
 
-				/**
-				* Creates a set of supermesh MeshNodes which contain the submeshes
-				* in meshes (passed by UUID), along with a set of duplicate material
-				* nodes for use by the supermeshes.
-				* Multiple supermeshes may be created, depending on the composition
-				* of meshes.
-				* Supermeshes and duplicated materials are passed back via the
-				* mergedMeshes and matNodes arrays.
-				* The matIDs map is used to map between the original graph materials
-				* and the stash graph materials, in case this method is called multiple
-				* times for one graph.
-				*/
-				bool processMeshGroup(
-					const repo::core::model::RepoScene *scene,
-					const MeshMap &bakedMeshNodes,
-					const std::set<repo::lib::RepoUUID>	&groupMeshIds,
-					const repo::lib::RepoUUID &rootID,
-					repo::core::model::RepoNodeSet &mergedMeshes,
-					const bool isGrouped);
+				Bvh buildBoundsBvh(
+					const std::vector<int>& binIndexes,
+					const std::vector<repo::core::model::StreamingMeshNode>& meshes
+				);
 
-				/**
-				* Groups the MeshNodes into sets based on their location and
-				* geometry size.
-				*/
-				std::vector<std::vector<repo::core::model::MeshNode>> clusterMeshNodes(
-					const std::vector<repo::core::model::MeshNode>& nodes
+				std::vector<size_t> getVertexCounts(
+					const Bvh& bvh,
+					const std::vector<int>& binIndexes,
+					const std::vector<repo::core::model::StreamingMeshNode>& meshes
 				);
 
-				/**
-				* Sort the given RepoNodeSet of meshes for multipart merging
-				* @param scene             scene as reference
-				* @param meshes            meshes to sort
-				* @param normalMeshes      container to store normal meshes
-				* @param transparentMeshes container to store (semi)transparent meshes
-				* @param texturedMeshes    container to store textured meshes
-				*/
-				void sortMeshes(
-					const repo::core::model::RepoScene *scene,
-					const repo::core::model::RepoNodeSet &meshes,
-					std::unordered_map<std::string, std::unordered_map<uint32_t, std::vector<std::set<repo::lib::RepoUUID>>>>	&normalMeshes,
-					std::unordered_map < std::string, std::unordered_map<uint32_t, std::vector<std::set<repo::lib::RepoUUID>>>>	&transparentMeshes,
-					std::unordered_map < std::string, std::unordered_map < uint32_t, std::unordered_map < repo::lib::RepoUUID,
-					std::vector<std::set<repo::lib::RepoUUID>>, repo::lib::RepoUUIDHasher >>> &texturedMeshes
+				std::vector<size_t> getSupermeshBranchNodes(
+					const Bvh &bvh,
+					const std::vector<size_t> &vertexCounts);
+
+				void splitBigClusters(
+					std::vector<std::vector<int>>& clusters
 				);
+								
 			};
 		}
 	}
diff --git a/bouncer/src/repo/manipulator/modelutility/repo_mesh_map_reorganiser.cpp b/bouncer/src/repo/manipulator/modelutility/repo_mesh_map_reorganiser.cpp
index 6b9a4eee..97b2c92d 100644
--- a/bouncer/src/repo/manipulator/modelutility/repo_mesh_map_reorganiser.cpp
+++ b/bouncer/src/repo/manipulator/modelutility/repo_mesh_map_reorganiser.cpp
@@ -142,7 +142,7 @@ MeshMapReorganiser::getSplitMapping() const {
 	return reMapSuccess ? splitMap : std::unordered_map<repo::lib::RepoUUID, std::vector<uint32_t>, repo::lib::RepoUUIDHasher>();
 }
 
-repo::core::model::SupermeshNode MeshMapReorganiser::getRemappedMesh() const
+std::unique_ptr<repo::core::model::SupermeshNode> MeshMapReorganiser::getRemappedMesh() const
 {
 	if (reMapSuccess)
 	{
@@ -153,7 +153,7 @@ repo::core::model::SupermeshNode MeshMapReorganiser::getRemappedMesh() const
 			newIds.insert(newIds.end(), buf.begin(), buf.end());
 		}
 
-		auto newMesh = repo::core::model::RepoBSONFactory::makeSupermeshNode(
+		return repo::core::model::RepoBSONFactory::makeSupermeshNode(
 			newVertices,
 			newFaces,
 			newNormals,
@@ -162,13 +162,11 @@ repo::core::model::SupermeshNode MeshMapReorganiser::getRemappedMesh() const
 			reMappedMappings,
 			mesh->getUniqueID(),
 			mesh->getSharedID(),
-			newIds);
-
-		return newMesh;
+			newIds);		
 	}
 	else
 	{
-		return repo::core::model::SupermeshNode();
+		return std::make_unique<repo::core::model::SupermeshNode>();
 	}
 }
 
diff --git a/bouncer/src/repo/manipulator/modelutility/repo_mesh_map_reorganiser.h b/bouncer/src/repo/manipulator/modelutility/repo_mesh_map_reorganiser.h
index 5a82d682..c4674e02 100644
--- a/bouncer/src/repo/manipulator/modelutility/repo_mesh_map_reorganiser.h
+++ b/bouncer/src/repo/manipulator/modelutility/repo_mesh_map_reorganiser.h
@@ -73,7 +73,7 @@ namespace repo {
 				* Get the mesh, with mesh mappings and buffers modified
 				* @return returns the modified mesh
 				*/
-				repo::core::model::SupermeshNode getRemappedMesh() const;
+				std::unique_ptr<repo::core::model::SupermeshNode> getRemappedMesh() const;
 
 				/**
 				* Return serialised faces of the modified mesh
diff --git a/bouncer/src/repo/manipulator/modelutility/repo_scene_builder.cpp b/bouncer/src/repo/manipulator/modelutility/repo_scene_builder.cpp
index 9ca3801f..18ac8859 100644
--- a/bouncer/src/repo/manipulator/modelutility/repo_scene_builder.cpp
+++ b/bouncer/src/repo/manipulator/modelutility/repo_scene_builder.cpp
@@ -192,6 +192,19 @@ std::shared_ptr<T> RepoSceneBuilder::addNode(const T& node)
 
 void RepoSceneBuilder::addNode(std::unique_ptr<repo::core::model::RepoNode> node)
 {
+
+	// If it is a mesh node, we need to handle materials and textures
+	auto meshNode = dynamic_cast<repo::core::model::MeshNode*>(node.get());
+	if (meshNode) {
+		auto material = meshNode->getMaterial();
+		addMaterialReference(material, meshNode->getSharedID());
+
+		if (material.hasTexture()) {
+			if(textureToUniqueId.find(material.texturePath) != textureToUniqueId.end())
+				meshNode->setTextureId(textureToUniqueId[material.texturePath]);
+		}
+	}
+
 	node->setRevision(revisionId);
 	queueNode(node.release());
 }
@@ -319,7 +332,7 @@ void RepoSceneBuilder::addParent(repo::lib::RepoUUID nodeUniqueId, repo::lib::Re
 
 	if (parentUpdates.find(nodeUniqueId) != parentUpdates.end())
 	{
-		parentUpdates[nodeUniqueId]->parentIds.push_back(parentSharedId);
+		parentUpdates[nodeUniqueId]->parentIds.insert(parentSharedId);
 	}
 	else
 	{
@@ -352,14 +365,103 @@ repo::manipulator::modelconvertor::ModelUnits RepoSceneBuilder::getUnits()
 	return this->units;
 }
 
-void RepoSceneBuilder::createIndexes()
+void RepoSceneBuilder::createIndexes(bool groupingsEnabled)
 {
 	using namespace repo::core::handler::database::index;
-	handler->createIndex(databaseName, projectName + "." + REPO_COLLECTION_HISTORY, Descending({ REPO_NODE_REVISION_LABEL_TIMESTAMP }));
-	handler->createIndex(databaseName, projectName + "." + REPO_COLLECTION_SCENE, Ascending({ REPO_NODE_REVISION_ID, "metadata.key", "metadata.value" }));
-	handler->createIndex(databaseName, projectName + "." + REPO_COLLECTION_SCENE, Ascending({ "metadata.key", "metadata.value" }));
-	handler->createIndex(databaseName, projectName + "." + REPO_COLLECTION_SCENE, Ascending({ REPO_NODE_REVISION_ID, REPO_NODE_LABEL_SHARED_ID, REPO_LABEL_TYPE }));
-	handler->createIndex(databaseName, projectName + "." + REPO_COLLECTION_SCENE, Ascending({ REPO_NODE_LABEL_SHARED_ID }));
+	auto historyCollection = projectName + "." + REPO_COLLECTION_HISTORY;
+	auto sceneCollection = projectName + "." + REPO_COLLECTION_SCENE;
+
+	handler->createIndex(databaseName, historyCollection, Descending({ REPO_NODE_REVISION_LABEL_TIMESTAMP }));
+	handler->createIndex(databaseName, sceneCollection, Ascending({ REPO_NODE_REVISION_ID, "metadata.key", "metadata.value" }));
+	handler->createIndex(databaseName, sceneCollection, Ascending({ "metadata.key", "metadata.value" }));
+	handler->createIndex(databaseName, sceneCollection, Ascending({ REPO_NODE_REVISION_ID, REPO_NODE_LABEL_SHARED_ID, REPO_LABEL_TYPE }));
+	handler->createIndex(databaseName, sceneCollection, Ascending({ REPO_NODE_LABEL_SHARED_ID }));
+
+	// Indexes for filtering during the supermeshing
+
+	// For getting all Transforms and all Materials
+	// For transforms, this cannot be a covered query because REPO_NODE_LABEL_MATRIX and REPO_NODE_LABEL_PARENTS are both arrays
+	// For materials, this cannot be a covered query because parents, ambient, and diffuse are all arrays
+	handler->createIndex(databaseName, sceneCollection, Ascending({ REPO_NODE_REVISION_ID, REPO_NODE_LABEL_TYPE }));
+
+	// For all groupings
+	// Should be covered query
+	if (groupingsEnabled)
+		handler->createIndex(databaseName, sceneCollection, Ascending({ REPO_NODE_REVISION_ID, REPO_NODE_LABEL_TYPE, REPO_NODE_MESH_LABEL_GROUPING }), true);
+	
+	// For the opaque group
+	// Cannot be a covered query because parents and bounding box are both arrays
+	if(groupingsEnabled)
+		handler->createIndex(databaseName, sceneCollection, Ascending({
+			REPO_NODE_REVISION_ID,
+			REPO_NODE_MESH_LABEL_PRIMITIVE,
+			REPO_NODE_MESH_LABEL_GROUPING,
+			REPO_FILTER_TAG_OPAQUE
+			}), true);
+	else
+		handler->createIndex(databaseName, sceneCollection, Ascending({
+			REPO_NODE_REVISION_ID,
+			REPO_NODE_MESH_LABEL_PRIMITIVE,
+			REPO_FILTER_TAG_OPAQUE
+			}), true);
+
+	// For the transparent group
+	// Cannot be a covered query because parents and bounding box are both arrays
+	if (groupingsEnabled)
+		handler->createIndex(databaseName, sceneCollection, Ascending({
+			REPO_NODE_REVISION_ID,
+			REPO_NODE_MESH_LABEL_PRIMITIVE,
+			REPO_NODE_MESH_LABEL_GROUPING,
+			REPO_FILTER_TAG_TRANSPARENT
+			}), true);
+	else
+		handler->createIndex(databaseName, sceneCollection, Ascending({
+			REPO_NODE_REVISION_ID,
+			REPO_NODE_MESH_LABEL_PRIMITIVE,
+			REPO_FILTER_TAG_TRANSPARENT
+			}), true);
+
+	// Get all texture ids
+	// Should be a covered query
+	if (groupingsEnabled)
+		handler->createIndex(databaseName, sceneCollection, Ascending({
+			REPO_NODE_REVISION_ID,
+			REPO_NODE_LABEL_TYPE,
+			REPO_NODE_MESH_LABEL_GROUPING
+			}), true);
+	// Would be this for without groupings, but already set above in the initial block
+	//else
+	//	handler->createIndex(databaseName, sceneCollection, Ascending({
+	//		REPO_NODE_REVISION_ID,
+	//		REPO_NODE_LABEL_TYPE,
+	//		REPO_NODE_LABEL_SHARED_ID
+	//		}));
+
+	// For the textured group
+	// Cannot be a covered query because parents and bounding box are both arrays
+	if (groupingsEnabled)
+		handler->createIndex(databaseName, sceneCollection, Ascending({
+			REPO_NODE_REVISION_ID,		
+			REPO_NODE_MESH_LABEL_PRIMITIVE,
+			REPO_NODE_MESH_LABEL_GROUPING,
+			REPO_FILTER_TAG_TEXTURE_ID
+			}), true);
+	else
+		handler->createIndex(databaseName, sceneCollection, Ascending({
+			REPO_NODE_REVISION_ID,
+			REPO_FILTER_TAG_TEXTURE_ID
+			}), true);
+	
+	// For pulling extra mesh information during the supermeshing
+	// Should be a covered query
+	handler->createIndex(databaseName, sceneCollection, Ascending({
+		REPO_NODE_LABEL_SHARED_ID,
+		REPO_NODE_MESH_LABEL_VERTICES_COUNT,
+		REPO_NODE_MESH_LABEL_FACES_COUNT,
+		REPO_NODE_MESH_LABEL_UV_CHANNELS_COUNT,
+		REPO_NODE_MESH_LABEL_PRIMITIVE,
+		REPO_LABEL_BINARY_REFERENCE
+		}), true);
 }
 
 RepoSceneBuilder::AsyncImpl::AsyncImpl(RepoSceneBuilder* builder):
diff --git a/bouncer/src/repo/manipulator/modelutility/repo_scene_builder.h b/bouncer/src/repo/manipulator/modelutility/repo_scene_builder.h
index 741950d0..26bc727c 100644
--- a/bouncer/src/repo/manipulator/modelutility/repo_scene_builder.h
+++ b/bouncer/src/repo/manipulator/modelutility/repo_scene_builder.h
@@ -115,7 +115,7 @@ namespace repo {
 				* an index on a large number of nodes at the end.
 				* Some of the indexes this creats could be moved to .io in the future.
 				*/
-				void createIndexes();
+				void createIndexes(bool groupingsEnabled);
 
 			private:
 
diff --git a/bouncer/src/repo/manipulator/modelutility/repo_scene_manager.cpp b/bouncer/src/repo/manipulator/modelutility/repo_scene_manager.cpp
index d2abf1a5..c025064c 100644
--- a/bouncer/src/repo/manipulator/modelutility/repo_scene_manager.cpp
+++ b/bouncer/src/repo/manipulator/modelutility/repo_scene_manager.cpp
@@ -20,7 +20,6 @@
 #include "repo/core/model/bson/repo_bson_teamspace.h"
 #include "../../error_codes.h"
 #include "../modeloptimizer/repo_optimizer_multipart.h"
-#include "../modeloptimizer/repo_optimizer_multipart.h"
 #include "../modelutility/repo_maker_selection_tree.h"
 
 #ifdef REPO_ASSET_GENERATOR_SUPPORT
@@ -30,65 +29,15 @@
 using namespace repo::lib;
 using namespace repo::manipulator::modelutility;
 
-bool SceneManager::commitWebBuffers(
-	repo::core::model::RepoScene                          *scene,
-	const std::string                                     &geoStashExt,
-	const repo_web_buffers_t                              &resultBuffers,
-	repo::core::handler::AbstractDatabaseHandler          *handler,
-	repo::core::handler::fileservice::FileManager         *fileManager)
-{
-	bool success = true;
-	std::string jsonStashExt = REPO_COLLECTION_STASH_JSON;
-	std::string repoAssetsStashExt = REPO_COLLECTION_STASH_BUNDLE;
-	std::string databaseName = scene->getDatabaseName();
-	std::string projectName = scene->getProjectName();
-
-	//Upload the files
-	for (const auto &bufferPair : resultBuffers.geoFiles)
-	{
-		if (success &= fileManager->uploadFileAndCommit(databaseName, projectName + "." + geoStashExt, bufferPair.first, bufferPair.second, {}, repo::core::handler::fileservice::FileManager::Encoding::Gzip)) // Web geometry files are gzipped by default
-		{
-			repoInfo << "File (" << bufferPair.first << ") added successfully to file storage.";
-		}
-		else
-		{
-			repoError << "Failed to add file  (" << bufferPair.first << ") to file storage";
-		}
-	}
-
-	for (const auto &bufferPair : resultBuffers.jsonFiles)
-	{
-		std::string errMsg;
-		std::string fileName = bufferPair.first;
-
-		if (success &= fileManager->uploadFileAndCommit(databaseName, projectName + "." + jsonStashExt, bufferPair.first, bufferPair.second))
-		{
-			repoInfo << "File (" << fileName << ") added successfully to file storage.";
-		}
-		else
-		{
-			repoError << "Failed to add file  (" << fileName << ") to  file storage.";
-		}
-	}
-
-	std::string errMsg;
-
-	if (!resultBuffers.repoAssets.isEmpty())
-	{
-		handler->upsertDocument(databaseName, projectName + "." + repoAssetsStashExt, resultBuffers.repoAssets, true);
-	}
-
-	return success;
-}
 
 uint8_t SceneManager::commitScene(
-	repo::core::model::RepoScene                          *scene,
-	const std::string									  &owner,
-	const std::string									  &tag,
-	const std::string									  &desc,
-	const repo::lib::RepoUUID              &revId,
-	repo::core::handler::AbstractDatabaseHandler          *handler,
-	repo::core::handler::fileservice::FileManager         *fileManager
+	repo::core::model::RepoScene									*scene,
+	const std::string												&owner,
+	const std::string												&tag,
+	const std::string												&desc,
+	const repo::lib::RepoUUID										&revId,
+	repo::core::handler::AbstractDatabaseHandler					*handler,
+	repo::core::handler::fileservice::FileManager					*fileManager
 ) {
 	uint8_t errCode = REPOERR_UPLOAD_FAILED;
 	std::string msg;
@@ -122,8 +71,7 @@ uint8_t SceneManager::commitScene(
 			{
 				repoInfo << "Generating Repo Bundles...";
 				scene->updateRevisionStatus(handler, repo::core::model::ModelRevisionNode::UploadStatus::GEN_WEB_STASH);
-				repo_web_buffers_t buffers;
-				if (success = generateWebViewBuffers(scene, repo::manipulator::modelconvertor::WebExportType::REPO, buffers, handler, fileManager))
+				if (success = generateWebViewBuffers(scene, repo::manipulator::modelconvertor::ExportType::REPO, handler))
 					repoInfo << "Repo Bundles for Stash stored into the database";
 				else
 					repoError << "failed to commit Repo Bundles";
@@ -157,7 +105,7 @@ uint8_t SceneManager::commitScene(
 }
 
 repo::core::model::RepoScene* SceneManager::fetchScene(
-	repo::core::handler::AbstractDatabaseHandler *handler,
+	repo::core::handler::AbstractDatabaseHandler  *handler,
 	const std::string                             &database,
 	const std::string                             &project,
 	const repo::lib::RepoUUID                     &uuid,
@@ -260,92 +208,78 @@ void SceneManager::fetchScene(
 	}
 }
 
-bool SceneManager::generateStashGraph(
-	repo::core::model::RepoScene              *scene
-)
+bool SceneManager::generateWebViewBuffers(
+	repo::core::model::RepoScene									*scene,
+	const repo::manipulator::modelconvertor::ExportType				&exType,
+	repo::core::handler::AbstractDatabaseHandler					*handler)
 {
-	bool success = false;
-	if (success = (scene && scene->hasRoot(repo::core::model::RepoScene::GraphType::DEFAULT)))
+	bool validScene =
+		scene
+		&& scene->isRevisioned()
+		&& scene->hasRoot(repo::core::model::RepoScene::GraphType::DEFAULT);
+		
+	if (validScene)
 	{
-		removeStashGraph(scene);
-
-		repoInfo << "Generating stash graph...";
-
-		repo::manipulator::modeloptimizer::MultipartOptimizer mpOpt;
-		if (!(success = mpOpt.apply(scene)))
+		auto database = scene->getDatabaseName();
+		auto collection = scene->getProjectName();
+		auto revId = scene->getRevisionID();
+
+		// Get world offset
+		std::vector<double> worldOffset;
+		auto bson = handler->findOneByUniqueID(database, collection + "." + REPO_COLLECTION_HISTORY, revId);
+		if (bson.hasField(REPO_NODE_REVISION_LABEL_WORLD_COORD_SHIFT))
 		{
-			repoError << "Failed to generate stash graph with MultipartOptimizer.";
+			worldOffset = bson.getDoubleVectorField(REPO_NODE_REVISION_LABEL_WORLD_COORD_SHIFT);
 		}
-	}
-	else
-	{
-		repoError << "Failed to generate stash graph: nullptr to scene or empty scene graph!";
-	}
-
-	return success;
-}
-
-bool SceneManager::generateWebViewBuffers(
-	repo::core::model::RepoScene                           *scene,
-	const repo::manipulator::modelconvertor::WebExportType &exType,
-	repo_web_buffers_t                                     &resultBuffers,
-	repo::core::handler::AbstractDatabaseHandler           *handler,
-	repo::core::handler::fileservice::FileManager         *fileManager)
-{
-	bool success = false;
-	if (success = (scene && scene->isRevisioned()))
-	{
-		bool toCommit = handler;
-
-		std::string geoStashExt;
-		std::string jsonStashExt = REPO_COLLECTION_STASH_JSON;
-
-		if (!scene->hasRoot(repo::core::model::RepoScene::GraphType::OPTIMIZED)) {
-
-			repoInfo << "Optimised scene not found. Attempt to generate...";
-
-			if (!scene->getAllMeshes(repo::core::model::RepoScene::GraphType::DEFAULT).size()) {
-				std::string msg;
-				scene->loadScene(handler, msg);
-			}
-			generateStashGraph(scene);
+		else {
+			worldOffset = std::vector<double>({ 0, 0, 0 });
 		}
 
+		// Initialise exporter		
+		std::unique_ptr<repo::manipulator::modelconvertor::AbstractModelExport> exporter = nullptr;
+
 		switch (exType)
 		{
-		case repo::manipulator::modelconvertor::WebExportType::REPO:
-			geoStashExt = REPO_COLLECTION_STASH_BUNDLE;
-			resultBuffers = generateRepoBundleBuffer(scene);
+		case repo::manipulator::modelconvertor::ExportType::REPO:
+#ifdef REPO_ASSET_GENERATOR_SUPPORT
+			exporter = std::make_unique<repo::manipulator::modelconvertor::RepoBundleExport>(
+				handler,
+				database,
+				collection,
+				revId,
+				worldOffset
+			);
+#else
+			repoError << "Bouncer must be built with REPO_ASSET_GENERATOR_SUPPORT ON in order to generate Repo Bundles.";
+			return false;
+#endif // REPO_ASSETGENERATOR
 			break;
 		default:
 			repoError << "Unknown export type with enum:  " << (uint16_t)exType;
 			return false;
 		}
 
-		if (success = resultBuffers.geoFiles.size())
-		{
-			if (toCommit)
-			{
-				success = commitWebBuffers(scene, geoStashExt, resultBuffers, handler, fileManager);
-			}
-		}
-		else
-		{
-			repoError << "Failed to generate web buffers: no geometry file generated";
-		}
+		repo::manipulator::modeloptimizer::MultipartOptimizer mpOpt;
+		return mpOpt.processScene(
+			scene->getDatabaseName(),
+			scene->getProjectName(),
+			scene->getRevisionID(),
+			handler,
+			exporter.get() 
+		);
 	}
 	else
 	{
 		repoError << "Failed to generate web buffers: scene is empty or not revisioned!";
 	}
 
-	return success;
+	return false;
 }
 
 bool SceneManager::generateAndCommitSelectionTree(
-	repo::core::model::RepoScene                           *scene,
-	repo::core::handler::AbstractDatabaseHandler           *handler,
-	repo::core::handler::fileservice::FileManager         *fileManager)
+	repo::core::model::RepoScene					*scene,
+	repo::core::handler::AbstractDatabaseHandler	*handler,
+	repo::core::handler::fileservice::FileManager	*fileManager)
 {
 	bool success = false;
 	if (success = scene && scene->isRevisioned() && handler)
@@ -389,27 +323,6 @@ bool SceneManager::generateAndCommitSelectionTree(
 	return success;
 }
 
-repo::lib::repo_web_buffers_t SceneManager::generateRepoBundleBuffer(
-	repo::core::model::RepoScene* scene)
-{
-	repo_web_buffers_t result;
-#ifdef REPO_ASSET_GENERATOR_SUPPORT
-	repo::manipulator::modelconvertor::RepoBundleExport bundleExport(scene);
-	if (bundleExport.isOk()) {
-		repoTrace << "Exporting Repo Bundles as buffer...";
-		result = bundleExport.getAllFilesExportedAsBuffer();
-	}
-	else
-	{
-		repoError << "Export of Repo Bundles failed.";
-	}
-#else
-	repoError << "Bouncer must be built with REPO_ASSET_GENERATOR_SUPPORT ON in order to generate Repo Bundles.";
-#endif // REPO_ASSETGENERATOR
-
-	return result;
-}
-
 bool isAddOnEnabled(repo::core::handler::AbstractDatabaseHandler *handler, const std::string &database, const std::string addOn) {
 
 	auto teamspace = repo::core::model::RepoTeamspace(handler->findOneByUniqueID(database, "teamspace", database));
@@ -417,7 +330,7 @@ bool isAddOnEnabled(repo::core::handler::AbstractDatabaseHandler *handler, const
 }
 
 bool SceneManager::isVrEnabled(
-	const repo::core::model::RepoScene                 *scene,
+	const repo::core::model::RepoScene           *scene,
 	repo::core::handler::AbstractDatabaseHandler *handler) const
 {
 	auto dbName = scene->getDatabaseName();
diff --git a/bouncer/src/repo/manipulator/modelutility/repo_scene_manager.h b/bouncer/src/repo/manipulator/modelutility/repo_scene_manager.h
index 3b25cb06..02efafd5 100644
--- a/bouncer/src/repo/manipulator/modelutility/repo_scene_manager.h
+++ b/bouncer/src/repo/manipulator/modelutility/repo_scene_manager.h
@@ -18,7 +18,7 @@
 #include "../../core/model/collection/repo_scene.h"
 #include "../../core/handler/repo_database_handler_abstract.h"
 #include "../../core/handler/fileservice/repo_file_manager.h"
-#include "../modelconvertor/export/repo_model_export_web.h"
+#include "../modelconvertor/export/repo_model_export_abstract.h"
 
 namespace repo {
 	namespace manipulator {
@@ -29,30 +29,14 @@ namespace repo {
 				SceneManager() {}
 				~SceneManager() {}
 
-				/**
-				* Commit web buffers
-				* @param scene repo scene related to repo scene
-				* @param geoStashExt geometry stash extension
-				* @param resultBuffers buffers to write
-				* @param hander mongo handler
-				* @param addTimestampToSettings whether we should be adding timestamp to settings upon success
-				*/
-				bool commitWebBuffers(
-					repo::core::model::RepoScene                          *scene,
-					const std::string                                     &geoStashExt,
-					const repo::lib::repo_web_buffers_t                   &resultBuffers,
-					repo::core::handler::AbstractDatabaseHandler          *handler,
-					repo::core::handler::fileservice::FileManager         *fileManager
-				);
-
 				uint8_t commitScene(
-					repo::core::model::RepoScene                          *scene,
-					const std::string									  &owner,
-					const std::string									  &tag,
-					const std::string									  &desc,
-					const repo::lib::RepoUUID                             &revId,
-					repo::core::handler::AbstractDatabaseHandler          *handler,
-					repo::core::handler::fileservice::FileManager         *fileManager
+					repo::core::model::RepoScene									*scene,
+					const std::string												&owner,
+					const std::string												&tag,
+					const std::string												&desc,
+					const repo::lib::RepoUUID										&revId,
+					repo::core::handler::AbstractDatabaseHandler					*handler,
+					repo::core::handler::fileservice::FileManager					*fileManager
 				);
 
 				/**
@@ -66,7 +50,7 @@ namespace repo {
 				* @return returns a pointer to a repoScene.
 				*/
 				repo::core::model::RepoScene* fetchScene(
-					repo::core::handler::AbstractDatabaseHandler *handler,
+					repo::core::handler::AbstractDatabaseHandler  *handler,
 					const std::string                             &database,
 					const std::string                             &project,
 					const repo::lib::RepoUUID                     &uuid,
@@ -76,7 +60,7 @@ namespace repo {
 					const std::vector<repo::core::model::ModelRevisionNode::UploadStatus> &includeStatus = {});
 
 				repo::core::model::RepoScene* fetchScene(
-					repo::core::handler::AbstractDatabaseHandler *handler,
+					repo::core::handler::AbstractDatabaseHandler  *handler,
 					const std::string                             &database,
 					const std::string                             &project,
 					const bool                             &ignoreRefScene = false,
@@ -104,23 +88,11 @@ namespace repo {
 				* @return return true upon success
 				*/
 				bool generateAndCommitSelectionTree(
-					repo::core::model::RepoScene                          *scene,
-					repo::core::handler::AbstractDatabaseHandler          *handler,
-					repo::core::handler::fileservice::FileManager         *fileManager
+					repo::core::model::RepoScene										*scene,
+					repo::core::handler::AbstractDatabaseHandler						*handler,
+					repo::core::handler::fileservice::FileManager						*fileManager
 				);
 
-				/**
-				* Generate a stash graph for the given scene and populate it
-				* into the given scene
-				* If a databasehandler is given and the scene is revisioned,
-				* it will commit the stash to database
-				* @param scene scene to generate stash graph for
-				* @param handler hander to the database
-				* @return returns true upon success
-				*/
-				bool generateStashGraph(
-					repo::core::model::RepoScene *scene);
-
 				/**
 				* Check if the scene is VR enabled
 				* This is primarily used for determining if we need to generate
@@ -130,8 +102,8 @@ namespace repo {
 				* @return returns true if VR is enabled
 				*/
 				bool isVrEnabled(
-					const repo::core::model::RepoScene                 *scene,
-					repo::core::handler::AbstractDatabaseHandler *handler) const;
+					const repo::core::model::RepoScene					*scene,
+					repo::core::handler::AbstractDatabaseHandler		*handler) const;
 
 				/**
 				* Generate a `exType` encoding for the given scene
@@ -143,11 +115,9 @@ namespace repo {
 				* @return returns repo_web_buffers upon success
 				*/
 				bool generateWebViewBuffers(
-					repo::core::model::RepoScene                           *scene,
-					const repo::manipulator::modelconvertor::WebExportType &exType,
-					repo::lib::repo_web_buffers_t                          &resultBuffers,
-					repo::core::handler::AbstractDatabaseHandler           *handler = nullptr,
-					repo::core::handler::fileservice::FileManager         *fileManager = nullptr);
+					repo::core::model::RepoScene									*scene,
+					const repo::manipulator::modelconvertor::ExportType				&exType,
+					repo::core::handler::AbstractDatabaseHandler					*handler = nullptr);
 
 				/**
 				* Remove stash graph entry for the given scene
diff --git a/bouncer/src/repo/manipulator/repo_manipulator.cpp b/bouncer/src/repo/manipulator/repo_manipulator.cpp
index c66c589c..2a3d07e1 100644
--- a/bouncer/src/repo/manipulator/repo_manipulator.cpp
+++ b/bouncer/src/repo/manipulator/repo_manipulator.cpp
@@ -177,11 +177,9 @@ void RepoManipulator::fetchScene(
 bool RepoManipulator::generateAndCommitRepoBundlesBuffer(
 	repo::core::model::RepoScene* scene)
 {
-	repo::lib::repo_web_buffers_t buffers;
 	return generateAndCommitWebViewBuffer(
 		scene,
-		buffers,
-		modelconvertor::WebExportType::REPO
+		modelconvertor::ExportType::REPO
 	);
 }
 
@@ -193,24 +191,12 @@ bool RepoManipulator::generateAndCommitSelectionTree(
 	return SceneManager.generateAndCommitSelectionTree(scene, dbHandler.get(), dbHandler->getFileManager().get());
 }
 
-bool RepoManipulator::generateStashGraph(
-	repo::core::model::RepoScene* scene
-)
-{
-	modelutility::SceneManager SceneManager;
-	return SceneManager.generateStashGraph(scene);
-}
-
 bool RepoManipulator::generateAndCommitWebViewBuffer(
 	repo::core::model::RepoScene* scene,
-	repo::lib::repo_web_buffers_t& buffers,
-	const modelconvertor::WebExportType& exType)
+	const modelconvertor::ExportType& exType)
 {
 	modelutility::SceneManager SceneManager;
-	if (!scene->hasRoot(repo::core::model::RepoScene::GraphType::OPTIMIZED)) {
-		SceneManager.generateStashGraph(scene);
-	}
-	return SceneManager.generateWebViewBuffers(scene, exType, buffers, dbHandler.get(), dbHandler->getFileManager().get());
+	return SceneManager.generateWebViewBuffers(scene, exType, dbHandler.get());
 }
 
 std::vector<repo::core::model::RepoBSON>
diff --git a/bouncer/src/repo/manipulator/repo_manipulator.h b/bouncer/src/repo/manipulator/repo_manipulator.h
index 06c1eb02..70966f00 100644
--- a/bouncer/src/repo/manipulator/repo_manipulator.h
+++ b/bouncer/src/repo/manipulator/repo_manipulator.h
@@ -28,7 +28,7 @@
 #include "../core/model/bson/repo_node_mesh.h"
 #include "../core/model/collection/repo_scene.h"
 #include "../lib/repo_config.h"
-#include "modelconvertor/export/repo_model_export_web.h"
+#include "modelconvertor/export/repo_model_export_abstract.h"
 #include "modelconvertor/import/repo_model_import_config.h"
 #include "modelutility/spatialpartitioning/repo_spatial_partitioner_abstract.h"
 
@@ -121,8 +121,7 @@ namespace repo {
 			*/
 			bool generateAndCommitWebViewBuffer(
 				repo::core::model::RepoScene          *scene,
-				repo::lib::repo_web_buffers_t         &buffers,
-				const modelconvertor::WebExportType   &exType);
+				const modelconvertor::ExportType   &exType);
 
 			/**
 			* Generate and commit RepoBundles for the given scene
@@ -133,16 +132,6 @@ namespace repo {
 			bool generateAndCommitRepoBundlesBuffer(
 				repo::core::model::RepoScene* scene);
 
-			/**
-			* Generate a stash graph for the given scene and populate it
-			* into the given scene
-			* @param scene scene to generate stash graph for
-			* @return returns true upon success
-			*/
-			bool generateStashGraph(
-				repo::core::model::RepoScene              *scene
-			);
-
 			/**
 			* Retrieve documents from a specified collection
 			* due to limitations of the transfer protocol this might need
diff --git a/bouncer/src/repo/repo_bouncer_global.h b/bouncer/src/repo/repo_bouncer_global.h
index 0c63277d..c5a40315 100644
--- a/bouncer/src/repo/repo_bouncer_global.h
+++ b/bouncer/src/repo/repo_bouncer_global.h
@@ -35,7 +35,7 @@
 //------------------------------------------------------------------------------
 #define BOUNCER_VMAJOR 5
 
-#define BOUNCER_VMINOR "18_0"
+#define BOUNCER_VMINOR "18_1"
 #define REPO_MAX_OBJ_SIZE (16 * 1024 * 1024)
 
 //
diff --git a/ifcUtils/repo_ifc_serialiser.cpp b/ifcUtils/repo_ifc_serialiser.cpp
index 2c478d97..258f3429 100644
--- a/ifcUtils/repo_ifc_serialiser.cpp
+++ b/ifcUtils/repo_ifc_serialiser.cpp
@@ -1043,9 +1043,9 @@ void IfcSerialiser::import(const IfcGeom::TriangulationElement* triangulation)
 		);
 		mesh.applyTransformation(matrix);
 		mesh.updateBoundingBox();
+		mesh.setMaterial(resolveMaterial(materials[pair.first]));
 		builder->addNode(mesh);
-		builder->addMaterialReference(resolveMaterial(materials[pair.first]), mesh.getSharedID());
-
+		
 		if (metaNode) {
 			metaNode->addParent(mesh.getSharedID());
 		}
diff --git a/submodules/asset_generator b/submodules/asset_generator
index 00b761c5..ea22b843 160000
--- a/submodules/asset_generator
+++ b/submodules/asset_generator
@@ -1 +1 @@
-Subproject commit 00b761c59fa69cf452d69315bb6a0a46a721e4cc
+Subproject commit ea22b843d1a09b65cb13595b387a48ef151864a8
diff --git a/test/src/unit/CMakeLists.txt b/test/src/unit/CMakeLists.txt
index 70636251..24945180 100644
--- a/test/src/unit/CMakeLists.txt
+++ b/test/src/unit/CMakeLists.txt
@@ -7,6 +7,7 @@
 add_subdirectory(repo)
 set(TEST_SOURCES
 	${TEST_SOURCES}
+	${CMAKE_CURRENT_SOURCE_DIR}/repo_test_common_tests.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_test_database_info.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_test_matchers.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_test_mesh_utils.cpp
@@ -16,6 +17,7 @@ set(TEST_SOURCES
 
 set(TEST_HEADERS
 	${TEST_HEADERS}
+	${CMAKE_CURRENT_SOURCE_DIR}/repo_test_common_tests.h
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_test_database_info.h
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_test_fileservice_info.h
 	${CMAKE_CURRENT_SOURCE_DIR}/repo_test_matchers.h
diff --git a/test/src/unit/repo/core/handler/ut_repo_database_handler_mongo.cpp b/test/src/unit/repo/core/handler/ut_repo_database_handler_mongo.cpp
index 594a8924..34207ee2 100644
--- a/test/src/unit/repo/core/handler/ut_repo_database_handler_mongo.cpp
+++ b/test/src/unit/repo/core/handler/ut_repo_database_handler_mongo.cpp
@@ -21,6 +21,7 @@
 #include <repo/core/model/bson/repo_bson_element.h>
 #include <repo/core/model/bson/repo_node.h>
 #include <repo/core/model/bson/repo_bson_builder.h>
+#include <repo/core/handler/fileservice/repo_blob_files_handler.h>
 
 #include <gtest/gtest.h>
 #include <gmock/gmock.h>
@@ -205,6 +206,38 @@ MATCHER_P(GetAllFromCollectionTailable_CounterIsDecreasing, start, "")
 	return true;
 }
 
+void populateBinaryData(
+	MongoDatabaseHandler* handler,
+	std::string db,
+	std::string col,
+	std::vector<repo::core::model::RepoBSON>& bsons
+) {
+	repo::core::handler::fileservice::BlobFilesHandler blobHandler(handler->getFileManager(), db, col);
+
+	for (auto& bson : bsons) {
+		if (bson.hasFileReference()) {
+			auto ref = bson.getBinaryReference();
+			auto buffer = blobHandler.readToBuffer(fileservice::DataRef::deserialise(ref));
+			bson.initBinaryBuffer(buffer);
+		}
+	}
+}
+
+void populateBinaryData(
+	MongoDatabaseHandler* handler,
+	std::string db,
+	std::string col,
+	repo::core::model::RepoBSON& bson
+) {
+	repo::core::handler::fileservice::BlobFilesHandler blobHandler(handler->getFileManager(), db, col);
+	
+	if (bson.hasFileReference()) {
+		auto ref = bson.getBinaryReference();
+		auto buffer = blobHandler.readToBuffer(fileservice::DataRef::deserialise(ref));
+		bson.initBinaryBuffer(buffer);
+	}	
+}
+
 TEST(MongoDatabaseHandlerTest, GetAllFromCollectionTailable)
 {
 	auto handler = getHandler();
@@ -230,6 +263,7 @@ TEST(MongoDatabaseHandlerTest, GetAllFromCollectionTailable)
 
 	{
 		auto bsons = handler->getAllFromCollectionTailable(db, col);
+		populateBinaryData(handler.get(), db, col, bsons);
 		EXPECT_THAT(bsons, UnorderedElementsAreArray(goldenData));
 	}
 
@@ -237,6 +271,8 @@ TEST(MongoDatabaseHandlerTest, GetAllFromCollectionTailable)
 	{
 		auto bsons1 = handler->getAllFromCollectionTailable(db, col, 0, 25);
 		auto bsons2 = handler->getAllFromCollectionTailable(db, col, 25, 25);
+		populateBinaryData(handler.get(), db, col, bsons1);
+		populateBinaryData(handler.get(), db, col, bsons2);
 
 		// Sets should be disjoint to eachother
 
@@ -258,6 +294,7 @@ TEST(MongoDatabaseHandlerTest, GetAllFromCollectionTailable)
 	// Reading over the end shouldn't result in an error
 	{
 		auto bsons = handler->getAllFromCollectionTailable(db, col, 40, 100);
+		populateBinaryData(handler.get(), db, col, bsons);
 		EXPECT_THAT(bsons.size(), Eq(10));
 		EXPECT_THAT(bsons, IsSubsetOf(goldenData));
 	}
@@ -301,6 +338,7 @@ TEST(MongoDatabaseHandlerTest, GetAllFromCollectionTailable)
 	// Test sort
 	{
 		auto sorted = handler->getAllFromCollectionTailable(db, col, 0, 0, {}, "counter", 1);
+		populateBinaryData(handler.get(), db, col, sorted);
 
 		// Counter should be increasing
 
@@ -524,6 +562,7 @@ TEST(MongoDatabaseHandlerTest, FindOneByCriteria)
 		auto id = repo::lib::RepoUUID("c9bed762-9b63-4307-824d-40261ce7f022");
 		query::Eq search("_id", id);
 		auto result = handler->findOneByCriteria(db, col, search);
+		populateBinaryData(handler.get(), db, col, result);
 
 		EXPECT_THAT(result.getStringField("type"), Eq("mesh"));
 		EXPECT_THAT(result.getBinary("faces"), Not(IsEmpty()));
@@ -535,6 +574,7 @@ TEST(MongoDatabaseHandlerTest, FindOneByCriteria)
 		auto id = std::string("9ecbebbc-c24d-4da5-b69b-388abfcfe314");
 		query::Eq search("_id", id);
 		auto result = handler->findOneByCriteria(db, col, search);
+		populateBinaryData(handler.get(), db, col, result);
 
 		EXPECT_THAT(result.getStringField("type"), Eq("mesh"));
 		EXPECT_THAT(result.getBinary("faces"), Not(IsEmpty()));
@@ -648,6 +688,7 @@ TEST(MongoDatabaseHandlerTest, FindOneByUniqueID)
 	{
 		auto id = repo::lib::RepoUUID("f6d60d2c-5e78-4725-86c1-05b2fcff44fa");
 		auto document = handler->findOneByUniqueID(db, col, id);
+		populateBinaryData(handler.get(), db, col, document);
 
 		EXPECT_THAT(document.getUUIDField("_id"), Eq(id));
 		EXPECT_THAT(document.getStringField("x"), Eq("document4"));
@@ -658,6 +699,7 @@ TEST(MongoDatabaseHandlerTest, FindOneByUniqueID)
 	{
 		auto id = "9ecbebbc-c24d-4da5-b69b-388abfcfe314";
 		auto document = handler->findOneByUniqueID(db, col, id);
+		populateBinaryData(handler.get(), db, col, document);
 
 		EXPECT_THAT(document.getStringField("_id"), Eq(id));
 		EXPECT_THAT(document.getStringField("x"), Eq("document6"));
@@ -887,6 +929,7 @@ TEST(MongoDatabaseHandlerTest, InsertManyDocumentsBinary)
 	// Read back the documents; this should also populate the binary buffers
 
 	auto actual = handler->getAllFromCollectionTailable(REPO_GTEST_DBNAME3, collection);
+	populateBinaryData(handler.get(), REPO_GTEST_DBNAME3, collection, actual);
 
 	// The original documents won't have the _blobRef document as this isn't
 	// created until inside the handler, so we convert both sets to RepoBSONs
diff --git a/test/src/unit/repo/core/model/bson/CMakeLists.txt b/test/src/unit/repo/core/model/bson/CMakeLists.txt
index bdae91d2..80981e62 100644
--- a/test/src/unit/repo/core/model/bson/CMakeLists.txt
+++ b/test/src/unit/repo/core/model/bson/CMakeLists.txt
@@ -23,6 +23,7 @@ set(TEST_SOURCES
 	${CMAKE_CURRENT_SOURCE_DIR}/ut_repo_node_metadata.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/ut_repo_node_model_revision.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/ut_repo_node_reference.cpp
+	${CMAKE_CURRENT_SOURCE_DIR}/ut_repo_node_streaming_mesh.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/ut_repo_node_supermesh.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/ut_repo_node_texture.cpp
 	${CMAKE_CURRENT_SOURCE_DIR}/ut_repo_node_transformation.cpp
diff --git a/test/src/unit/repo/core/model/bson/ut_repo_bson_factory.cpp b/test/src/unit/repo/core/model/bson/ut_repo_bson_factory.cpp
index 6ec87ae6..4c9b786d 100644
--- a/test/src/unit/repo/core/model/bson/ut_repo_bson_factory.cpp
+++ b/test/src/unit/repo/core/model/bson/ut_repo_bson_factory.cpp
@@ -27,10 +27,12 @@
 #include <repo/core/model/bson/repo_bson_factory.h>
 #include <repo/core/model/bson/repo_bson_builder.h>
 #include <repo/lib/datastructure/repo_variant_utils.h>
+#include "repo/lib/datastructure/repo_structs.h"
 
 using namespace repo::lib;
 using namespace repo::core::model;
 using namespace testing;
+using namespace repo;
 
 TEST(RepoBSONFactoryTest, MakeMaterialNodeTest)
 {
diff --git a/test/src/unit/repo/core/model/bson/ut_repo_node_mesh.cpp b/test/src/unit/repo/core/model/bson/ut_repo_node_mesh.cpp
index aa3388a4..0312725f 100644
--- a/test/src/unit/repo/core/model/bson/ut_repo_node_mesh.cpp
+++ b/test/src/unit/repo/core/model/bson/ut_repo_node_mesh.cpp
@@ -52,28 +52,39 @@ void MeshNodeTestDeserialise(mesh_data data)
 
 TEST(MeshNodeTest, Deserialise)
 {
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, false, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, false, 1, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, false, 2, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, true, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, true, 1, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, true, 2, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, false, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, false, 1, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, false, 2, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, true, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, true, 1, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, true, 2, 100));
-
-	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, false, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(false, false, 1, 3, false, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(false, true, 0, 3, false, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(false, true, 1, 3, false, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(true, false, 0, 3, false, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(true, false, 1, 3, false, 0, 100));
-	MeshNodeTestDeserialise(mesh_data(true, true, 1, 3, false, 0, 100));
-
-	MeshNodeTestDeserialise(mesh_data(true, true, 3, 3, false, 0, 100));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, false, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, false, 1, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, false, 2, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, true, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, true, 1, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 2, true, 2, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, false, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, false, 1, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, false, 2, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, true, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, true, 1, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, true, 2, 100, ""));
+
+	MeshNodeTestDeserialise(mesh_data(false, false, 0, 3, false, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, false, 1, 3, false, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, true, 0, 3, false, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(false, true, 1, 3, false, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(true, false, 0, 3, false, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(true, false, 1, 3, false, 0, 100, ""));
+	MeshNodeTestDeserialise(mesh_data(true, true, 1, 3, false, 0, 100, ""));
+
+	MeshNodeTestDeserialise(mesh_data(true, true, 3, 3, false, 0, 100, ""));
+}
+
+TEST(MeshNodeTest, DeserialiseGroupings) {
+	auto meshDataNoGrouping = mesh_data(false, false, 0, 2, false, 0, 100, "");
+	auto meshNodeNoGrouping = MeshNode(meshNodeTestBSONFactory(meshDataNoGrouping));
+	EXPECT_EQ(meshNodeNoGrouping.getGrouping(), "");
+
+	auto grouping = getRandomString(10);
+	auto meshDataGrouping = mesh_data(false, false, 0, 2, false, 0, 100, grouping);
+	auto meshNodeGrouping = MeshNode(meshNodeTestBSONFactory(meshDataGrouping));
+	EXPECT_EQ(meshNodeGrouping.getGrouping(), grouping);
 }
 
 TEST(MeshNodeTest, Serialise)
@@ -150,6 +161,30 @@ TEST(MeshNodeTest, Serialise)
 	EXPECT_THAT(((RepoBSON)node).getIntField(REPO_NODE_MESH_LABEL_UV_CHANNELS_COUNT), Eq(node.getNumUVChannels()));
 	((RepoBSON)node).getBinaryFieldAsVector(REPO_NODE_MESH_LABEL_UV_CHANNELS, uvs);
 	EXPECT_THAT(uvs, ElementsAreArray(node.getUVChannelsSerialised()));
+
+	// Grouping
+	auto grouping = getRandomString(10);
+	node.setGrouping(grouping);
+	EXPECT_EQ(((RepoBSON)node).getStringField(REPO_NODE_MESH_LABEL_GROUPING), grouping);
+
+	// Material properties
+	auto mat = repo::lib::repo_material_t::DefaultMaterial();
+	
+	mat.opacity = 1.f;
+	node.setMaterial(mat);	
+	EXPECT_EQ(((RepoBSON)node).getObjectField(REPO_FILTER_OBJECT_NAME).getBoolField(REPO_FILTER_PROP_OPAQUE), true);
+
+	mat.opacity = 0.5f;
+	node.setMaterial(mat);
+	EXPECT_EQ(((RepoBSON)node).getObjectField(REPO_FILTER_OBJECT_NAME).getBoolField(REPO_FILTER_PROP_TRANSPARENT), true);
+
+	auto texId = repo::lib::RepoUUID::createUUID();
+	node.setTextureId(texId);
+	EXPECT_EQ(((RepoBSON)node).getObjectField(REPO_FILTER_OBJECT_NAME).getUUIDField(REPO_FILTER_PROP_TEXTURE_ID), texId);
+
+	MeshNode nodeNoUv;
+	nodeNoUv.setTextureId(texId);
+	EXPECT_FALSE(((RepoBSON)nodeNoUv).getObjectField(REPO_FILTER_OBJECT_NAME).hasField(REPO_FILTER_PROP_TEXTURE_ID));
 }
 
 TEST(MeshNodeTest, TypeTest)
diff --git a/test/src/unit/repo/core/model/bson/ut_repo_node_streaming_mesh.cpp b/test/src/unit/repo/core/model/bson/ut_repo_node_streaming_mesh.cpp
new file mode 100644
index 00000000..d071206c
--- /dev/null
+++ b/test/src/unit/repo/core/model/bson/ut_repo_node_streaming_mesh.cpp
@@ -0,0 +1,222 @@
+/**
+*  Copyright (C) 2015 3D Repo Ltd
+*
+*  This program is free software: you can redistribute it and/or modify
+*  it under the terms of the GNU Affero General Public License as
+*  published by the Free Software Foundation, either version 3 of the
+*  License, or (at your option) any later version.
+*
+*  This program is distributed in the hope that it will be useful,
+*  but WITHOUT ANY WARRANTY; without even the implied warranty of
+*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+*  GNU Affero General Public License for more details.
+*
+*  You should have received a copy of the GNU Affero General Public License
+*  along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <gtest/gtest.h>
+#include <gmock/gmock.h>
+#include <gtest/gtest-matchers.h>
+
+#include <repo/core/model/bson/repo_node_streaming_mesh.h>
+#include <repo/core/model/bson/repo_bson_builder.h>
+#include "../../../../repo_test_mesh_utils.h"
+
+using namespace repo::core::model;
+using namespace testing;
+using namespace repo::test::utils::mesh;
+
+TEST(StreamingMeshNodeTest, EmptyConstructor)
+{
+	StreamingMeshNode empty;
+
+	// Check basic data that should be accessible
+	// though only containing default values
+	EXPECT_EQ(empty.getNumVertices(), 0);
+	EXPECT_TRUE(empty.getSharedId().isDefaultValue());
+	EXPECT_TRUE(empty.getParent().isDefaultValue());	
+	EXPECT_EQ(empty.getBoundingBox(), repo::lib::RepoBounds());
+
+	// Check data that should not be accessible yet
+	EXPECT_TRUE(empty.getUniqueId().isDefaultValue());
+	EXPECT_EQ(empty.getNumLoadedFaces(), 0);	
+	EXPECT_EQ(empty.getLoadedFaces().size(), 0);
+	EXPECT_EQ(empty.getNumLoadedVertices(), 0);
+	EXPECT_EQ(empty.getLoadedVertices().size(), 0);
+	EXPECT_EQ(empty.getLoadedNormals().size(), 0);
+	EXPECT_EQ(empty.getLoadedUVChannelsSeparated().size(), 0);
+}
+
+TEST(StreamingMeshNodeTest, LoadBasicStreamingMeshNode) {
+
+	// Create mesh Data and StreamingMeshNode
+	auto meshData = mesh_data(false, true, 3, 2, true, 1, 100, "");
+	auto bson = meshNodeTestBSONFactory(meshData);
+	StreamingMeshNode limited = StreamingMeshNode(bson);
+
+	// Check basic data that should be accessible	
+	EXPECT_EQ(limited.getNumVertices(), 100);
+	EXPECT_EQ(limited.getSharedId(), meshData.sharedId);
+	EXPECT_EQ(limited.getParent(), meshData.parents[0]);	
+	EXPECT_EQ(limited.getBoundingBox(),
+		repo::lib::RepoBounds(repo::lib::RepoVector3D(meshData.boundingBox[0]),
+			repo::lib::RepoVector3D(meshData.boundingBox[1])));
+	
+	// Check data that should not be accessible yet
+	EXPECT_TRUE(limited.getUniqueId().isDefaultValue());
+	EXPECT_EQ(limited.getNumLoadedFaces(), 0);
+	EXPECT_EQ(limited.getLoadedFaces().size(), 0);
+	EXPECT_EQ(limited.getNumLoadedVertices(), 0);
+	EXPECT_EQ(limited.getLoadedVertices().size(), 0);
+	EXPECT_EQ(limited.getLoadedNormals().size(), 0);
+	EXPECT_EQ(limited.getLoadedUVChannelsSeparated().size(), 0);
+}
+
+void TestLoadSupermeshingData(mesh_data meshData) {
+	// Create mesh data	
+	auto bson = meshNodeTestBSONFactory(meshData);
+
+	// Create limited node
+	StreamingMeshNode node = StreamingMeshNode(bson);
+
+	// Check data that should not be accessible yet
+	EXPECT_TRUE(node.getUniqueId().isDefaultValue());
+	EXPECT_EQ(node.getNumLoadedFaces(), 0);
+	EXPECT_EQ(node.getLoadedFaces().size(), 0);
+	EXPECT_EQ(node.getNumLoadedVertices(), 0);
+	EXPECT_EQ(node.getLoadedVertices().size(), 0);
+	EXPECT_EQ(node.getLoadedNormals().size(), 0);
+	EXPECT_EQ(node.getLoadedUVChannelsSeparated().size(), 0);
+
+	// Inflate node
+	auto data = bson.getBinariesAsBuffer();
+	auto fakeRef = repo::core::handler::fileservice::DataRef("file", 0, data.second.size());
+	bson.replaceBinaryWithReference(fakeRef.serialise(), data.first);
+	node.loadSupermeshingData(bson, data.second, false);
+
+	// Check the inflated node's data
+	EXPECT_EQ(node.getUniqueId(), meshData.uniqueId);
+	EXPECT_EQ(node.getNumLoadedFaces(), meshData.faces.size());
+	EXPECT_EQ(node.getNumLoadedVertices(), meshData.vertices.size());
+	EXPECT_THAT(node.getLoadedFaces(), ElementsAreArray(meshData.faces));
+	EXPECT_THAT(node.getLoadedVertices(), ElementsAreArray(meshData.vertices));
+	EXPECT_THAT(node.getLoadedNormals(), ElementsAreArray(meshData.normals));
+	EXPECT_THAT(node.getLoadedUVChannelsSeparated(), ElementsAreArray(meshData.uvChannels));
+
+	// Deflate node
+	node.unloadSupermeshingData();
+
+	// Check the deflated node's data
+	// It should be back to the previous state		
+	EXPECT_TRUE(node.getUniqueId().isDefaultValue());
+	EXPECT_EQ(node.getNumLoadedFaces(), 0);
+	EXPECT_EQ(node.getLoadedFaces().size(), 0);
+	EXPECT_EQ(node.getNumLoadedVertices(), 0);
+	EXPECT_EQ(node.getLoadedVertices().size(), 0);
+	EXPECT_EQ(node.getLoadedNormals().size(), 0);
+	EXPECT_EQ(node.getLoadedUVChannelsSeparated().size(), 0);
+}
+
+TEST(StreamingMeshNodeTest, LoadSupermeshingData) {
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 2, false, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 2, false, 1, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 2, false, 2, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 2, true, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 2, true, 1, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 2, true, 2, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 3, false, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 3, false, 1, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 3, false, 2, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 3, true, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 3, true, 1, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 3, true, 2, 100, ""));
+
+	TestLoadSupermeshingData(mesh_data(false, false, 0, 3, false, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, false, 1, 3, false, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, true, 0, 3, false, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(false, true, 1, 3, false, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(true, false, 0, 3, false, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(true, false, 1, 3, false, 0, 100, ""));
+	TestLoadSupermeshingData(mesh_data(true, true, 1, 3, false, 0, 100, ""));
+
+	TestLoadSupermeshingData(mesh_data(true, true, 3, 3, false, 0, 100, ""));
+}
+
+TEST(StreamingMeshNodeTest, TransformBounds) {
+	// Create mesh Data and StreamingMeshNode
+	auto meshData = mesh_data(false, true, 3, 2, true, 1, 100, "");
+	auto bson = meshNodeTestBSONFactory(meshData);
+	StreamingMeshNode limited = StreamingMeshNode(bson);
+
+	// Create matrix
+	auto m = repo::lib::RepoMatrix::translate(repo::lib::RepoVector3D(10, 0, 0))
+		* repo::lib::RepoMatrix::rotationX(0.12f)
+		* repo::lib::RepoMatrix::rotationY(0.8f)
+		* repo::lib::RepoMatrix::rotationZ(1.02f);
+
+	// Get bounding box from the mesh data
+	
+	auto vMin = repo::lib::RepoVector3D64(
+		meshData.boundingBox[0][0],
+		meshData.boundingBox[0][1],
+		meshData.boundingBox[0][2]);
+	auto vMax = repo::lib::RepoVector3D64(
+		meshData.boundingBox[1][0],
+		meshData.boundingBox[1][1],
+		meshData.boundingBox[1][2]);
+	
+	// Transform bounding box from the mesh data
+	auto vMinTransformed = m * vMin;
+	auto vMaxTransformed = m * vMax;
+	auto boundsTransformed = repo::lib::RepoBounds(vMinTransformed, vMaxTransformed);
+
+	// Transform via StreamingMeshNode and compare
+	limited.transformBounds(m);	
+	EXPECT_EQ(limited.getBoundingBox(), boundsTransformed);
+}
+TEST(StreamingMeshNodeTest, BakeMeshData) {
+	// Create mesh Data and StreamingMeshNode
+	auto meshData = mesh_data(false, true, 3, 2, true, 1, 100, "");
+	auto bson = meshNodeTestBSONFactory(meshData);
+	StreamingMeshNode node = StreamingMeshNode(bson);
+
+	// Create matrix
+	auto m = repo::lib::RepoMatrix::translate(repo::lib::RepoVector3D(10, 0, 0))
+		* repo::lib::RepoMatrix::rotationX(0.12f)
+		* repo::lib::RepoMatrix::rotationY(0.8f)
+		* repo::lib::RepoMatrix::rotationZ(1.02f);
+
+	// Apply matrix to vertices
+	std::vector<repo::lib::RepoVector3D> transformedVertices;
+	for (auto v : meshData.vertices) {
+		transformedVertices.push_back(m * v);
+	}
+
+	// Apply matrix to normals
+	auto matInverse = m.invert();
+	auto worldMat = matInverse.transpose();
+	auto mData = worldMat.getData();
+	mData[3] = mData[7] = mData[11] = 0;
+	mData[12] = mData[13] = mData[14] = 0;
+	repo::lib::RepoMatrix multMat(mData);
+	std::vector<repo::lib::RepoVector3D> transformedNormals;
+	for (auto v : meshData.normals) {
+		auto n = multMat * v;
+		n.normalize();
+		transformedNormals.push_back(n);
+	}
+
+	// Inflate node
+	auto data = bson.getBinariesAsBuffer();
+	auto fakeRef = repo::core::handler::fileservice::DataRef("file", 0, data.second.size());
+	bson.replaceBinaryWithReference(fakeRef.serialise(), data.first);
+	node.loadSupermeshingData(bson, data.second, false);
+
+	// Apply transformation for baking
+	node.bakeLoadedMeshes(m);
+		
+	// Check the transformed data
+	EXPECT_THAT(node.getLoadedVertices(), ElementsAreArray(transformedVertices));
+	EXPECT_THAT(node.getLoadedNormals(), ElementsAreArray(transformedNormals));
+}
\ No newline at end of file
diff --git a/test/src/unit/repo/core/model/collection/ut_repo_scene.cpp b/test/src/unit/repo/core/model/collection/ut_repo_scene.cpp
index 1d4a3aaa..3d7e5382 100644
--- a/test/src/unit/repo/core/model/collection/ut_repo_scene.cpp
+++ b/test/src/unit/repo/core/model/collection/ut_repo_scene.cpp
@@ -61,7 +61,8 @@ static MeshNode makeMeshNode(
 		3,
 		false,
 		0,
-		1024
+		1024,
+		""
 	));
 	m.changeName(name, true);
 	m.setSharedID(repo::lib::RepoUUID::createUUID());
diff --git a/test/src/unit/repo/manipulator/modelconvertor/import/odaImport/ut_repo_model_import_oda.cpp b/test/src/unit/repo/manipulator/modelconvertor/import/odaImport/ut_repo_model_import_oda.cpp
index b4d03ea6..4fd68225 100644
--- a/test/src/unit/repo/manipulator/modelconvertor/import/odaImport/ut_repo_model_import_oda.cpp
+++ b/test/src/unit/repo/manipulator/modelconvertor/import/odaImport/ut_repo_model_import_oda.cpp
@@ -26,6 +26,7 @@
 #include "../../../../../repo_test_database_info.h"
 #include "../../../../../repo_test_scene_utils.h"
 #include "../../../../../repo_test_matchers.h"
+#include "../../../../../repo_test_common_tests.h"
 #include "repo/manipulator/modelconvertor/import/odaHelper/file_processor_nwd.h"
 
 using namespace repo::manipulator::modelconvertor;
@@ -131,7 +132,7 @@ TEST_F(NwdTestSuite, Sample2025NWDTree)
 	nodes = utils.findTransformationNodesByName("Wall-Ext_102Bwk-75Ins-100LBlk-12P");
 	EXPECT_THAT(nodes.size(), Eq(1));
 
-	auto children = nodes[0].getChildren();
+	auto children = nodes[0].getChildren({repo::core::model::NodeType::MESH, repo::core::model::NodeType::TRANSFORMATION});
 	EXPECT_THAT(children.size(), Eq(6));
 
 	for (auto n : children) {
@@ -576,3 +577,52 @@ TEST(ODAModelImport, DefaultViewDisplayOptions)
 		EXPECT_THAT(scene.findNodeByMetadata("Element ID", "307098").hasTextures(), IsFalse());
 	}
 }
+
+TEST_F(NwdTestSuite, MetadataParentsNWD)
+{
+	// All metadata nodes must also have their sibling meshnodes as parents,
+	// in order to resolve ids by mesh node in the frontend
+
+	{
+		SceneUtils scene(ODAModelImportUtils::ModelImportManagerImport("MetadataParentsNWD", getDataPath(nwdModel2025)));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(ODAModelImportUtils::ModelImportManagerImport("MetadataParentsNWD", getDataPath("groupsAndReferences.nwc")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(ODAModelImportUtils::ModelImportManagerImport("MetadataParentsNWD", getDataPath("orientedColumns.nwd")));
+		common::checkMetadataInheritence(scene);
+	}
+}
+
+TEST(ODAModelImport, MetadataParents)
+{
+	{
+		SceneUtils scene(ODAModelImportUtils::ModelImportManagerImport("MetadataParentsRVT", getDataPath("sample2025.rvt")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(ODAModelImportUtils::ModelImportManagerImport("MetadataParentsRVT", getDataPath("MetaTest2.rvt")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(ODAModelImportUtils::ModelImportManagerImport("MetadataParentsDWG", getDataPath("nestedBlocks.dwg")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(ODAModelImportUtils::ModelImportManagerImport("MetadataParentsDWG", getDataPath("colouredBoxes.dwg")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(ODAModelImportUtils::ModelImportManagerImport("MetadataParentsDGN", getDataPath("sample.dgn")));
+		common::checkMetadataInheritence(scene);
+	}
+}
\ No newline at end of file
diff --git a/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_3drepo.cpp b/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_3drepo.cpp
index 2c0f90bc..7ae613ae 100644
--- a/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_3drepo.cpp
+++ b/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_3drepo.cpp
@@ -27,6 +27,7 @@
 #include "../../../../repo_test_scene_utils.h"
 #include "../../../../repo_test_mesh_utils.h"
 #include "../../../../repo_test_matchers.h"
+#include "../../../../repo_test_common_tests.h"
 
 using namespace repo::manipulator::modelconvertor;
 using namespace testing;
@@ -276,8 +277,6 @@ TEST(RepoModelImport, EmptyTransforms)
 	}
 }
 
-#pragma optimize("", off)
-
 TEST(RepoModelImport, TransformHierarchy1)
 {
 	/*
@@ -399,4 +398,29 @@ TEST(RepoModelImport, Colours)
 	EXPECT_THAT(scene.findLeafNode("[287]").getColours(), ElementsAre(repo::lib::repo_color4d_t(0.949019611, 0.403921604, 0.133333296, 1)));
 	EXPECT_THAT(scene.findLeafNode("[28B]").getColours(), ElementsAre(repo::lib::repo_color4d_t(0.898039222, 0.909803927, 0.470588207, 1)));
 	EXPECT_THAT(scene.findLeafNode("[28F]").getColours(), ElementsAre(repo::lib::repo_color4d_t(0.368627489, 0.403921604, 0.686274529, 1)));
+}
+
+TEST(RepoModelImport, MetadataParents)
+{
+	uint8_t errCode;
+
+	{
+		SceneUtils scene(RepoModelImportUtils::ImportBIMFile(getDataPath("RepoModelImport/columns_navis.bim"), errCode));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(RepoModelImportUtils::ImportBIMFile(getDataPath("RepoModelImport/blockHierarchy.civils.bim004.bim"), errCode));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(RepoModelImportUtils::ImportBIMFile(getDataPath("RepoModelImport/columns_revit.bim"), errCode));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(RepoModelImportUtils::ImportBIMFile(getDataPath("RepoModelImport/metadata.bim004.bim"), errCode));
+		common::checkMetadataInheritence(scene);
+	}
 }
\ No newline at end of file
diff --git a/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_ifc.cpp b/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_ifc.cpp
index 06156193..bcb95577 100644
--- a/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_ifc.cpp
+++ b/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_ifc.cpp
@@ -25,6 +25,7 @@
 #include "../../../../repo_test_database_info.h"
 #include "../../../../repo_test_scene_utils.h"
 #include "../../../../repo_test_matchers.h"
+#include "../../../../repo_test_common_tests.h"
 
 using namespace repo::manipulator::modelconvertor;
 using namespace repo::core::model;
@@ -814,4 +815,39 @@ TEST(IFCModelImport, Ifc4x3_Add2)
 	auto scene = IfcModelImportUtils::ModelImportManagerImport("VersionTests", getDataPath(ifc4x3_add2Model));
 	SceneUtils utils(scene);
 	EXPECT_TRUE(utils.isPopulated());
+}
+
+TEST(IFCModelImport, MetadataParents)
+{
+	uint8_t errCode;
+
+	{
+		SceneUtils scene(IfcModelImportUtils::ModelImportManagerImport("IfcMetadataTests", getDataPath("duct.ifc")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(IfcModelImportUtils::ModelImportManagerImport("IfcMetadataTests", getDataPath("duplex.ifc")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(IfcModelImportUtils::ModelImportManagerImport("IfcMetadataTests", getDataPath("ifc4x3.ifc")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(IfcModelImportUtils::ModelImportManagerImport("IfcMetadataTests", getDataPath("ifc2x3.ifc")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(IfcModelImportUtils::ModelImportManagerImport("IfcMetadataTests", getDataPath("simpleHouse1.ifc")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(IfcModelImportUtils::ModelImportManagerImport("IfcMetadataTests", getDataPath("Wall.ifc")));
+		common::checkMetadataInheritence(scene);
+	}
 }
\ No newline at end of file
diff --git a/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_synchro.cpp b/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_synchro.cpp
index 46dfef4e..c0cf59f4 100644
--- a/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_synchro.cpp
+++ b/test/src/unit/repo/manipulator/modelconvertor/import/ut_repo_model_import_synchro.cpp
@@ -17,9 +17,33 @@
 
 #include <gtest/gtest.h>
 #include <repo/manipulator/modelconvertor/import/repo_model_import_synchro.h>
+#include <repo/manipulator/modelconvertor/import/repo_model_import_manager.h>
 #include "../../../../repo_test_database_info.h"
+#include "../../../../repo_test_utils.h"
+#include "../../../../repo_test_scene_utils.h"
+#include "../../../../repo_test_common_tests.h"
 
 using namespace repo::manipulator::modelconvertor;
+using namespace testing;
+
+repo::core::model::RepoScene* ModelImportManagerImport(std::string db, std::string path)
+{
+	auto config = ModelImportConfig();
+	config.databaseName = "SynchroTestDb";
+	config.revisionId = repo::lib::RepoUUID::createUUID();
+	config.projectName = db;
+
+	auto handler = getHandler();
+
+	uint8_t err;
+	std::string msg;
+
+	ModelImportManager manager;
+	auto scene = manager.ImportFromFile(path, config, handler, err);
+	scene->commit(handler.get(), handler->getFileManager().get(), msg, "testuser", "", "", config.getRevisionId());
+
+	return scene;
+}
 
 TEST(SynchroModelImport, ConstructorTest)
 {
@@ -40,4 +64,20 @@ TEST(SynchroModelImport, ImportModel)
 	auto scene = import.importModel(getDataPath(synchroVersion6_4), handler, errCode);	
 	EXPECT_EQ(0, errCode);
 	ASSERT_TRUE(scene);
+}
+
+TEST(SynchroModelImport, MetadataParents)
+{
+	uint8_t errCode;
+
+	{
+		SceneUtils scene(ModelImportManagerImport("SynchroMetadataTests", getDataPath("synchro6_4.spm")));
+		common::checkMetadataInheritence(scene);
+	}
+
+	{
+		SceneUtils scene(ModelImportManagerImport("SynchroMetadataTests", getDataPath("synchro6_5.spm")));
+		common::checkMetadataInheritence(scene);
+	}
+
 }
\ No newline at end of file
diff --git a/test/src/unit/repo/manipulator/modeloptimizer/ut_repo_optimizer_multipart.cpp b/test/src/unit/repo/manipulator/modeloptimizer/ut_repo_optimizer_multipart.cpp
index bd575d3a..8d5c5c0f 100644
--- a/test/src/unit/repo/manipulator/modeloptimizer/ut_repo_optimizer_multipart.cpp
+++ b/test/src/unit/repo/manipulator/modeloptimizer/ut_repo_optimizer_multipart.cpp
@@ -21,307 +21,510 @@
 #include <limits>
 #include <test/src/unit/repo_test_mesh_utils.h>
 #include <repo/core/model/bson/repo_bson_factory.h>
+#include <repo/manipulator/modelutility/repo_scene_builder.h>
+#include <test/src/unit/repo_test_database_info.h>
 
 using namespace repo::manipulator::modeloptimizer;
 using namespace repo::test::utils::mesh;
 
-const static repo::core::model::RepoScene::GraphType DEFAULT_GRAPH = repo::core::model::RepoScene::GraphType::DEFAULT;
-const static repo::core::model::RepoScene::GraphType OPTIMIZED_GRAPH = repo::core::model::RepoScene::GraphType::OPTIMIZED;
-
-TEST(MultipartOptimizer, ConstructorTest)
-{
-	MultipartOptimizer();
-}
-
-TEST(MultipartOptimizer, DeconstructorTest)
-{
-	auto ptr = new MultipartOptimizer();
-	delete ptr;
-}
-
-TEST(MultipartOptimizer, ApplyOptimizationTestEmpty)
-{
-	auto opt = MultipartOptimizer();
-	repo::core::model::RepoScene *empty = nullptr;
-	repo::core::model::RepoScene *empty2 = new repo::core::model::RepoScene();
-
-	EXPECT_FALSE(opt.apply(empty));
-	EXPECT_FALSE(opt.apply(empty2));
-
-	delete empty2;
-}
+#define DBMULTIPARTOPTIMIZERTEST "multipartOptimiserTest"
 
 // The functions below compare the geometry of the original MeshNodes with the
 // stash MeshNodes as a triangle soup. The geometry should be identical.
 
-
 TEST(MultipartOptimizer, TestAllMerged)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
 
-	auto nMesh = 3;
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
-	for (int i = 0; i < nMesh; ++i)
-		meshes.insert(createRandomMesh(10, false, 3, { rootID }));
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestAllMerged";
+	auto revId = repo::lib::RepoUUID::createUUID();
 
-	repo::core::model::RepoScene *scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
 
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
 
-	EXPECT_EQ(1, scene->getAllMeshes(OPTIMIZED_GRAPH).size());
+	auto nMesh = 3;
+	
+	for (int i = 0; i < nMesh; ++i) {
+		auto randNode = createRandomMesh(10, false, 3, "", { rootNodeId });
+		sceneBuilder.addNode(std::move(randNode));			
+	}
 
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+	sceneBuilder.finalise();
+	
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
+
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 1);
+	
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
 
 TEST(MultipartOptimizer, TestWithUV)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
+
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestWithUV";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+
+	// The new mpOpt drops geometry that has no material, so we add a texture node as well
+	auto texNode = repo::core::model::RepoBSONFactory::makeTextureNode("", 0, 0, 0, 0, { rootNodeId });
+	sceneBuilder.addNode(texNode);
+	auto texNodeId = texNode.getUniqueID();
 
 	auto nMesh = 3;
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
-	for (int i = 0; i < nMesh; ++i)
-		meshes.insert(createRandomMesh(10, i == 1, 3, { rootID }));
+	
+	for (int i = 0; i < nMesh; ++i) {
+		auto randNode = createRandomMesh(10, i == 1, 3, "", { rootNodeId });
+		if (i == 1) {
+			randNode->setTextureId(texNodeId);
+		}
+		sceneBuilder.addNode(std::move(randNode));
+	}
 
-	repo::core::model::RepoScene *scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
+	sceneBuilder.finalise();
 
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
 
-	EXPECT_EQ(2, scene->getAllMeshes(OPTIMIZED_GRAPH).size());
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
 
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 2);
+	
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
 
 TEST(MultipartOptimizer, TestMixedPrimitives)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
-
-	auto nVertices = 10;
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
-	meshes.insert(createRandomMesh(nVertices, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(nVertices, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(nVertices, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(nVertices, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(nVertices, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(nVertices, false, 1, { rootID })); // unsupported primitive types must be identified as such and not combined with known types
-
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
 
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestMixedPrimitives";
+	auto revId = repo::lib::RepoUUID::createUUID();
 
-	EXPECT_EQ(3, scene->getAllMeshes(OPTIMIZED_GRAPH).size());
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
 
-	// ensure that the batching has been successful.
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
 
-	auto nodes = scene->getAllMeshes(OPTIMIZED_GRAPH);
-	for (auto& node : nodes)
-	{
-		auto meshNode = dynamic_cast<repo::core::model::MeshNode*>(node);
-		switch (meshNode->getPrimitive())
+	auto nVertices = 10;
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 3, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 3, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 3, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 1, "", { rootNodeId })); // unsupported primitive types must be identified as such and not combined with known types
+
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
+
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 2); // The new mpOpt just ignores unsupported primitives
+	
+	// ensure that the batching has been successful.	
+	for (auto& node : mockExporter->getSupermeshes())
+	{		
+		switch (node.getPrimitive())
 		{
 		case repo::core::model::MeshNode::Primitive::LINES:
-			ASSERT_EQ(nVertices * 2, meshNode->getVertices().size());
+			ASSERT_EQ(nVertices * 2, node.getVertices().size());
 			break;
 		case repo::core::model::MeshNode::Primitive::TRIANGLES:
-			ASSERT_EQ(nVertices * 3, meshNode->getVertices().size());
-			break;
-		case repo::core::model::MeshNode::Primitive::POINTS:
-			ASSERT_EQ(nVertices * 1, meshNode->getVertices().size());
+			ASSERT_EQ(nVertices * 3, node.getVertices().size());
 			break;
 		default:
-			repoTrace << (int)meshNode->getPrimitive();
+			repoTrace << (int)node.getPrimitive();
 			EXPECT_TRUE(false); // No other topologies should be encountered in this test.
 			break;
 		}
 	}
 
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
 
 TEST(MultipartOptimizer, TestSingleLargeMesh)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
 
-	auto nMesh = 3;
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
-	meshes.insert(createRandomMesh(65536, false, 2, { rootID }));
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestSingleLargeMesh";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
 
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+	
+	auto largeMesh = createRandomMesh(65536, false, 2, "", { rootNodeId });
+	sceneBuilder.addNode(std::move(largeMesh));
+	
+	sceneBuilder.finalise();
 
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
 
-	EXPECT_EQ(1, scene->getAllMeshes(OPTIMIZED_GRAPH).size());
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
 
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 1);
+
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
 
 TEST(MultipartOptimizer, TestSingleOversizedMesh)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
 
-	auto nMesh = 3;
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
-	meshes.insert(createRandomMesh(1200000 + 1, false, 3, { rootID })); // 1200000 comes from the const in repo_optimizer_multipart.cpp
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestSingleOversizedMesh";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
 
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
+		
+	sceneBuilder.addNode(createRandomMesh(1200000 + 1, false, 3, "", { rootNodeId })); // 1200000 comes from the const in repo_optimizer_multipart.cpp
 
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
+	sceneBuilder.finalise();
 
-	EXPECT_EQ(2, scene->getAllMeshes(OPTIMIZED_GRAPH).size()); // even with one triangle over, large meshes should be split
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
 
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 2); // even with one triangle over, large meshes should be split
+
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
 
 TEST(MultipartOptimizer, TestMultipleOversizedMeshes)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
 
-	auto nMesh = 3;
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
-	meshes.insert(createRandomMesh(65536, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(65537, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(128537, false, 3, { rootID }));
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestMultipleOversizedMeshes";
+	auto revId = repo::lib::RepoUUID::createUUID();
 
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
 
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+
+	sceneBuilder.addNode(createRandomMesh(65536, false, 3, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(65537, false, 3, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(128537, false, 3, "", { rootNodeId }));
+
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
+
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
 
 	// Mesh splitting is not determinsitic so we don't check the final mesh
 	// count in this test
 
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
 
 TEST(MultipartOptimizer, TestMultiplesMeshes)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
+
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestMultiplesMeshes";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
 
 
 	// These vertex counts, along with the primitive size, are multiples of
 	// the max supermesh size and are designed to trip up supermeshing edge
 	// cases
+	sceneBuilder.addNode(createRandomMesh(16384, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(16384, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(16384, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(16384, false, 2, "", { rootNodeId }));
+
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
 
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
-	meshes.insert(createRandomMesh(16384, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(16384, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(16384, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(16384, false, 2, { rootID }));
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
 
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
+	EXPECT_TRUE(result);
 
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
+	EXPECT_TRUE(mockExporter->isFinalised());
 
-	EXPECT_EQ(1, scene->getAllMeshes(OPTIMIZED_GRAPH).size());
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 1);
 
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
 
 TEST(MultipartOptimizer, TestMultipleSmallAndLargeMeshes)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
-
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
-	meshes.insert(createRandomMesh(16384, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(16384, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(16384, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(16384, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(65536, false, 2, { rootID }));
-	meshes.insert(createRandomMesh(128000, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(16384, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(8000, false, 3, { rootID }));
-
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
-
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
-
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestMultipleSmallAndLargeMeshes";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+
+	sceneBuilder.addNode(createRandomMesh(16384, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(16384, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(16384, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(16384, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(65536, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(128000, false, 3, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(16384, false, 3, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(8000, false, 3, "", { rootNodeId }));
+
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
+
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
 
 TEST(MultipartOptimizer, TestTinyMeshes)
 {
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
 
-	repo::core::model::RepoNodeSet meshes, trans, dummy;
-	trans.insert(root);
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestTinyMeshes";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+
 	for (size_t i = 0; i < 10000; i++)
 	{
-		meshes.insert(createRandomMesh(4, false, 3, { rootID }));
+		sceneBuilder.addNode(createRandomMesh(4, false, 3, "", { rootNodeId }));
 	}
 
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, dummy, dummy, dummy, trans);
-	ASSERT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	ASSERT_FALSE(scene->hasRoot(OPTIMIZED_GRAPH));
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
 
-	EXPECT_TRUE(opt.apply(scene));
-	EXPECT_TRUE(scene->hasRoot(DEFAULT_GRAPH));
-	EXPECT_TRUE(scene->hasRoot(OPTIMIZED_GRAPH));
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
 
 	// Make sure no supermesh has more than 5000 mappings (submeshes). We won't
 	// see the effects in the unit test but when we try to commit the node
 	// it will fail.
 
-	for (const auto stash : scene->getAllMeshes(OPTIMIZED_GRAPH))
+	for (const auto supermeshNode : mockExporter->getSupermeshes())
 	{
-		auto mapping = dynamic_cast<repo::core::model::SupermeshNode*>(stash)->getMeshMapping();
+		auto mapping = supermeshNode.getMeshMapping();
 		EXPECT_LE(mapping.size(), 5000);
 	}
 
-	EXPECT_TRUE(compareMeshes(scene->getAllMeshes(DEFAULT_GRAPH), scene->getAllMeshes(OPTIMIZED_GRAPH)));
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
+}
+
+TEST(MultipartOptimizer, TestGroupings)
+{
+	auto opt = MultipartOptimizer();
+
+	auto handler = getHandler();
+	std::string database = DBMULTIPARTOPTIMIZERTEST;
+	std::string projectName = "TestAllMerged";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+
+	auto nVertices = 10;
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 2, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 2, "group1", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 2, "group2", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 3, "", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 3, "group1", { rootNodeId }));
+	sceneBuilder.addNode(createRandomMesh(nVertices, false, 3, "group2", { rootNodeId }));
+
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
+
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 6); // Six groups should have been formed
+
+	EXPECT_TRUE(compareMeshes(
+		database,
+		projectName,
+		revId,
+		mockExporter.get()));
 }
\ No newline at end of file
diff --git a/test/src/unit/repo/manipulator/modelutility/ut_repo_mesh_map_reorganiser.cpp b/test/src/unit/repo/manipulator/modelutility/ut_repo_mesh_map_reorganiser.cpp
index 5940c1a2..c8610b6b 100644
--- a/test/src/unit/repo/manipulator/modelutility/ut_repo_mesh_map_reorganiser.cpp
+++ b/test/src/unit/repo/manipulator/modelutility/ut_repo_mesh_map_reorganiser.cpp
@@ -23,52 +23,79 @@
 #include <limits>
 #include <unordered_set>
 #include <test/src/unit/repo_test_mesh_utils.h>
+#include <test/src/unit/repo_test_database_info.h>
+#include <repo/manipulator/modelutility/repo_scene_builder.h>
 
 using namespace repo::test::utils::mesh;
 using namespace repo::manipulator::modelutility;
 using namespace repo::manipulator::modeloptimizer;
 
+#define DBMESHMAPREORGANISERTEST "meshMapReorganiserTest"
+
 TEST(MeshMapReorganiser, VeryLargeMesh)
 {
 	// This snippet creates a Supermesh using the Multipart Optimizer
 
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
-	repo::core::model::RepoNodeSet meshes, trans;
-	trans.insert(root);
-	meshes.insert(createRandomMesh(327890, false, 3, { rootID }));
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, {}, {}, {}, trans);
-	opt.apply(scene);
 
-	auto supermesh = (repo::core::model::SupermeshNode*)*scene->getAllSupermeshes(repo::core::model::RepoScene::GraphType::OPTIMIZED).begin();
+	auto handler = getHandler();
+	std::string database = DBMESHMAPREORGANISERTEST;
+	std::string projectName = "VeryLargeMesh";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+
+	sceneBuilder.addNode(createRandomMesh(327890, false, 3, "", { rootNodeId }));
+	
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
 
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 1);
+	
 	// Check our test data - the supermesh at this stage should have one, large mapping.
-	EXPECT_EQ(supermesh->getMeshMapping().size(), 1);
+	auto supermesh = mockExporter->getSupermeshes()[0];
+	EXPECT_EQ(supermesh.getMeshMapping().size(), 1);
 
-	MeshMapReorganiser reSplitter(supermesh, 65536, SIZE_MAX);
+	MeshMapReorganiser reSplitter(&supermesh, 65536, SIZE_MAX);
 
 	auto remapped = reSplitter.getRemappedMesh();
 
 	// Check that the primitive has been preserved
-	EXPECT_EQ(remapped.getPrimitive(), supermesh->getPrimitive());
+	EXPECT_EQ(remapped->getPrimitive(), supermesh.getPrimitive());
 
 	// Check that the submesh ids have been preserved. Though the supermesh
 	// will be split into chunks, there should only be one submesh id as there
 	// aren't multiple elements.
 
-	auto ids = remapped.getSubmeshIds();
+	auto ids = remapped->getSubmeshIds();
 	auto end = std::unique(ids.begin(), ids.end());
 	auto count = end - ids.begin();
 	EXPECT_EQ(count, 1);
 
 	// The supermesh should be split into six chunks (with the same submesh ids)
 
-	EXPECT_EQ(remapped.getMeshMapping().size(), 6);
+	EXPECT_EQ(remapped->getMeshMapping().size(), 6);
 
 	// Mapping ids should be identities
 
-	for (auto m : remapped.getMeshMapping())
+	for (auto m : remapped->getMeshMapping())
 	{
 		EXPECT_TRUE(m.mesh_id.isDefaultValue());
 		EXPECT_TRUE(m.material_id.isDefaultValue());
@@ -80,7 +107,7 @@ TEST(MeshMapReorganiser, VeryLargeMesh)
 
 	auto splitMapping = reSplitter.getSplitMapping();
 	EXPECT_EQ(splitMapping.size(), 1);
-	auto originalId = supermesh->getMeshMapping()[0].mesh_id;
+	auto originalId = supermesh.getMeshMapping()[0].mesh_id;
 	auto usageIt = splitMapping.find(originalId);
 	EXPECT_FALSE(usageIt == splitMapping.end());
 	auto usage = usageIt->second;
@@ -93,32 +120,57 @@ TEST(MeshMapReorganiser, MultipleTinyMeshes)
 	// This snippet creates a Supermesh using the Multipart Optimizer
 
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
-	repo::core::model::RepoNodeSet meshes, trans;
-	trans.insert(root);
 
+	auto handler = getHandler();
+	std::string database = DBMESHMAPREORGANISERTEST;
+	std::string projectName = "InterleavedMixedSplit";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+		
 	const int NUM_SUBMESHES = 789;
 	const int NUM_VERTICES = 256;
-
+		
+	std::vector<repo::lib::RepoUUID> meshIds;
 	for (int i = 0; i < NUM_SUBMESHES; i++)
 	{
-		meshes.insert(createRandomMesh(NUM_VERTICES, false, 3, { rootID }));
+		auto node = createRandomMesh(NUM_VERTICES, false, 3, "", { rootNodeId });
+		meshIds.push_back(node->getUniqueID());
+		sceneBuilder.addNode(std::move(node));
 	}
 
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, {}, {}, {}, trans);
-	opt.apply(scene);
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
+
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
 
-	auto supermesh = (repo::core::model::SupermeshNode*)*scene->getAllSupermeshes(repo::core::model::RepoScene::GraphType::OPTIMIZED).begin();
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 1);
 
 	// Check our test data - the supermesh at this stage should have one, large mapping.
-	EXPECT_EQ(supermesh->getMeshMapping().size(), NUM_SUBMESHES);
+	auto supermesh = mockExporter->getSupermeshes()[0];
+	
+	EXPECT_EQ(supermesh.getMeshMapping().size(), NUM_SUBMESHES);
 
-	MeshMapReorganiser reSplitter(supermesh, 65536, SIZE_MAX);
+	MeshMapReorganiser reSplitter(&supermesh, 65536, SIZE_MAX);
 
 	auto remapped = reSplitter.getRemappedMesh();
 
-	auto ids = remapped.getSubmeshIds();
+	auto ids = remapped->getSubmeshIds();
 	auto end = std::unique(ids.begin(), ids.end());
 	auto count = end - ids.begin();
 	EXPECT_EQ(count, NUM_SUBMESHES);
@@ -126,11 +178,11 @@ TEST(MeshMapReorganiser, MultipleTinyMeshes)
 	// The supermesh should be split into two chunks (with the same submesh ids)
 
 	auto expectedSplit = ceil((NUM_SUBMESHES * NUM_VERTICES) / 65536.0f);
-	EXPECT_EQ(remapped.getMeshMapping().size(), expectedSplit);
+	EXPECT_EQ(remapped->getMeshMapping().size(), expectedSplit);
 
 	// Mapping ids should be identities
 
-	for (auto m : remapped.getMeshMapping())
+	for (auto m : remapped->getMeshMapping())
 	{
 		EXPECT_TRUE(m.mesh_id.isDefaultValue());
 		EXPECT_TRUE(m.material_id.isDefaultValue());
@@ -147,9 +199,9 @@ TEST(MeshMapReorganiser, MultipleTinyMeshes)
 
 	std::unordered_set<int> usages;
 
-	for (auto m : meshes)
+	for (auto id : meshIds)
 	{
-		auto split = splitMapping.find(m->getUniqueID());
+		auto split = splitMapping.find(id);
 
 		// Every original mesh id should be present in the split mappings
 		EXPECT_TRUE(split != splitMapping.end());
@@ -172,34 +224,69 @@ TEST(MeshMapReorganiser, InterleavedMixedSplit)
 	// that will need to be split.
 
 	auto opt = MultipartOptimizer();
-	auto root = new repo::core::model::TransformationNode(repo::core::model::RepoBSONFactory::makeTransformationNode());
-	auto rootID = root->getSharedID();
-	repo::core::model::RepoNodeSet meshes, trans;
-	trans.insert(root);
-
-	meshes.insert(createRandomMesh(129, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(65537, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(1341, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(68, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(80000, false, 3, { rootID }));
-	meshes.insert(createRandomMesh(17981, false, 3, { rootID }));
 
+	auto handler = getHandler();
+	std::string database = DBMESHMAPREORGANISERTEST;
+	std::string projectName = "InterleavedMixedSplit";
+	auto revId = repo::lib::RepoUUID::createUUID();
+
+	auto sceneBuilder = repo::manipulator::modelutility::RepoSceneBuilder(handler, database, projectName, revId);
+	
+	auto rootNode = repo::core::model::RepoBSONFactory::makeTransformationNode({}, "rootNode", {});
+	sceneBuilder.addNode(rootNode);
+	auto rootNodeId = rootNode.getSharedID();
+		
+	auto randNode1 = createRandomMesh(129, false, 3, "", { rootNodeId });
+	auto randNode2 = createRandomMesh(65537, false, 3, "", { rootNodeId });
+	auto randNode3 = createRandomMesh(1341, false, 3, "", { rootNodeId });
+	auto randNode4 = createRandomMesh(68, false, 3, "", { rootNodeId });
+	auto randNode5 = createRandomMesh(80000, false, 3, "", { rootNodeId });
+	auto randNode6 = createRandomMesh(17981, false, 3, "", { rootNodeId });
+	
+	std::vector<repo::lib::RepoUUID> meshIds;
+	meshIds.push_back(randNode1->getUniqueID());
+	meshIds.push_back(randNode2->getUniqueID());
+	meshIds.push_back(randNode3->getUniqueID());
+	meshIds.push_back(randNode4->getUniqueID());
+	meshIds.push_back(randNode5->getUniqueID());
+	meshIds.push_back(randNode6->getUniqueID());
+
+	sceneBuilder.addNode(std::move(randNode1));
+	sceneBuilder.addNode(std::move(randNode2));
+	sceneBuilder.addNode(std::move(randNode3));
+	sceneBuilder.addNode(std::move(randNode4));
+	sceneBuilder.addNode(std::move(randNode5));
+	sceneBuilder.addNode(std::move(randNode6));
+
+	sceneBuilder.finalise();
+
+	auto mockExporter = std::make_unique<TestModelExport>(handler.get(), database, projectName, revId, std::vector<double>({ 0, 0, 0 }));
+
+	bool result = opt.processScene(
+		database,
+		projectName,
+		revId,
+		handler.get(),
+		mockExporter.get()
+	);
+
+	EXPECT_TRUE(result);
+
+	EXPECT_TRUE(mockExporter->isFinalised());
+
+	EXPECT_EQ(mockExporter->getSupermeshCount(), 1);
+		
+	// Check our test data	
+	auto supermesh = mockExporter->getSupermeshes()[0];
+	
 	const int NUM_SUBMESHES = 6;
+	EXPECT_EQ(supermesh.getMeshMapping().size(), NUM_SUBMESHES);
 
-	repo::core::model::RepoScene* scene = new repo::core::model::RepoScene({}, meshes, {}, {}, {}, trans);
-	opt.apply(scene);
-
-	auto supermesh = (repo::core::model::SupermeshNode*)*scene->getAllSupermeshes(repo::core::model::RepoScene::GraphType::OPTIMIZED).begin();
-
-	// Check our test data
-
-	EXPECT_EQ(supermesh->getMeshMapping().size(), NUM_SUBMESHES);
-
-	MeshMapReorganiser reSplitter(supermesh, 65536, SIZE_MAX);
+	MeshMapReorganiser reSplitter(&supermesh, 65536, SIZE_MAX);
 
 	auto remapped = reSplitter.getRemappedMesh();
 
-	auto splits = remapped.getMeshMapping();
+	auto splits = remapped->getMeshMapping();
 
 	// Mapping ids should be identities
 
@@ -210,7 +297,7 @@ TEST(MeshMapReorganiser, InterleavedMixedSplit)
 		EXPECT_TRUE(m.shared_id.isDefaultValue());
 	}
 
-	auto ids = remapped.getSubmeshIds();
+	auto ids = remapped->getSubmeshIds();
 	auto end = std::unique(ids.begin(), ids.end());
 	auto count = end - ids.begin();
 	EXPECT_EQ(count, NUM_SUBMESHES);
@@ -222,9 +309,9 @@ TEST(MeshMapReorganiser, InterleavedMixedSplit)
 	EXPECT_EQ(splitMapping.size(), NUM_SUBMESHES);
 
 	std::unordered_set<int> usages;
-	for (auto m : meshes)
+	for (auto id : meshIds)
 	{
-		auto split = splitMapping.find(m->getUniqueID());
+		auto split = splitMapping.find(id);
 
 		// Every original mesh id should be present in the split mappings
 		EXPECT_TRUE(split != splitMapping.end());
diff --git a/test/src/unit/repo_test_common_tests.cpp b/test/src/unit/repo_test_common_tests.cpp
new file mode 100644
index 00000000..37857b4b
--- /dev/null
+++ b/test/src/unit/repo_test_common_tests.cpp
@@ -0,0 +1,46 @@
+/**
+*  Copyright (C) 2025 3D Repo Ltd
+*
+*  This program is free software: you can redistribute it and/or modify
+*  it under the terms of the GNU Affero General Public License as
+*  published by the Free Software Foundation, either version 3 of the
+*  License, or (at your option) any later version.
+*
+*  This program is distributed in the hope that it will be useful,
+*  but WITHOUT ANY WARRANTY; without even the implied warranty of
+*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+*  GNU Affero General Public License for more details.
+*
+*  You should have received a copy of the GNU Affero General Public License
+*  along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include "repo_test_common_tests.h"
+
+#include <gtest/gtest.h>
+#include <gmock/gmock.h>
+#include <gtest/gtest-matchers.h>
+
+using namespace testing;
+using namespace testing::common;
+
+void testing::common::checkMetadataInheritence(SceneUtils& scene)
+{
+	for (auto& metadata : scene.getMetadataNodes())
+	{
+		auto meshParents = metadata.getParents({ repo::core::model::NodeType::MESH });
+
+		for (auto& parent : metadata.getParents({ repo::core::model::NodeType::TRANSFORMATION }))
+		{
+			// When a metadata node is a child of transformation leaf node (i.e.
+			// with only unnamed meshes), it should also be a child of those mesh
+			// nodes.
+
+			if (parent.isLeaf())
+			{
+				auto meshSiblings = parent.getMeshes();
+				EXPECT_THAT(meshSiblings, IsSubsetOf(meshParents));
+			}
+		}
+	}
+}
\ No newline at end of file
diff --git a/test/src/unit/repo_test_common_tests.h b/test/src/unit/repo_test_common_tests.h
new file mode 100644
index 00000000..865186e6
--- /dev/null
+++ b/test/src/unit/repo_test_common_tests.h
@@ -0,0 +1,26 @@
+/**
+*  Copyright (C) 2025 3D Repo Ltd
+*
+*  This program is free software: you can redistribute it and/or modify
+*  it under the terms of the GNU Affero General Public License as
+*  published by the Free Software Foundation, either version 3 of the
+*  License, or (at your option) any later version.
+*
+*  This program is distributed in the hope that it will be useful,
+*  but WITHOUT ANY WARRANTY; without even the implied warranty of
+*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+*  GNU Affero General Public License for more details.
+*
+*  You should have received a copy of the GNU Affero General Public License
+*  along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#pragma once
+
+#include "repo_test_scene_utils.h"
+
+namespace testing {
+	namespace common {
+		void checkMetadataInheritence(testing::SceneUtils& scene);
+	}
+}
\ No newline at end of file
diff --git a/test/src/unit/repo_test_mesh_utils.cpp b/test/src/unit/repo_test_mesh_utils.cpp
index 4091cd3f..9274ccc4 100644
--- a/test/src/unit/repo_test_mesh_utils.cpp
+++ b/test/src/unit/repo_test_mesh_utils.cpp
@@ -27,42 +27,50 @@
 using namespace repo::core::model;
 using namespace testing;
 
-MeshNode* repo::test::utils::mesh::createRandomMesh(const int nVertices, const bool hasUV, const int primitiveSize, const std::vector<repo::lib::RepoUUID>& parent)
+std::unique_ptr<MeshNode> repo::test::utils::mesh::createRandomMesh(
+	const int nVertices,
+	const bool hasUV,
+	const int primitiveSize,
+	const std::string grouping,
+	const std::vector<repo::lib::RepoUUID>& parent)
 {
-	auto mesh = makeMeshNode(mesh_data(true, true, 0, primitiveSize, false, hasUV ? 1 : 0, nVertices));
+	auto mesh = makeMeshNode(mesh_data(true, true, 0, primitiveSize, false, hasUV ? 1 : 0, nVertices, grouping));
 	mesh.addParents(parent);
-	return new MeshNode(mesh);
+
+	// The new mpOpt drops geometry that has no material, so we set the default here
+	mesh.setMaterial(repo::lib::repo_material_t::DefaultMaterial());	
+
+	return std::make_unique<MeshNode>(mesh);
 }
 
-bool repo::test::utils::mesh::compareMeshes(repo::core::model::RepoNodeSet original, repo::core::model::RepoNodeSet stash)
+bool repo::test::utils::mesh::compareMeshes(
+	std::string database,
+	std::string projectName,
+	repo::lib::RepoUUID revId,
+	TestModelExport* mockExporter
+) 
 {
-	std::vector<GenericFace> defaultFaces;
-	for (const auto node : original)
-	{
-		addFaces(dynamic_cast<repo::core::model::MeshNode*>(node), defaultFaces);
-	}
+	// Get processed geometry from the mock exporter
+	auto processedFaces = getFacesFromMockExporter(mockExporter);
 
-	std::vector<GenericFace> stashFaces;
-	for (const auto node : stash)
-	{
-		addFaces(dynamic_cast<repo::core::model::MeshNode*>(node), stashFaces);
-	}
+	// Get unprocessed geometry from the database
+	auto unprocessedFaces = getFacesFromDatabase(database, projectName, revId);
 
-	if (defaultFaces.size() != stashFaces.size())
+	// Check for lenght equality first
+	if (unprocessedFaces.size() != processedFaces.size())
 	{
 		return false;
 	}
 
 	// Faces are compared exactly using a hash table for speed.
-
 	std::map<long, std::vector<GenericFace>> actual;
 
-	for (auto& face : stashFaces)
+	for (auto& face : processedFaces)
 	{
 		actual[face.hash()].push_back(face);
 	}
 
-	for (auto& face : defaultFaces)
+	for (auto& face : unprocessedFaces)
 	{
 		auto& others = actual[face.hash()];
 		for (auto& other : others) {
@@ -81,7 +89,7 @@ bool repo::test::utils::mesh::compareMeshes(repo::core::model::RepoNodeSet origi
 
 	// Did we find a match for all faces?
 
-	for (const auto& face : defaultFaces)
+	for (const auto& face : unprocessedFaces)
 	{
 		if (!face.hit)
 		{
@@ -92,18 +100,156 @@ bool repo::test::utils::mesh::compareMeshes(repo::core::model::RepoNodeSet origi
 	return true;
 }
 
-void repo::test::utils::mesh::addFaces(repo::core::model::MeshNode* mesh, std::vector<GenericFace>& faces)
+
+// Helper method for getting binary data from the nodes in getFacesFromDatabase(...)
+template <class T>
+void deserialiseVector(
+	const repo::core::model::RepoBSON& bson,
+	const std::vector<uint8_t>& buffer,
+	std::vector<T>& vec)
 {
-	if (mesh == nullptr)
-	{
-		return;
+	auto start = bson.getIntField(REPO_LABEL_BINARY_START);
+	auto size = bson.getIntField(REPO_LABEL_BINARY_SIZE);
+
+	vec.resize(size / sizeof(T));
+	memcpy(vec.data(), buffer.data() + (sizeof(uint8_t) * start), size);
+}
+
+std::vector<repo::test::utils::mesh::GenericFace> repo::test::utils::mesh::getFacesFromDatabase(std::string database, std::string projectName, repo::lib::RepoUUID revId)
+{
+	std::vector<GenericFace> genericFaces;
+	
+	// Assemble query for db.
+	auto handler = getHandler();
+	std::string sceneCollection = projectName + "." + REPO_COLLECTION_SCENE;
+	repo::core::handler::fileservice::BlobFilesHandler blobHandler(handler->getFileManager(), database, sceneCollection);
+
+	repo::core::handler::database::query::RepoQueryBuilder filter;
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_REVISION_ID, revId));
+	filter.append(repo::core::handler::database::query::Eq(REPO_NODE_LABEL_TYPE, std::string(REPO_NODE_TYPE_MESH)));
+
+	// One test inserts unknown primitive types in the database to see whether the mpOpt processes them. For a accurate comparison we need to ignore them.
+	filter.append(repo::core::handler::database::query::Or(
+		repo::core::handler::database::query::Eq(REPO_NODE_MESH_LABEL_PRIMITIVE, 2),
+		repo::core::handler::database::query::Eq(REPO_NODE_MESH_LABEL_PRIMITIVE, 3)
+	));
+
+	// Query
+	auto cursor = handler->findCursorByCriteria(database, sceneCollection, filter);
+
+	// Process the results
+	for (auto bson : (*cursor)) {
+
+		std::vector<repo::lib::RepoVector3D> vertices;
+		std::vector<repo::lib::RepoVector3D> normals;
+		std::vector<repo::lib::repo_face_t> faces;
+		std::vector < std::vector<repo::lib::RepoVector2D>> channels;
+
+		auto binRef = bson.getBinaryReference();
+		auto dataRef = repo::core::handler::fileservice::DataRef::deserialise(binRef);
+		auto buffer = blobHandler.readToBuffer(dataRef);
+
+		auto blobRefBson = bson.getObjectField(REPO_LABEL_BINARY_REFERENCE);
+		auto elementsBson = blobRefBson.getObjectField(REPO_LABEL_BINARY_ELEMENTS);
+
+		if (elementsBson.hasField(REPO_NODE_MESH_LABEL_VERTICES)) {
+			auto vertBson = elementsBson.getObjectField(REPO_NODE_MESH_LABEL_VERTICES);
+			deserialiseVector(vertBson, buffer, vertices);
+		}
+
+		if (elementsBson.hasField(REPO_NODE_MESH_LABEL_NORMALS)) {
+			auto normBson = elementsBson.getObjectField(REPO_NODE_MESH_LABEL_NORMALS);
+			deserialiseVector(normBson, buffer, normals);
+		}
+
+		if (bson.hasField(REPO_NODE_MESH_LABEL_UV_CHANNELS_COUNT))
+		{
+			// The new multipart optimiser drops UVs of meshes that have no texture,
+			// so we need to make sure that we drop it here too.
+			auto matFilterBson = bson.getObjectField(REPO_FILTER_OBJECT_NAME);
+			if (matFilterBson.hasField(REPO_FILTER_PROP_TEXTURE_ID)) {
+								
+				std::vector<repo::lib::RepoVector2D> serialisedChannels;
+				auto uvBson = elementsBson.getObjectField(REPO_NODE_MESH_LABEL_UV_CHANNELS);
+				deserialiseVector(uvBson, buffer, serialisedChannels);
+
+				if (serialisedChannels.size())
+				{
+					//get number of channels and split the serialised.
+					uint32_t nChannels = bson.getIntField(REPO_NODE_MESH_LABEL_UV_CHANNELS_COUNT);
+					uint32_t vecPerChannel = serialisedChannels.size() / nChannels;
+					channels.reserve(nChannels);
+					for (uint32_t i = 0; i < nChannels; i++)
+					{
+						channels.push_back(std::vector<repo::lib::RepoVector2D>());
+						channels[i].reserve(vecPerChannel);
+
+						uint32_t offset = i * vecPerChannel;
+						channels[i].insert(channels[i].begin(), serialisedChannels.begin() + offset,
+							serialisedChannels.begin() + offset + vecPerChannel);
+					}
+				}
+			}
+		}
+
+		if (elementsBson.hasField(REPO_NODE_MESH_LABEL_FACES)) {
+
+			int32_t faceCount = bson.getIntField(REPO_NODE_MESH_LABEL_FACES_COUNT);
+			faces.reserve(faceCount);
+
+			std::vector<uint32_t> serialisedFaces = std::vector<uint32_t>();
+			auto faceBson = elementsBson.getObjectField(REPO_NODE_MESH_LABEL_FACES);
+			deserialiseVector(faceBson, buffer, serialisedFaces);
+
+			int mNumIndicesIndex = 0;
+			while (serialisedFaces.size() > mNumIndicesIndex)
+			{
+				int mNumIndices = serialisedFaces[mNumIndicesIndex];
+				if (serialisedFaces.size() > mNumIndicesIndex + mNumIndices)
+				{
+					repo::lib::repo_face_t face;
+					face.resize(mNumIndices);
+					for (int i = 0; i < mNumIndices; ++i)
+						face[i] = serialisedFaces[mNumIndicesIndex + 1 + i];
+					faces.push_back(face);
+					mNumIndicesIndex += mNumIndices + 1;
+				}
+				else
+				{
+					repoError << "Cannot copy all faces. Buffer size is smaller than expected!";
+				}
+			}
+
+		}
+
+		// Add this mesh's faces
+		addFaces(vertices, normals, faces, channels, genericFaces);
+
 	}
+	
+	return genericFaces;
+}
+
+std::vector<repo::test::utils::mesh::GenericFace> repo::test::utils::mesh::getFacesFromMockExporter(TestModelExport* mockExporter)
+{
+	std::vector<GenericFace> genericFaces;
+
+	auto meshes = mockExporter->getSupermeshes();
+
+	for (auto& supermesh : meshes) {
+		addFaces(supermesh, genericFaces);
+	}
+
+	return genericFaces;
+}
 
-	auto vertices = mesh->getVertices();
-	auto channels = mesh->getUVChannelsSeparated();
-	auto normals = mesh->getNormals();
+void repo::test::utils::mesh::addFaces(repo::core::model::MeshNode &mesh, std::vector<GenericFace>& faces)
+{
+	auto vertices = mesh.getVertices();
+	auto channels = mesh.getUVChannelsSeparated();
+	auto normals = mesh.getNormals();
 
-	for (const auto face : mesh->getFaces())
+	for (const auto face : mesh.getFaces())
 	{
 		GenericFace portableFace;
 		portableFace.hit = 0;
@@ -127,6 +273,37 @@ void repo::test::utils::mesh::addFaces(repo::core::model::MeshNode* mesh, std::v
 	}
 }
 
+void repo::test::utils::mesh::addFaces(
+	const std::vector<repo::lib::RepoVector3D>& vertices,
+	const std::vector<repo::lib::RepoVector3D>& normals,
+	const std::vector<repo::lib::repo_face_t>& faces,
+	const std::vector<std::vector<repo::lib::RepoVector2D>> &uvChannels,
+	std::vector<GenericFace>& genericFaces)
+{
+	for (const auto face : faces)
+	{
+		GenericFace portableFace;
+		portableFace.hit = 0;
+
+		for (const auto index : face)
+		{
+			portableFace.push(vertices[index]);
+
+			for (const auto channel : uvChannels)
+			{
+				portableFace.push(channel[index]);
+			}
+
+			if (normals.size())
+			{
+				portableFace.push(normals[index]);
+			}
+		}
+
+		genericFaces.push_back(portableFace);
+	}
+}
+
 /**
 * This implementation should be the reference for the node database schema and
 * should be effectively independent, but equivalent, to the serialise method
@@ -242,6 +419,11 @@ RepoBSON repo::test::utils::mesh::meshNodeTestBSONFactory(mesh_data data)
 		}
 	}
 
+	// Grouping
+	if (!data.grouping.empty()) {
+		builder.append(REPO_NODE_MESH_LABEL_GROUPING, data.grouping);
+	}
+
 	return builder.obj();
 }
 
@@ -253,7 +435,7 @@ MeshNode repo::test::utils::mesh::makeMeshNode(mesh_data data)
 MeshNode repo::test::utils::mesh::makeDeterministicMeshNode(int primitive, bool normals, int uvs)
 {
 	restartRand();
-	return makeMeshNode(mesh_data(false, false, false, primitive, normals, uvs, 100));
+	return makeMeshNode(mesh_data(false, false, false, primitive, normals, uvs, 100, ""));
 }
 
 std::vector<repo::lib::RepoVector3D> repo::test::utils::mesh::makeVertices(int num)
@@ -327,7 +509,8 @@ repo::test::utils::mesh::mesh_data::mesh_data(
 	int faceSize,
 	bool normals,
 	int numUvChannels,
-	int numVertices
+	int numVertices,
+	std::string grouping
 )
 {
 	if (name) {
@@ -382,6 +565,8 @@ repo::test::utils::mesh::mesh_data::mesh_data(
 	this->boundingBox.push_back({
 		max.x, max.y, max.z
 		});
+
+	this->grouping = grouping;
 }
 
 repo::lib::RepoMatrix repo::test::utils::mesh::makeTransform(bool translation, bool rotation)
@@ -417,6 +602,7 @@ void repo::test::utils::mesh::compareMeshNode(mesh_data expected, MeshNode actua
 	{
 		EXPECT_THAT(actual.getUVChannelsSeparated()[i], ElementsAreArray(expected.uvChannels[i]));
 	}
+	EXPECT_EQ(actual.getGrouping(), expected.grouping);
 }
 
 float repo::test::utils::mesh::hausdorffDistance(const std::vector<repo::core::model::MeshNode>& meshes)
diff --git a/test/src/unit/repo_test_mesh_utils.h b/test/src/unit/repo_test_mesh_utils.h
index 47eb6e35..b598c2ba 100644
--- a/test/src/unit/repo_test_mesh_utils.h
+++ b/test/src/unit/repo_test_mesh_utils.h
@@ -22,6 +22,10 @@
 
 #include <boost/functional/hash.hpp>
 #include <repo/core/model/bson/repo_node_mesh.h>
+#include <repo/manipulator/modelconvertor/export/repo_model_export_abstract.h>
+#include <repo/core/handler/database/repo_query.h>
+#include <repo/core/handler/fileservice/repo_data_ref.h>
+#include <repo/core/handler/fileservice/repo_blob_files_handler.h>
 
 namespace repo {
 	namespace test {
@@ -42,7 +46,8 @@ namespace repo {
 						int faceSize,
 						bool normals,
 						int numUvChannels,
-						int numVertices
+						int numVertices,
+						std::string grouping
 					);
 
 					std::string name;
@@ -54,6 +59,7 @@ namespace repo {
 					std::vector<repo::lib::repo_face_t> faces;
 					std::vector<repo::lib::RepoVector3D> normals;
 					std::vector<std::vector<repo::lib::RepoVector2D>> uvChannels;
+					std::string grouping;
 				};
 
 				struct GenericFace
@@ -90,14 +96,55 @@ namespace repo {
 					}
 				};
 
+				// Dummy exporter to catch geometry produced by the multipart optimizer
+				class TestModelExport : public repo::manipulator::modelconvertor::AbstractModelExport
+				{
+				public:
+					TestModelExport(
+						repo::core::handler::AbstractDatabaseHandler *dbHandler,
+						const std::string databaseName,
+						const std::string projectName,
+						const repo::lib::RepoUUID revId,
+						const std::vector<double> worldOffset
+					) : AbstractModelExport(dbHandler, databaseName, projectName, revId, worldOffset)
+					{						
+					}
+
+					void addSupermesh(repo::core::model::SupermeshNode* supermesh) {
+						supermeshNodes.push_back(*supermesh);
+					}
+
+					void finalise() {
+						finalised = true;
+						// Do nothing else
+					};
+
+					bool isFinalised() {
+						return finalised;
+					};
+
+					std::vector<repo::core::model::SupermeshNode> getSupermeshes() {
+						return supermeshNodes;
+					};
+
+					int getSupermeshCount() {
+						return supermeshNodes.size();
+					};
+
+				private:
+					bool finalised = false;
+					std::vector<repo::core::model::SupermeshNode> supermeshNodes;
+				};
+
 				/**
 				* Builds a triangle soup MeshNode with exactly nVertices, and faces
 				* constructed such that each vertex is referenced at least once.
 				*/
-				repo::core::model::MeshNode* createRandomMesh(
+				std::unique_ptr<repo::core::model::MeshNode> createRandomMesh(
 					const int nVertices,
 					const bool hasUV,
 					const int primitiveSize,
+					const std::string grouping,
 					const std::vector<repo::lib::RepoUUID>& parent);
 
 				/*
@@ -105,13 +152,33 @@ namespace repo {
 				* hulls are identical.
 				*/
 				bool compareMeshes(
-					repo::core::model::RepoNodeSet original,
-					repo::core::model::RepoNodeSet stash);
+					std::string database,
+					std::string projectName,
+					repo::lib::RepoUUID revId,
+					TestModelExport* mockExporter
+				);
+
+				std::vector<GenericFace> getFacesFromDatabase(
+					std::string database,
+					std::string projectName,
+					repo::lib::RepoUUID revId
+				);
+
+				std::vector<GenericFace> getFacesFromMockExporter(
+					TestModelExport* mockExporter
+				);
 
 				void addFaces(
-					repo::core::model::MeshNode* mesh,
+					repo::core::model::MeshNode &mesh,
 					std::vector<GenericFace>& faces);
 
+				void addFaces(
+					const std::vector<repo::lib::RepoVector3D> &vertices,
+					const std::vector<repo::lib::RepoVector3D> &normals,
+					const std::vector<repo::lib::repo_face_t> &faces,
+					const std::vector < std::vector<repo::lib::RepoVector2D>>& uvChannels,
+					std::vector<GenericFace>& genericFaces);
+
 				/*
 				* Creates a RepoBSON for a MeshNode based on the mesh_data
 				*/
diff --git a/test/src/unit/repo_test_scene_utils.cpp b/test/src/unit/repo_test_scene_utils.cpp
index 7a1332c4..87836e91 100644
--- a/test/src/unit/repo_test_scene_utils.cpp
+++ b/test/src/unit/repo_test_scene_utils.cpp
@@ -27,18 +27,56 @@ using namespace testing;
 
 std::vector<SceneUtils::NodeInfo> SceneUtils::findNodesByMetadata(std::string key, std::string value)
 {
-	std::vector<NodeInfo> info;
+	std::vector<repo::core::model::RepoNode*> transforms;
+	std::vector<repo::core::model::RepoNode*> meshes;
+	std::set<repo::lib::RepoUUID> parents;
 	for (auto& n : scene->getAllMetadata(repo::core::model::RepoScene::GraphType::DEFAULT))
 	{
 		auto m = dynamic_cast<MetadataNode*>(n);
 		auto metadata = m->getAllMetadata();
 		if (boost::apply_visitor(repo::lib::StringConversionVisitor(), metadata[key]) == value) {
 			for (auto p : m->getParentIDs()) {
-				info.push_back(getNodeInfo(scene->getNodeBySharedID(repo::core::model::RepoScene::GraphType::DEFAULT, p)));
+
+				auto n = scene->getNodeBySharedID(repo::core::model::RepoScene::GraphType::DEFAULT, p);
+
+				if (n->getTypeAsEnum() == repo::core::model::NodeType::MESH) {
+					meshes.push_back(n);
+				}
+				else if (n->getTypeAsEnum() == repo::core::model::NodeType::TRANSFORMATION) {
+					transforms.push_back(n);
+				}
 			}
 		}
 	}
-	return info;
+
+	std::vector<NodeInfo> nodes;
+
+	for (auto& t : transforms) {
+		nodes.push_back(getNodeInfo(t));
+		parents.insert(t->getSharedID());
+	}
+
+	for (auto& m : meshes) {
+		if (m->getName().size()) {
+			nodes.push_back(getNodeInfo(m));
+			continue;
+		}
+
+		bool hasParentInList = false;
+		for (auto& p : transforms) {
+			for (auto& mp : m->getParentIDs()) {
+				if (p->getSharedID() == mp) {
+					hasParentInList = true;
+				}
+			}
+		}
+
+		if (!hasParentInList) {
+			nodes.push_back(getNodeInfo(m));
+		}
+	}
+
+	return nodes;
 }
 
 SceneUtils::NodeInfo SceneUtils::findNodeByMetadata(std::string key, std::string value)
@@ -119,25 +157,27 @@ SceneUtils::NodeInfo SceneUtils::findLeafNode(std::string name)
 	return nodes[0];
 }
 
-std::vector<SceneUtils::NodeInfo> SceneUtils::getChildNodes(repo::core::model::RepoNode* node, bool ignoreMeta)
+std::vector<SceneUtils::NodeInfo> SceneUtils::getChildNodes(repo::core::model::RepoNode* node, Filter filter)
 {
 	std::vector<NodeInfo> nodes;
 	for (auto& n : scene->getChildrenAsNodes(repo::core::model::RepoScene::GraphType::DEFAULT, node->getSharedID()))
 	{
-		if (ignoreMeta && n->getTypeAsEnum() == repo::core::model::NodeType::METADATA) {
-			continue;
+		if (filter.size() == 0 || std::find(filter.begin(), filter.end(), n->getTypeAsEnum()) != filter.end()) {
+			nodes.push_back(getNodeInfo(n));
 		}
-		nodes.push_back(getNodeInfo(n));
 	}
 	return nodes;
 }
 
-std::vector<SceneUtils::NodeInfo> SceneUtils::getParentNodes(repo::core::model::RepoNode* node)
+std::vector<SceneUtils::NodeInfo> SceneUtils::getParentNodes(repo::core::model::RepoNode* node, Filter filter)
 {
 	std::vector<NodeInfo> nodes;
-	for (auto& n : scene->getParentNodesFiltered(repo::core::model::RepoScene::GraphType::DEFAULT, node, repo::core::model::NodeType::TRANSFORMATION))
-	{
-		nodes.push_back(getNodeInfo(n));
+	for (auto& pid : node->getParentIDs()) {
+		auto p = scene->getNodeBySharedID(repo::core::model::RepoScene::GraphType::DEFAULT, pid);
+		if(filter.size() == 0 || std::find(filter.begin(), filter.end(), p->getTypeAsEnum()) != filter.end())
+		{
+			nodes.push_back(getNodeInfo(p));
+		}
 	}
 	return nodes;
 }
@@ -180,11 +220,9 @@ std::vector<SceneUtils::NodeInfo> SceneUtils::NodeInfo::getMeshes(repo::core::mo
 std::vector<SceneUtils::NodeInfo> SceneUtils::NodeInfo::getMeshes()
 {
 	std::vector<SceneUtils::NodeInfo> meshNodes;
-	for (auto& c : scene->getChildNodes(node, true))
+	for (auto& c : scene->getChildNodes(node, { repo::core::model::NodeType::MESH }))
 	{
-		if (dynamic_cast<MeshNode*>(c.node)) {
-			meshNodes.push_back(c);
-		}
+		meshNodes.push_back(c);
 	}
 	if (dynamic_cast<MeshNode*>(this->node)) {
 		meshNodes.push_back(*this);
@@ -239,10 +277,8 @@ std::vector<SceneUtils::NodeInfo> SceneUtils::NodeInfo::getTextures()
 	}
 	
 	for (auto m : meshes) {
-		for (auto c : scene->getChildNodes(m, true)) {
-			if (auto material = dynamic_cast<MaterialNode*>(c.node)) {
-				materials.push_back(material);
-			}
+		for (auto c : scene->getChildNodes(m, { repo::core::model::NodeType::MATERIAL })) {
+			materials.push_back(dynamic_cast<MaterialNode*>(c.node));
 		}
 	}
 	
@@ -251,16 +287,27 @@ std::vector<SceneUtils::NodeInfo> SceneUtils::NodeInfo::getTextures()
 	}
 	
 	for (auto m : materials) {
-		for (auto& c : scene->getChildNodes(m, true)) {
-			if (dynamic_cast<TextureNode*>(c.node)) {
-				textures.push_back(c);
-			}
+		for (auto& c : scene->getChildNodes(m, { repo::core::model::NodeType::TEXTURE })) {
+			textures.push_back(c);
 		}
 	}
 
 	return textures;
 }
 
+std::vector<SceneUtils::NodeInfo> SceneUtils::NodeInfo::getSiblings(Filter p, Filter s)
+{
+	std::vector<SceneUtils::NodeInfo> siblings;
+	for (auto& p : this->getParents(p)) {
+		for (auto& c : scene->getChildNodes(p.node, s)) {
+			if (c != *this) {
+				siblings.push_back(c);
+			}
+		}
+	}
+	return siblings;
+}
+
 std::vector<SceneUtils::NodeInfo> SceneUtils::getMeshes()
 {
 	std::vector<SceneUtils::NodeInfo> meshes;
@@ -270,26 +317,31 @@ std::vector<SceneUtils::NodeInfo> SceneUtils::getMeshes()
 	return meshes;
 }
 
+std::vector<SceneUtils::NodeInfo> SceneUtils::getMetadataNodes()
+{
+	std::vector<SceneUtils::NodeInfo> metadata;
+	for (auto n : scene->getAllMetadata(repo::core::model::RepoScene::GraphType::DEFAULT)) {
+		metadata.push_back(getNodeInfo(n));
+	}
+	return metadata;
+}
+
 std::unordered_map<std::string, repo::lib::RepoVariant> SceneUtils::NodeInfo::getMetadata()
 {
 	std::unordered_map<std::string, repo::lib::RepoVariant> metadata;
-	for (auto c : scene->getChildNodes(node, false))
+	for (auto c : scene->getChildNodes(node, { repo::core::model::NodeType::METADATA }))
 	{
-		if (dynamic_cast<MetadataNode*>(c.node)) {
-			auto m = dynamic_cast<MetadataNode*>(c.node)->getAllMetadata();
-			metadata.insert(m.begin(), m.end());
-		}
+		auto m = dynamic_cast<MetadataNode*>(c.node)->getAllMetadata();
+		metadata.insert(m.begin(), m.end());
 	}
 	return metadata;
 }
 
 repo::lib::repo_material_t SceneUtils::NodeInfo::getMaterial()
 {
-	for (auto& c : scene->getChildNodes(node, true))
+	for (auto& c : scene->getChildNodes(node, {repo::core::model::NodeType::MATERIAL}))
 	{
-		if (dynamic_cast<MaterialNode*>(c.node)) {
-			return dynamic_cast<MaterialNode*>(c.node)->getMaterialStruct();
-		}
+		return dynamic_cast<MaterialNode*>(c.node)->getMaterialStruct();
 	}
 	throw std::runtime_error("Node does not have a material");
 }
@@ -316,7 +368,7 @@ bool SceneUtils::NodeInfo::hasTransparency()
 std::vector<std::string> SceneUtils::NodeInfo::getChildNames()
 {
 	std::vector<std::string> names;
-	for (auto c : scene->getChildNodes(node, true))
+	for (auto c : scene->getChildNodes(node, { repo::core::model::NodeType::MESH, repo::core::model::NodeType::TRANSFORMATION }))
 	{
 		if (!c.node->getName().empty()) {
 			names.push_back(c.node->getName());
@@ -327,13 +379,19 @@ std::vector<std::string> SceneUtils::NodeInfo::getChildNames()
 
 SceneUtils::NodeInfo SceneUtils::NodeInfo::getParent() const
 {
-	auto parents = scene->getParentNodes(node);
+	auto parents = scene->getParentNodes(node, { repo::core::model::NodeType::TRANSFORMATION });
 	return parents[0];
 }
 
+std::vector<SceneUtils::NodeInfo> SceneUtils::NodeInfo::getParents(SceneUtils::Filter filter) const
+{
+	auto parents = scene->getParentNodes(node, filter);
+	return parents;
+}
+
 std::string SceneUtils::NodeInfo::getPath() const
 {
-	auto parents = scene->getParentNodes(node);
+	auto parents = scene->getParentNodes(node, { repo::core::model::NodeType::TRANSFORMATION });
 	if (parents.size()) {
 		return parents[0].getPath() + "->" + name();
 	}
diff --git a/test/src/unit/repo_test_scene_utils.h b/test/src/unit/repo_test_scene_utils.h
index f5b5ad67..e5216b9c 100644
--- a/test/src/unit/repo_test_scene_utils.h
+++ b/test/src/unit/repo_test_scene_utils.h
@@ -32,6 +32,8 @@ namespace testing {
 		{
 		}
 
+		using Filter = std::initializer_list<repo::core::model::NodeType>;
+
 		struct NodeInfo
 		{
 			size_t numVisibleChildren;
@@ -60,12 +62,14 @@ namespace testing {
 
 			NodeInfo getParent() const;
 
+			std::vector<NodeInfo> getParents(Filter parents) const;
+
 			SceneUtils* scene;
 			repo::core::model::RepoNode* node;
 
-			std::vector<NodeInfo> getChildren()
+			std::vector<NodeInfo> getChildren(Filter children)
 			{
-				return scene->getChildNodes(node, true);
+				return scene->getChildNodes(node, children);
 			}
 
 			/* Names of all children (excluding metadata); children that don't
@@ -83,6 +87,8 @@ namespace testing {
 
 			std::vector<NodeInfo> getTextures();
 
+			std::vector<NodeInfo> getSiblings(Filter parents, Filter siblings);
+
 			bool hasTextures();
 
 			std::unordered_map<std::string, repo::lib::RepoVariant> getMetadata();
@@ -98,6 +104,16 @@ namespace testing {
 			bool hasTransparency();
 
 			std::string getPath() const;
+
+            bool operator==(const NodeInfo& other) const
+            {
+				return node->getUniqueID() == other.node->getUniqueID();
+            }
+
+            bool operator!=(const NodeInfo& other) const
+            {
+				return !(*this == other);
+            }
 		};
 
 		std::vector<NodeInfo> findNodesByMetadata(std::string key, std::string value);
@@ -106,9 +122,10 @@ namespace testing {
 		NodeInfo findLeafNode(std::string name);
 		std::vector<NodeInfo> findLeafNodes(std::string name);
 		std::vector<NodeInfo> findTransformationNodesByName(std::string name);
-		std::vector<NodeInfo> getChildNodes(repo::core::model::RepoNode* node, bool ignoreMeta);
-		std::vector<NodeInfo> getParentNodes(repo::core::model::RepoNode* node);
+		std::vector<NodeInfo> getChildNodes(repo::core::model::RepoNode* node, Filter filter);
+		std::vector<NodeInfo> getParentNodes(repo::core::model::RepoNode* node, Filter filter);
 		std::vector<NodeInfo> getMeshes();
+		std::vector<NodeInfo> getMetadataNodes();
 		repo::lib::RepoMatrix getWorldTransform(repo::core::model::RepoNode* node);
 		NodeInfo getNodeInfo(repo::core::model::RepoNode* node);
 		NodeInfo getRootNode();
diff --git a/tools/bouncer_worker/package.json b/tools/bouncer_worker/package.json
index d9c18fc1..9e3ec2d0 100644
--- a/tools/bouncer_worker/package.json
+++ b/tools/bouncer_worker/package.json
@@ -1,6 +1,6 @@
 {
   "name": "3drepobouncerWorker",
-  "version": "5.18.0",
+  "version": "5.18.1",
   "engines": {
     "node": "22.x.x"
   },
